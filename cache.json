{"2024-06-24T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.16864v1","updated":"2024-06-24T17:59:58Z","published":"2024-06-24T17:59:58Z","title":"StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal","summary":"  This work addresses the challenge of high-quality surface normal estimation\nfrom monocular colored inputs (i.e., images and videos), a field which has\nrecently been revolutionized by repurposing diffusion priors. However, previous\nattempts still struggle with stochastic inference, conflicting with the\ndeterministic nature of the Image2Normal task, and costly ensembling step,\nwhich slows down the estimation process. Our method, StableNormal, mitigates\nthe stochasticity of the diffusion process by reducing inference variance, thus\nproducing \"Stable-and-Sharp\" normal estimates without any additional ensembling\nprocess. StableNormal works robustly under challenging imaging conditions, such\nas extreme lighting, blurring, and low quality. It is also robust against\ntransparent and reflective surfaces, as well as cluttered scenes with numerous\nobjects. Specifically, StableNormal employs a coarse-to-fine strategy, which\nstarts with a one-step normal estimator (YOSO) to derive an initial normal\nguess, that is relatively coarse but reliable, then followed by a\nsemantic-guided refinement process (SG-DRN) that refines the normals to recover\ngeometric details. The effectiveness of StableNormal is demonstrated through\ncompetitive performance in standard datasets such as DIODE-indoor, iBims,\nScannetV2 and NYUv2, and also in various downstream tasks, such as surface\nreconstruction and normal enhancement. These results evidence that StableNormal\nretains both the \"stability\" and \"sharpness\" for accurate normal estimation.\nStableNormal represents a baby attempt to repurpose diffusion priors for\ndeterministic estimation. To democratize this, code and models have been\npublicly available in hf.co/Stable-X\n","authors":["Chongjie Ye","Lingteng Qiu","Xiaodong Gu","Qi Zuo","Yushuang Wu","Zilong Dong","Liefeng Bo","Yuliang Xiu","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2406.16864v1.pdf","comment":"HF Demo: hf.co/Stable-X, Video:\n  https://www.youtube.com/watch?v=sylXTxG_U2U"},{"id":"http://arxiv.org/abs/2406.16866v1","updated":"2024-06-24T17:59:58Z","published":"2024-06-24T17:59:58Z","title":"Revisiting Referring Expression Comprehension Evaluation in the Era of\n  Large Multimodal Models","summary":"  Referring expression comprehension (REC) involves localizing a target\ninstance based on a textual description. Recent advancements in REC have been\ndriven by large multimodal models (LMMs) like CogVLM, which achieved 92.44%\naccuracy on RefCOCO. However, this study questions whether existing benchmarks\nsuch as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive\ncapabilities. We begin with a manual examination of these benchmarks, revealing\nhigh labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg,\nwhich undermines the authenticity of evaluations. We address this by excluding\nproblematic instances and reevaluating several LMMs capable of handling the REC\ntask, showing significant accuracy improvements, thus highlighting the impact\nof benchmark noise. In response, we introduce Ref-L4, a comprehensive REC\nbenchmark, specifically designed to evaluate modern REC models. Ref-L4 is\ndistinguished by four key features: 1) a substantial sample size with 45,341\nannotations; 2) a diverse range of object categories with 365 distinct types\nand varying instance scales from 30 to 3,767; 3) lengthy referring expressions\naveraging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique\nwords. We evaluate a total of 24 large models on Ref-L4 and provide valuable\ninsights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as\nour Ref-L4 benchmark and evaluation code, are available at\nhttps://github.com/JierunChen/Ref-L4.\n","authors":["Jierun Chen","Fangyun Wei","Jinjing Zhao","Sizhe Song","Bohuai Wu","Zhuoxuan Peng","S. -H. Gary Chan","Hongyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16863v1","updated":"2024-06-24T17:59:56Z","published":"2024-06-24T17:59:56Z","title":"FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models","summary":"  Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.\n","authors":["Haonan Qiu","Zhaoxi Chen","Zhouxia Wang","Yingqing He","Menghan Xia","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16863v1.pdf","comment":"Project Page: http://haonanqiu.com/projects/FreeTraj.html, Code Repo:\n  https://github.com/arthur-qiu/FreeTraj"},{"id":"http://arxiv.org/abs/2406.16862v1","updated":"2024-06-24T17:59:45Z","published":"2024-06-24T17:59:45Z","title":"Dreamitate: Real-World Visuomotor Policy Learning via Video Generation","summary":"  A key challenge in manipulation is learning a policy that can robustly\ngeneralize to diverse visual environments. A promising mechanism for learning\nrobust policies is to leverage video generative models, which are pretrained on\nlarge-scale datasets of internet videos. In this paper, we propose a visuomotor\npolicy learning framework that fine-tunes a video diffusion model on human\ndemonstrations of a given task. At test time, we generate an example of an\nexecution of the task conditioned on images of a novel scene, and use this\nsynthesized execution directly to control the robot. Our key insight is that\nusing common tools allows us to effortlessly bridge the embodiment gap between\nthe human hand and the robot manipulator. We evaluate our approach on four\ntasks of increasing complexity and demonstrate that harnessing internet-scale\ngenerative models allows the learned policy to achieve a significantly higher\ndegree of generalization than existing behavior cloning approaches.\n","authors":["Junbang Liang","Ruoshi Liu","Ege Ozguroglu","Sruthi Sudhakar","Achal Dave","Pavel Tokmakov","Shuran Song","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2406.16862v1.pdf","comment":"Project page: https://dreamitate.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2406.16860v1","updated":"2024-06-24T17:59:42Z","published":"2024-06-24T17:59:42Z","title":"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs","summary":"  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n","authors":["Shengbang Tong","Ellis Brown","Penghao Wu","Sanghyun Woo","Manoj Middepogu","Sai Charitha Akula","Jihan Yang","Shusheng Yang","Adithya Iyer","Xichen Pan","Austin Wang","Rob Fergus","Yann LeCun","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2406.16860v1.pdf","comment":"Website at https://cambrian-mllm.github.io"},{"id":"http://arxiv.org/abs/2406.16855v1","updated":"2024-06-24T17:58:47Z","published":"2024-06-24T17:58:47Z","title":"DreamBench++: A Human-Aligned Benchmark for Personalized Image\n  Generation","summary":"  Personalized image generation holds great promise in assisting humans in\neveryday work and life due to its impressive function in creatively generating\npersonalized content. However, current evaluations either are automated but\nmisalign with humans or require human evaluations that are time-consuming and\nexpensive. In this work, we present DreamBench++, a human-aligned benchmark\nautomated by advanced multimodal GPT models. Specifically, we systematically\ndesign the prompts to let GPT be both human-aligned and self-aligned, empowered\nwith task reinforcement. Further, we construct a comprehensive dataset\ncomprising diverse images and prompts. By benchmarking 7 modern generative\nmodels, we demonstrate that DreamBench++ results in significantly more\nhuman-aligned evaluation, helping boost the community with innovative findings.\n","authors":["Yuang Peng","Yuxin Cui","Haomiao Tang","Zekun Qi","Runpei Dong","Jing Bai","Chunrui Han","Zheng Ge","Xiangyu Zhang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2406.16855v1.pdf","comment":"Project page: https://dreambenchplus.github.io/"},{"id":"http://arxiv.org/abs/2406.16852v1","updated":"2024-06-24T17:58:06Z","published":"2024-06-24T17:58:06Z","title":"Long Context Transfer from Language to Vision","summary":"  Video sequences offer valuable temporal information, but existing large\nmultimodal models (LMMs) fall short in understanding extremely long videos.\nMany works address this by reducing the number of visual tokens using visual\nresamplers. Alternatively, in this paper, we approach this problem from the\nperspective of the language model. By simply extrapolating the context length\nof the language backbone, we enable LMMs to comprehend orders of magnitude more\nvisual tokens without any video training. We call this phenomenon long context\ntransfer and carefully ablate its properties. To effectively measure LMMs'\nability to generalize to long contexts in the vision modality, we develop\nV-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark\ninspired by the language model's NIAH test. Our proposed Long Video Assistant\n(LongVA) can process 2000 frames or over 200K visual tokens without additional\ncomplexities. With its extended context length, LongVA achieves\nstate-of-the-art performance on Video-MME among 7B-scale models by densely\nsampling more input frames. Our work is open-sourced at\nhttps://github.com/EvolvingLMMs-Lab/LongVA.\n","authors":["Peiyuan Zhang","Kaichen Zhang","Bo Li","Guangtao Zeng","Jingkang Yang","Yuanhan Zhang","Ziyue Wang","Haoran Tan","Chunyuan Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16851v1","updated":"2024-06-24T17:58:03Z","published":"2024-06-24T17:58:03Z","title":"Losing Visual Needles in Image Haystacks: Vision Language Models are\n  Easily Distracted in Short and Long Contexts","summary":"  We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking exponential decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.\n","authors":["Aditya Sharma","Michael Saxon","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16851v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.16850v1","updated":"2024-06-24T17:57:05Z","published":"2024-06-24T17:57:05Z","title":"From Perfect to Noisy World Simulation: Customizable Embodied\n  Multi-modal Perturbations for SLAM Robustness Benchmarking","summary":"  Embodied agents require robust navigation systems to operate in unstructured\nenvironments, making the robustness of Simultaneous Localization and Mapping\n(SLAM) models critical to embodied agent autonomy. While real-world datasets\nare invaluable, simulation-based benchmarks offer a scalable approach for\nrobustness evaluations. However, the creation of a challenging and controllable\nnoisy world with diverse perturbations remains under-explored. To this end, we\npropose a novel, customizable pipeline for noisy data synthesis, aimed at\nassessing the resilience of multi-modal SLAM models against various\nperturbations. The pipeline comprises a comprehensive taxonomy of sensor and\nmotion perturbations for embodied multi-modal (specifically RGB-D) sensing,\ncategorized by their sources and propagation order, allowing for procedural\ncomposition. We also provide a toolbox for synthesizing these perturbations,\nenabling the transformation of clean environments into challenging noisy\nsimulations. Utilizing the pipeline, we instantiate the large-scale\nNoisy-Replica benchmark, which includes diverse perturbation types, to evaluate\nthe risk tolerance of existing advanced RGB-D SLAM models. Our extensive\nanalysis uncovers the susceptibilities of both neural (NeRF and Gaussian\nSplatting -based) and non-neural SLAM models to disturbances, despite their\ndemonstrated accuracy in standard benchmarks. Our code is publicly available at\nhttps://github.com/Xiaohao-Xu/SLAM-under-Perturbation.\n","authors":["Xiaohao Xu","Tianyi Zhang","Sibo Wang","Xiang Li","Yongqi Chen","Ye Li","Bhiksha Raj","Matthew Johnson-Roberson","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16850v1.pdf","comment":"50 pages. arXiv admin note: substantial text overlap with\n  arXiv:2402.08125"},{"id":"http://arxiv.org/abs/2406.16848v1","updated":"2024-06-24T17:55:02Z","published":"2024-06-24T17:55:02Z","title":"Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation","summary":"  Significant advances have been made toward building accurate automatic\nsegmentation models for adult gliomas. However, the performance of these models\noften degrades when applied to pediatric glioma due to their imaging and\nclinical differences (domain shift). Obtaining sufficient annotated data for\npediatric glioma is typically difficult because of its rare nature. Also,\nmanual annotations are scarce and expensive. In this work, we propose\nDomain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation\nfrom adult glioma (source domain) to pediatric glioma (target domain).\nSpecifically, we add a domain classifier connected with a gradient reversal\nlayer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high\naccuracy, the GRL is activated with the goal of transferring domain-invariant\nfeatures from the classifier to the segmentation model while preserving\nsegmentation accuracy on the source domain. The accuracy of the classifier\nslowly degrades to chance levels. No annotations are used in the target domain.\nThe method is compared to 8 different supervised models using BraTS-Adult\nglioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows\nnotable performance enhancements in the tumor core (TC) region compared to the\nmodel that only uses adult data: ~32% better Dice scores and ~20 better 95th\npercentile Hausdorff distances. Moreover, our unsupervised approach shows no\nstatistically significant difference compared to the practical upper bound\nmodel using manual annotations from both datasets in TC region. The code is\nshared at https://github.com/Fjr9516/DA_nnUNet.\n","authors":["Jingru Fu","Simone Bendazzoli","Örjan Smedby","Rodrigo Moreno"],"pdf_url":"https://arxiv.org/pdf/2406.16848v1.pdf","comment":"10 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2403.01263v2","updated":"2024-06-24T17:49:37Z","published":"2024-03-02T16:51:35Z","title":"Single-image camera calibration with model-free distortion correction","summary":"  Camera calibration is a process of paramount importance in computer vision\napplications that require accurate quantitative measurements. The popular\nmethod developed by Zhang relies on the use of a large number of images of a\nplanar grid of fiducial points captured in multiple poses. Although flexible\nand easy to implement, Zhang's method has some limitations. The simultaneous\noptimization of the entire parameter set, including the coefficients of a\npredefined distortion model, may result in poor distortion correction at the\nimage boundaries or in miscalculation of the intrinsic parameters, even with a\nreasonably small reprojection error. Indeed, applications involving image\nstitching (e.g. multi-camera systems) require accurate mapping of distortion up\nto the outermost regions of the image. Moreover, intrinsic parameters affect\nthe accuracy of camera pose estimation, which is fundamental for applications\nsuch as vision servoing in robot navigation and automated assembly. This paper\nproposes a method for estimating the complete set of calibration parameters\nfrom a single image of a planar speckle pattern covering the entire sensor. The\ncorrespondence between image points and physical points on the calibration\ntarget is obtained using Digital Image Correlation. The effective focal length\nand the extrinsic parameters are calculated separately after a prior evaluation\nof the principal point. At the end of the procedure, a dense and uniform\nmodel-free distortion map is obtained over the entire image. Synthetic data\nwith different noise levels were used to test the feasibility of the proposed\nmethod and to compare its metrological performance with Zhang's method.\nReal-world tests demonstrate the potential of the developed method to reveal\naspects of the image formation that are hidden by averaging over multiple\nimages.\n","authors":["Katia Genovese"],"pdf_url":"https://arxiv.org/pdf/2403.01263v2.pdf","comment":"Accepted manuscript"},{"id":"http://arxiv.org/abs/2406.16817v1","updated":"2024-06-24T17:26:06Z","published":"2024-06-24T17:26:06Z","title":"GPT-4V Explorations: Mining Autonomous Driving","summary":"  This paper explores the application of the GPT-4V(ision) large visual\nlanguage model to autonomous driving in mining environments, where traditional\nsystems often falter in understanding intentions and making accurate decisions\nduring emergencies. GPT-4V introduces capabilities for visual question\nanswering and complex scene comprehension, addressing challenges in these\nspecialized settings.Our evaluation focuses on its proficiency in scene\nunderstanding, reasoning, and driving functions, with specific tests on its\nability to recognize and interpret elements such as pedestrians, various\nvehicles, and traffic devices. While GPT-4V showed robust comprehension and\ndecision-making skills, it faced difficulties in accurately identifying\nspecific vehicle types and managing dynamic interactions. Despite these\nchallenges, its effective navigation and strategic decision-making demonstrate\nits potential as a reliable agent for autonomous driving in the complex\nconditions of mining environments, highlighting its adaptability and\noperational viability in industrial settings.\n","authors":["Zixuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.16817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16815v1","updated":"2024-06-24T17:25:39Z","published":"2024-06-24T17:25:39Z","title":"ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians","summary":"  High-fidelity 3D garment synthesis from text is desirable yet challenging for\ndigital avatar creation. Recent diffusion-based approaches via Score\nDistillation Sampling (SDS) have enabled new possibilities but either\nintricately couple with human body or struggle to reuse. We introduce\nClotheDreamer, a 3D Gaussian-based method for generating wearable,\nproduction-ready 3D garment assets from text prompts. We propose a novel\nrepresentation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate\noptimization. DCGS represents clothed avatar as one Gaussian model but freezes\nbody Gaussian splats. To enhance quality and completeness, we incorporate\nbidirectional SDS to supervise clothed avatar and garment RGBD renderings\nrespectively with pose conditions and propose a new pruning strategy for loose\nclothing. Our approach can also support custom clothing templates as input.\nBenefiting from our design, the synthetic 3D garment can be easily applied to\nvirtual try-on and support physically accurate animation. Extensive experiments\nshowcase our method's superior and competitive performance. Our project page is\nat https://ggxxii.github.io/clothedreamer.\n","authors":["Yufei Liu","Junshu Tang","Chu Zheng","Shijie Zhang","Jinkun Hao","Junwei Zhu","Dongjin Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16815v1.pdf","comment":"Project Page: https://ggxxii.github.io/clothedreamer"},{"id":"http://arxiv.org/abs/2406.16807v1","updated":"2024-06-24T17:19:34Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16784v1","updated":"2024-06-24T16:45:28Z","published":"2024-06-24T16:45:28Z","title":"The Progression of Transformers from Language to Vision to MOT: A\n  Literature Review on Multi-Object Tracking with Transformers","summary":"  The transformer neural network architecture allows for autoregressive\nsequence-to-sequence modeling through the use of attention layers. It was\noriginally created with the application of machine translation but has\nrevolutionized natural language processing. Recently, transformers have also\nbeen applied across a wide variety of pattern recognition tasks, particularly\nin computer vision. In this literature review, we describe major advances in\ncomputer vision utilizing transformers. We then focus specifically on\nMulti-Object Tracking (MOT) and discuss how transformers are increasingly\nbecoming competitive in state-of-the-art MOT works, yet still lag behind\ntraditional deep learning methods.\n","authors":["Abhi Kamboj"],"pdf_url":"https://arxiv.org/pdf/2406.16784v1.pdf","comment":"This report was written in November 2022, and may not contain more\n  recent works since then"},{"id":"http://arxiv.org/abs/2406.16776v1","updated":"2024-06-24T16:35:58Z","published":"2024-06-24T16:35:58Z","title":"Instance Consistency Regularization for Semi-Supervised 3D Instance\n  Segmentation","summary":"  Large-scale datasets with point-wise semantic and instance labels are crucial\nto 3D instance segmentation but also expensive. To leverage unlabeled data,\nprevious semi-supervised 3D instance segmentation approaches have explored\nself-training frameworks, which rely on high-quality pseudo labels for\nconsistency regularization. They intuitively utilize both instance and semantic\npseudo labels in a joint learning manner. However, semantic pseudo labels\ncontain numerous noise derived from the imbalanced category distribution and\nnatural confusion of similar but distinct categories, which leads to severe\ncollapses in self-training. Motivated by the observation that 3D instances are\nnon-overlapping and spatially separable, we ask whether we can solely rely on\ninstance consistency regularization for improved semi-supervised segmentation.\nTo this end, we propose a novel self-training network InsTeacher3D to explore\nand exploit pure instance knowledge from unlabeled data. We first build a\nparallel base 3D instance segmentation model DKNet, which distinguishes each\ninstance from the others via discriminative instance kernels without reliance\non semantic segmentation. Based on DKNet, we further design a novel instance\nconsistency regularization framework to generate and leverage high-quality\ninstance pseudo labels. Experimental results on multiple large-scale datasets\nshow that the InsTeacher3D significantly outperforms prior state-of-the-art\nsemi-supervised approaches. Code is available:\nhttps://github.com/W1zheng/InsTeacher3D.\n","authors":["Yizheng Wu","Zhiyu Pan","Kewei Wang","Xingyi Li","Jiahao Cui","Liwen Xiao","Guosheng Lin","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2406.16776v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.15252v2","updated":"2024-06-24T16:22:55Z","published":"2024-06-21T15:43:46Z","title":"VideoScore: Building Automatic Metrics to Simulate Fine-grained Human\n  Feedback for Video Generation","summary":"  The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train VideoScore (initialized from Mantis)\nbased on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between VideoScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that VideoScore has consistently much higher correlation with human\njudges than other metrics. Due to these results, we believe VideoScore can\nserve as a great proxy for human raters to (1) rate different video models to\ntrack progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.\n","authors":["Xuan He","Dongfu Jiang","Ge Zhang","Max Ku","Achint Soni","Sherman Siu","Haonan Chen","Abhranil Chandra","Ziyan Jiang","Aaran Arulraj","Kai Wang","Quy Duc Do","Yuansheng Ni","Bohan Lyu","Yaswanth Narsupalli","Rongqi Fan","Zhiheng Lyu","Yuchen Lin","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.15252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01902v2","updated":"2024-06-24T16:11:33Z","published":"2023-06-02T20:19:19Z","title":"Unlearnable Examples for Diffusion Models: Protect Data from\n  Unauthorized Exploitation","summary":"  Diffusion models have demonstrated remarkable performance in image generation\ntasks, paving the way for powerful AIGC applications. However, these\nwidely-used generative models can also raise security and privacy concerns,\nsuch as copyright infringement, and sensitive data leakage. To tackle these\nissues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard\nimages from unauthorized exploitation. Our approach involves designing an\nalgorithm to generate sample-wise perturbation noise for each image to be\nprotected. This imperceptible protective noise makes the data almost\nunlearnable for diffusion models, i.e., diffusion models trained or fine-tuned\non the protected data cannot generate high-quality and diverse images related\nto the protected training data. Theoretically, we frame this as a max-min\noptimization problem and introduce EUDP, a noise scheduler-based method to\nenhance the effectiveness of the protective noise. We evaluate our methods on\nboth Denoising Diffusion Probabilistic Model and Latent Diffusion Models,\ndemonstrating that training diffusion models on the protected data lead to a\nsignificant reduction in the quality of the generated images. Especially, the\nexperimental results on Stable Diffusion demonstrate that our method\neffectively safeguards images from being used to train Diffusion Models in\nvarious tasks, such as training specific objects and styles. This achievement\nholds significant importance in real-world scenarios, as it contributes to the\nprotection of privacy and copyright against AI-generated content.\n","authors":["Zhengyue Zhao","Jinhao Duan","Xing Hu","Kaidi Xu","Chenan Wang","Rui Zhang","Zidong Du","Qi Guo","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2306.01902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16754v1","updated":"2024-06-24T16:00:20Z","published":"2024-06-24T16:00:20Z","title":"The MRI Scanner as a Diagnostic: Image-less Active Sampling","summary":"  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),\nusing MRI as a Point-of-Care (POC) disease identification tool poses\nsignificant accessibility challenges due to the use of high magnetic field\nstrength and lengthy acquisition times. We ask a simple question: Can we\ndynamically optimise acquired samples, at the patient level, according to an\n(automated) downstream decision task, while discounting image reconstruction?\nWe propose an ML-based framework that learns an active sampling strategy, via\nreinforcement learning, at a patient-level to directly infer disease from\nundersampled k-space. We validate our approach by inferring Meniscus Tear in\nundersampled knee MRI data, where we achieve diagnostic performance comparable\nwith ML-based diagnosis, using fully sampled k-space data. We analyse\ntask-specific sampling policies, showcasing the adaptability of our active\nsampling approach. The introduced frugal sampling strategies have the potential\nto reduce high field strength requirements that in turn strengthen the\nviability of MRI-based POC disease identification and associated preliminary\nscreening tools.\n","authors":["Yuning Du","Rohan Dharmakumar","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2406.16754v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2404.02072v5","updated":"2024-06-24T15:52:57Z","published":"2024-04-02T16:20:02Z","title":"EGTR: Extracting Graph from Transformer for Scene Graph Generation","summary":"  Scene Graph Generation (SGG) is a challenging task of detecting objects and\npredicting relationships between objects. After DETR was developed, one-stage\nSGG models based on a one-stage object detector have been actively studied.\nHowever, complex modeling is used to predict the relationship between objects,\nand the inherent relationship between object queries learned in the multi-head\nself-attention of the object detector has been neglected. We propose a\nlightweight one-stage SGG model that extracts the relation graph from the\nvarious relationships learned in the multi-head self-attention layers of the\nDETR decoder. By fully utilizing the self-attention by-products, the relation\ngraph can be extracted effectively with a shallow relation extraction head.\nConsidering the dependency of the relation extraction task on the object\ndetection task, we propose a novel relation smoothing technique that adjusts\nthe relation label adaptively according to the quality of the detected objects.\nBy the relation smoothing, the model is trained according to the continuous\ncurriculum that focuses on object detection task at the beginning of training\nand performs multi-task learning as the object detection performance gradually\nimproves. Furthermore, we propose a connectivity prediction task that predicts\nwhether a relation exists between object pairs as an auxiliary task of the\nrelation extraction. We demonstrate the effectiveness and efficiency of our\nmethod for the Visual Genome and Open Image V6 datasets. Our code is publicly\navailable at https://github.com/naver-ai/egtr.\n","authors":["Jinbae Im","JeongYeon Nam","Nokyung Park","Hyungmin Lee","Seunghyun Park"],"pdf_url":"https://arxiv.org/pdf/2404.02072v5.pdf","comment":"CVPR 2024 (Best paper award candidate)"},{"id":"http://arxiv.org/abs/2311.08695v2","updated":"2024-06-24T15:51:13Z","published":"2023-11-15T04:50:30Z","title":"Attribute Diversity Determines the Systematicity Gap in VQA","summary":"  The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n","authors":["Ian Berlot-Attwell","Kumar Krishna Agrawal","A. Michael Carrell","Yash Sharma","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2311.08695v2.pdf","comment":"33 pages, 20 figures"},{"id":"http://arxiv.org/abs/2312.00084v2","updated":"2024-06-24T15:44:42Z","published":"2023-11-30T07:17:43Z","title":"Can Protective Perturbation Safeguard Personal Data from Being Exploited\n  by Stable Diffusion?","summary":"  Stable Diffusion has established itself as a foundation model in generative\nAI artistic applications, receiving widespread research and application. Some\nrecent fine-tuning methods have made it feasible for individuals to implant\npersonalized concepts onto the basic Stable Diffusion model with minimal\ncomputational costs on small datasets. However, these innovations have also\ngiven rise to issues like facial privacy forgery and artistic copyright\ninfringement. In recent studies, researchers have explored the addition of\nimperceptible adversarial perturbations to images to prevent potential\nunauthorized exploitation and infringements when personal data is used for\nfine-tuning Stable Diffusion. Although these studies have demonstrated the\nability to protect images, it is essential to consider that these methods may\nnot be entirely applicable in real-world scenarios. In this paper, we\nsystematically evaluate the use of perturbations to protect images within a\npractical threat model. The results suggest that these approaches may not be\nsufficient to safeguard image privacy and copyright effectively. Furthermore,\nwe introduce a purification method capable of removing protected perturbations\nwhile preserving the original image structure to the greatest extent possible.\nExperiments reveal that Stable Diffusion can effectively learn from purified\nimages over all protective methods.\n","authors":["Zhengyue Zhao","Jinhao Duan","Kaidi Xu","Chenan Wang","Rui Zhang","Zidong Du","Qi Guo","Xing Hu"],"pdf_url":"https://arxiv.org/pdf/2312.00084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.00816v4","updated":"2024-06-24T15:40:01Z","published":"2023-06-01T15:42:06Z","title":"Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and\n  Compatible Triggers","summary":"  Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non benign samples, dubbed \\textit{backdoor attack}. Currently, implementing\nbackdoor attacks in physical scenarios still faces significant challenges.\nPhysical attacks are labor-intensive and time-consuming, and the triggers are\nselected in a manual and heuristic way. Moreover, expanding digital attacks to\nphysical scenarios faces many challenges due to their sensitivity to visual\ndistortions and the absence of counterparts in the real world. To address these\nchallenges, we define a novel trigger called the \\textbf{V}isible,\n\\textbf{S}emantic, \\textbf{S}ample-Specific, and \\textbf{C}ompatible (VSSC)\ntrigger, to achieve effective, stealthy and robust simultaneously, which can\nalso be effectively deployed in the physical scenario using corresponding\nobjects. To implement the VSSC trigger, we propose an automated pipeline\ncomprising three modules: a trigger selection module that systematically\nidentifies suitable triggers leveraging large language models, a trigger\ninsertion module that employs generative models to seamlessly integrate\ntriggers into images, and a quality assessment module that ensures the natural\nand successful insertion of triggers through vision-language models. Extensive\nexperimental results and analysis validate the effectiveness, stealthiness, and\nrobustness of the VSSC trigger. It can not only maintain robustness under\nvisual distortions but also demonstrates strong practicality in the physical\nscenario. We hope that the proposed VSSC trigger and implementation approach\ncould inspire future studies on designing more practical triggers in backdoor\nattacks.\n","authors":["Ruotong Wang","Hongrui Chen","Zihao Zhu","Li Liu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2306.00816v4.pdf","comment":"23 pages, 21 figures, 18 tables"},{"id":"http://arxiv.org/abs/2311.01380v2","updated":"2024-06-24T15:39:13Z","published":"2023-11-02T16:37:27Z","title":"Sim2Real Bilevel Adaptation for Object Surface Classification using\n  Vision-Based Tactile Sensors","summary":"  In this paper, we address the Sim2Real gap in the field of vision-based\ntactile sensors for classifying object surfaces. We train a Diffusion Model to\nbridge this gap using a relatively small dataset of real-world images randomly\ncollected from unlabeled everyday objects via the DIGIT sensor. Subsequently,\nwe employ a simulator to generate images by uniformly sampling the surface of\nobjects from the YCB Model Set. These simulated images are then translated into\nthe real domain using the Diffusion Model and automatically labeled to train a\nclassifier. During this training, we further align features of the two domains\nusing an adversarial procedure. Our evaluation is conducted on a dataset of\ntactile images obtained from a set of ten 3D printed YCB objects. The results\nreveal a total accuracy of 81.9%, a significant improvement compared to the\n34.7% achieved by the classifier trained solely on simulated images. This\ndemonstrates the effectiveness of our approach. We further validate our\napproach using the classifier on a 6D object pose estimation task from tactile\ndata.\n","authors":["Gabriele M. Caddeo","Andrea Maracani","Paolo D. Alfano","Nicola A. Piga","Lorenzo Rosasco","Lorenzo Natale"],"pdf_url":"https://arxiv.org/pdf/2311.01380v2.pdf","comment":"6 pages, accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2406.14862v2","updated":"2024-06-24T15:30:34Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16724v1","updated":"2024-06-24T15:29:08Z","published":"2024-06-24T15:29:08Z","title":"μ-Net: A Deep Learning-Based Architecture for μ-CT Segmentation","summary":"  X-ray computed microtomography ({\\mu}-CT) is a non-destructive technique that\ncan generate high-resolution 3D images of the internal anatomy of medical and\nbiological samples. These images enable clinicians to examine internal anatomy\nand gain insights into the disease or anatomical morphology. However,\nextracting relevant information from 3D images requires semantic segmentation\nof the regions of interest, which is usually done manually and results\ntime-consuming and tedious. In this work, we propose a novel framework that\nuses a convolutional neural network (CNN) to automatically segment the full\nmorphology of the heart of Carassius auratus. The framework employs an\noptimized 2D CNN architecture that can infer a 3D segmentation of the sample,\navoiding the high computational cost of a 3D CNN architecture. We tackle the\nchallenges of handling large and high-resoluted image data (over a thousand\npixels in each dimension) and a small training database (only three samples) by\nproposing a standard protocol for data normalization and processing. Moreover,\nwe investigate how the noise, contrast, and spatial resolution of the sample\nand the training of the architecture are affected by the reconstruction\ntechnique, which depends on the number of input images. Experiments show that\nour framework significantly reduces the time required to segment new samples,\nallowing a faster microtomography analysis of the Carassius auratus heart\nshape. Furthermore, our framework can work with any bio-image (biological and\nmedical) from {\\mu}-CT with high-resolution and small dataset size\n","authors":["Pierangela Bruno","Edoardo De Rose","Carlo Adornetto","Francesco Calimeri","Sandro Donato","Raffaele Giuseppe Agostino","Daniela Amelio","Riccardo Barberi","Maria Carmela Cerra","Maria Caterina Crocco","Mariacristina Filice","Raffaele Filosa","Gianluigi Greco","Sandra Imbrogno","Vincenzo Formoso"],"pdf_url":"https://arxiv.org/pdf/2406.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16710v1","updated":"2024-06-24T15:11:35Z","published":"2024-06-24T15:11:35Z","title":"Portrait3D: 3D Head Generation from Single In-the-wild Portrait Image","summary":"  While recent works have achieved great success on one-shot 3D common object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, Portrait3D, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the id-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from single in-the-wild portrait\nimages. The project page is at https://jinkun-hao.github.io/Portrait3D/.\n","authors":["Jinkun Hao","Junshu Tang","Jiangning Zhang","Ran Yi","Yijia Hong","Moran Li","Weijian Cao","Yating Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2406.16710v1.pdf","comment":"https://jinkun-hao.github.io/Portrait3D/"},{"id":"http://arxiv.org/abs/2406.09681v2","updated":"2024-06-24T15:11:27Z","published":"2024-06-14T03:07:23Z","title":"Asymmetrical Siamese Network for Point Clouds Normal Estimation","summary":"  In recent years, deep learning-based point cloud normal estimation has made\ngreat progress. However, existing methods mainly rely on the PCPNet dataset,\nleading to overfitting. In addition, the correlation between point clouds with\ndifferent noise scales remains unexplored, resulting in poor performance in\ncross-domain scenarios. In this paper, we explore the consistency of intrinsic\nfeatures learned from clean and noisy point clouds using an Asymmetric Siamese\nNetwork architecture. By applying reasonable constraints between features\nextracted from different branches, we enhance the quality of normal estimation.\nMoreover, we introduce a novel multi-view normal estimation dataset that\nincludes a larger variety of shapes with different noise levels. Evaluation of\nexisting methods on this new dataset reveals their inability to adapt to\ndifferent types of shapes, indicating a degree of overfitting. Extensive\nexperiments show that the proposed dataset poses significant challenges for\npoint cloud normal estimation and that our feature constraint mechanism\neffectively improves upon existing methods and reduces overfitting in current\narchitectures.\n","authors":["Wei Jin","Jun Zhou","Nannan Li","Haba Madeline","Xiuping Liu"],"pdf_url":"https://arxiv.org/pdf/2406.09681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18435v2","updated":"2024-06-24T15:07:34Z","published":"2024-03-19T17:57:24Z","title":"QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation\n  Challenge","summary":"  Uncertainty in medical image segmentation tasks, especially inter-rater\nvariability, arising from differences in interpretations and annotations by\nvarious experts, presents a significant challenge in achieving consistent and\nreliable image segmentation. This variability not only reflects the inherent\ncomplexity and subjective nature of medical image interpretation but also\ndirectly impacts the development and evaluation of automated segmentation\nalgorithms. Accurately modeling and quantifying this variability is essential\nfor enhancing the robustness and clinical applicability of these algorithms. We\nreport the set-up and summarize the benchmark results of the Quantification of\nUncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was\norganized in conjunction with International Conferences on Medical Image\nComputing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The\nchallenge focuses on the uncertainty quantification of medical image\nsegmentation which considers the omnipresence of inter-rater variability in\nimaging datasets. The large collection of images with multi-rater annotations\nfeatures various modalities such as MRI and CT; various organs such as the\nbrain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.\nA total of 24 teams submitted different solutions to the problem, combining\nvarious baseline models, Bayesian neural networks, and ensemble model\ntechniques. The obtained results indicate the importance of the ensemble\nmodels, as well as the need for further research to develop efficient 3D\nmethods for uncertainty quantification methods in 3D segmentation tasks.\n","authors":["Hongwei Bran Li","Fernando Navarro","Ivan Ezhov","Amirhossein Bayat","Dhritiman Das","Florian Kofler","Suprosanna Shit","Diana Waldmannstetter","Johannes C. Paetzold","Xiaobin Hu","Benedikt Wiestler","Lucas Zimmer","Tamaz Amiranashvili","Chinmay Prabhakar","Christoph Berger","Jonas Weidner","Michelle Alonso-Basant","Arif Rashid","Ujjwal Baid","Wesam Adel","Deniz Ali","Bhakti Baheti","Yingbin Bai","Ishaan Bhatt","Sabri Can Cetindag","Wenting Chen","Li Cheng","Prasad Dutand","Lara Dular","Mustafa A. Elattar","Ming Feng","Shengbo Gao","Henkjan Huisman","Weifeng Hu","Shubham Innani","Wei Jiat","Davood Karimi","Hugo J. Kuijf","Jin Tae Kwak","Hoang Long Le","Xiang Lia","Huiyan Lin","Tongliang Liu","Jun Ma","Kai Ma","Ting Ma","Ilkay Oksuz","Robbie Holland","Arlindo L. Oliveira","Jimut Bahan Pal","Xuan Pei","Maoying Qiao","Anindo Saha","Raghavendra Selvan","Linlin Shen","Joao Lourenco Silva","Ziga Spiclin","Sanjay Talbar","Dadong Wang","Wei Wang","Xiong Wang","Yin Wang","Ruiling Xia","Kele Xu","Yanwu Yan","Mert Yergin","Shuang Yu","Lingxi Zeng","YingLin Zhang","Jiachen Zhao","Yefeng Zheng","Martin Zukovec","Richard Do","Anton Becker","Amber Simpson","Ender Konukoglu","Andras Jakab","Spyridon Bakas","Leo Joskowicz","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2405.18435v2.pdf","comment":"initial technical report"},{"id":"http://arxiv.org/abs/2406.16701v1","updated":"2024-06-24T15:04:14Z","published":"2024-06-24T15:04:14Z","title":"Demystifying the Effect of Receptive Field Size in U-Net Models for\n  Medical Image Segmentation","summary":"  Medical image segmentation is a critical task in healthcare applications, and\nU-Nets have demonstrated promising results. This work delves into the\nunderstudied aspect of receptive field (RF) size and its impact on the U-Net\nand Attention U-Net architectures. This work explores several critical elements\nincluding the relationship between RF size, characteristics of the region of\ninterest, and model performance, as well as the balance between RF size and\ncomputational costs for U-Net and Attention U-Net methods for different\ndatasets. This work also proposes a mathematical notation for representing the\ntheoretical receptive field (TRF) of a given layer in a network and proposes\ntwo new metrics - effective receptive field (ERF) rate and the Object rate to\nquantify the fraction of significantly contributing pixels within the ERF\nagainst the TRF area and assessing the relative size of the segmentation object\ncompared to the TRF size respectively. The results demonstrate that there\nexists an optimal TRF size that successfully strikes a balance between\ncapturing a wider global context and maintaining computational efficiency,\nthereby optimizing model performance. Interestingly, a distinct correlation is\nobserved between the data complexity and the required TRF size; segmentation\nbased solely on contrast achieved peak performance even with smaller TRF sizes,\nwhereas more complex segmentation tasks necessitated larger TRFs. Attention\nU-Net models consistently outperformed their U-Net counterparts, highlighting\nthe value of attention mechanisms regardless of TRF size. These novel insights\npresent an invaluable resource for developing more efficient U-Net-based\narchitectures for medical imaging and pave the way for future exploration. A\ntool is also developed that calculates the TRF for a U-Net (and Attention\nU-Net) model, and also suggest an appropriate TRF size for a given model and\ndataset.\n","authors":["Vincent Loos","Rohit Pardasani","Navchetan Awasthi"],"pdf_url":"https://arxiv.org/pdf/2406.16701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11433v2","updated":"2024-06-24T15:03:52Z","published":"2023-09-20T16:10:53Z","title":"A Systematic Review of Few-Shot Learning in Medical Imaging","summary":"  The lack of annotated medical images limits the performance of deep learning\nmodels, which usually need large-scale labelled datasets. Few-shot learning\ntechniques can reduce data scarcity issues and enhance medical image analysis,\nespecially with meta-learning. This systematic review gives a comprehensive\noverview of few-shot learning in medical imaging. We searched the literature\nsystematically and selected 80 relevant articles published from 2018 to 2023.\nWe clustered the articles based on medical outcomes, such as tumour\nsegmentation, disease classification, and image registration; anatomical\nstructure investigated (i.e. heart, lung, etc.); and the meta-learning method\nused. For each cluster, we examined the papers' distributions and the results\nprovided by the state-of-the-art. In addition, we identified a generic pipeline\nshared among all the studies. The review shows that few-shot learning can\novercome data scarcity in most outcomes and that meta-learning is a popular\nchoice to perform few-shot learning because it can adapt to new tasks with few\nlabelled samples. In addition, following meta-learning, supervised learning and\nsemi-supervised learning stand out as the predominant techniques employed to\ntackle few-shot learning challenges in medical imaging and also best\nperforming. Lastly, we observed that the primary application areas\npredominantly encompass cardiac, pulmonary, and abdominal domains. This\nsystematic review aims to inspire further research to improve medical image\nanalysis and patient care.\n","authors":["Eva Pachetti","Sara Colantonio"],"pdf_url":"https://arxiv.org/pdf/2309.11433v2.pdf","comment":"48 pages, 29 figures, 10 tables, submitted to Elsevier on 19 Sep 2023"},{"id":"http://arxiv.org/abs/2406.16695v1","updated":"2024-06-24T14:58:17Z","published":"2024-06-24T14:58:17Z","title":"Geometry-Aware Score Distillation via 3D Consistent Noising and Gradient\n  Consistency Modeling","summary":"  Score distillation sampling (SDS), the methodology in which the score from\npretrained 2D diffusion models is distilled into 3D representation, has\nrecently brought significant advancements in text-to-3D generation task.\nHowever, this approach is still confronted with critical geometric\ninconsistency problems such as the Janus problem. Starting from a hypothesis\nthat such inconsistency problems may be induced by multiview inconsistencies\nbetween 2D scores predicted from various viewpoints, we introduce GSD, a simple\nand general plug-and-play framework for incorporating 3D consistency and\ntherefore geometry awareness into the SDS process. Our methodology is composed\nof three components: 3D consistent noising, designed to produce 3D consistent\nnoise maps that perfectly follow the standard Gaussian distribution,\ngeometry-based gradient warping for identifying correspondences between\npredicted gradients of different viewpoints, and novel gradient consistency\nloss to optimize the scene geometry toward producing more consistent gradients.\nWe demonstrate that our method significantly improves performance, successfully\naddressing the geometric inconsistency problems in text-to-3D generation task\nwith minimal computation cost and being compatible with existing score\ndistillation-based models. Our project page is available at\nhttps://ku-cvlab.github.io/GSD/.\n","authors":["Min-Seop Kwak","Donghoon Ahn","Ines Hyeonsu Kim","Jin-wha Kim","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2406.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16683v1","updated":"2024-06-24T14:43:02Z","published":"2024-06-24T14:43:02Z","title":"Repulsive Score Distillation for Diverse Sampling of Diffusion Models","summary":"  Score distillation sampling has been pivotal for integrating diffusion models\ninto generation of complex visuals. Despite impressive results it suffers from\nmode collapse and lack of diversity. To cope with this challenge, we leverage\nthe gradient flow interpretation of score distillation to propose Repulsive\nScore Distillation (RSD). In particular, we propose a variational framework\nbased on repulsion of an ensemble of particles that promotes diversity. Using a\nvariational approximation that incorporates a coupling among particles, the\nrepulsion appears as a simple regularization that allows interaction of\nparticles based on their relative pairwise similarity, measured e.g., via\nradial basis kernels. We design RSD for both unconstrained and constrained\nsampling scenarios. For constrained sampling we focus on inverse problems in\nthe latent space that leads to an augmented variational formulation, that\nstrikes a good balance between compute, quality and diversity. Our extensive\nexperiments for text-to-image generation, and inverse problems demonstrate that\nRSD achieves a superior trade-off between diversity and quality compared with\nstate-of-the-art alternatives.\n","authors":["Nicolas Zilberstein","Morteza Mardani","Santiago Segarra"],"pdf_url":"https://arxiv.org/pdf/2406.16683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16658v1","updated":"2024-06-24T14:08:27Z","published":"2024-06-24T14:08:27Z","title":"Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods","summary":"  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n","authors":["Remi Laumont","Yiqiu Dong","Martin Skovgaard Andersen"],"pdf_url":"https://arxiv.org/pdf/2406.16658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16641v1","updated":"2024-06-24T13:45:31Z","published":"2024-06-24T13:45:31Z","title":"Vision-Language Consistency Guided Multi-modal Prompt Learning for Blind\n  AI Generated Image Quality Assessment","summary":"  Recently, textual prompt tuning has shown inspirational performance in\nadapting Contrastive Language-Image Pre-training (CLIP) models to natural image\nquality assessment. However, such uni-modal prompt learning method only tunes\nthe language branch of CLIP models. This is not enough for adapting CLIP models\nto AI generated image quality assessment (AGIQA) since AGIs visually differ\nfrom natural images. In addition, the consistency between AGIs and user input\ntext prompts, which correlates with the perceptual quality of AGIs, is not\ninvestigated to guide AGIQA. In this letter, we propose vision-language\nconsistency guided multi-modal prompt learning for blind AGIQA, dubbed\nCLIP-AGIQA. Specifically, we introduce learnable textual and visual prompts in\nlanguage and vision branches of CLIP models, respectively. Moreover, we design\na text-to-image alignment quality prediction task, whose learned\nvision-language consistency knowledge is used to guide the optimization of the\nabove multi-modal prompts. Experimental results on two public AGIQA datasets\ndemonstrate that the proposed method outperforms state-of-the-art quality\nassessment models. The source code is available at\nhttps://github.com/JunFu1995/CLIP-AGIQA.\n","authors":["Jun Fu","Wei Zhou","Qiuping Jiang","Hantao Liu","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2406.16641v1.pdf","comment":"Accepted by IEEE Signal Processing Letter"},{"id":"http://arxiv.org/abs/2406.16638v1","updated":"2024-06-24T13:44:06Z","published":"2024-06-24T13:44:06Z","title":"Feature Fusion for Human Activity Recognition using Parameter-Optimized\n  Multi-Stage Graph Convolutional Network and Transformer Models","summary":"  Human activity recognition (HAR) is a crucial area of research that involves\nunderstanding human movements using computer and machine vision technology.\nDeep learning has emerged as a powerful tool for this task, with models such as\nConvolutional Neural Networks (CNNs) and Transformers being employed to capture\nvarious aspects of human motion. One of the key contributions of this work is\nthe demonstration of the effectiveness of feature fusion in improving HAR\naccuracy by capturing spatial and temporal features, which has important\nimplications for the development of more accurate and robust activity\nrecognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa,\nand TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and\nevaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG\nachieved high accuracies and f1-scores, while LARa and PKU-MMD had lower\nscores. Feature fusion improved results across datasets.\n","authors":["Mohammad Belal","Taimur Hassan","Abdelfatah Ahmed","Ahmad Aljarah","Nael Alsheikh","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2406.16638v1.pdf","comment":"7 pages, 1 figure, conference"},{"id":"http://arxiv.org/abs/2406.16633v1","updated":"2024-06-24T13:30:55Z","published":"2024-06-24T13:30:55Z","title":"MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network","summary":"  End-to-end (E2E) training approaches are commonly plagued by high memory\nconsumption, reduced efficiency in training, challenges in model\nparallelization, and suboptimal biocompatibility. Local learning is considered\na novel interactive training method that holds promise as an alternative to\nE2E. Nonetheless, conventional local learning methods fall short in achieving\nhigh model accuracy due to inadequate local inter-module interactions. In this\npaper, we introduce a new model known as the Scaling Supervised Local Learning\nwith Multilaminar Leap Augmented Auxiliary Network (MLAAN). MLAAN features an\ninnovative supervised local learning approach coupled with a robust\nreinforcement module. This dual-component design enables the MLAAN to integrate\nsmoothly with established local learning techniques, thereby enhancing the\nefficacy of the foundational methods. The method simultaneously acquires the\nlocal and global features of the model separately by constructing an\nindependent auxiliary network and a cascade auxiliary network on the one hand\nand incorporates a leap augmented module, which serves to counteract the\nreduced learning capacity often associated with weaker supervision. This\narchitecture not only augments the exchange of information amongst the local\nmodules but also effectively mitigates the model's tendency toward myopia. The\nexperimental evaluations conducted on four benchmark datasets, CIFAR-10,\nSTL-10, SVHN, and ImageNet, demonstrate that the integration of MLAAN with\nexisting supervised local learning methods significantly enhances the original\nmethodologies. Of particular note, MLAAN enables local learning methods to\ncomprehensively outperform end-to-end training approaches in terms of optimal\nperformance while saving GPU memory.\n","authors":["Yuming Zhang","Shouxin Zhang","Peizhe Wang","Feiyu Zhu","Dongzhi Guan","Jiabin Liu","Changpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2406.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16623v1","updated":"2024-06-24T13:13:31Z","published":"2024-06-24T13:13:31Z","title":"Articulate your NeRF: Unsupervised articulated object modeling via\n  conditional view synthesis","summary":"  We propose a novel unsupervised method to learn the pose and\npart-segmentation of articulated objects with rigid parts. Given two\nobservations of an object in different articulation states, our method learns\nthe geometry and appearance of object parts by using an implicit model from the\nfirst observation, distils the part segmentation and articulation from the\nsecond observation while rendering the latter observation. Additionally, to\ntackle the complexities in the joint optimization of part segmentation and\narticulation, we propose a voxel grid-based initialization strategy and a\ndecoupled optimization procedure. Compared to the prior unsupervised work, our\nmodel obtains significantly better performance, and generalizes to objects with\nmultiple parts while it can be efficiently from few views for the latter\nobservation.\n","authors":["Jianning Deng","Kartic Subr","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.16623v1.pdf","comment":"9 pages for the maincontent, excluding references and supplementaries"},{"id":"http://arxiv.org/abs/2406.16620v1","updated":"2024-06-24T13:05:39Z","published":"2024-06-24T13:05:39Z","title":"OmAgent: A Multi-modal Agent Framework for Complex Video Understanding\n  with Task Divide-and-Conquer","summary":"  Recent advancements in Large Language Models (LLMs) have expanded their\ncapabilities to multimodal contexts, including comprehensive video\nunderstanding. However, processing extensive videos such as 24-hour CCTV\nfootage or full-length films presents significant challenges due to the vast\ndata and processing demands. Traditional methods, like extracting key frames or\nconverting frames to text, often result in substantial information loss. To\naddress these shortcomings, we develop OmAgent, efficiently stores and\nretrieves relevant video frames for specific queries, preserving the detailed\ncontent of videos. Additionally, it features an Divide-and-Conquer Loop capable\nof autonomous reasoning, dynamically invoking APIs and tools to enhance query\nprocessing and accuracy. This approach ensures robust video understanding,\nsignificantly reducing information loss. Experimental results affirm OmAgent's\nefficacy in handling various types of videos and complex tasks. Moreover, we\nhave endowed it with greater autonomy and a robust tool-calling system,\nenabling it to accomplish even more intricate tasks.\n","authors":["Lu Zhang","Tiancheng Zhao","Heting Ying","Yibo Ma","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2406.16620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10212v2","updated":"2024-06-24T13:02:57Z","published":"2024-06-14T17:48:45Z","title":"NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity","summary":"  Photoelasticity enables full-field stress analysis in transparent objects\nthrough stress-induced birefringence. Existing techniques are limited to 2D\nslices and require destructively slicing the object. Recovering the internal 3D\nstress distribution of the entire object is challenging as it involves solving\na tensor tomography problem and handling phase wrapping ambiguities. We\nintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress\ntensor fields as neural implicit representations from polarization\nmeasurements. Our key insight is to jointly handle phase unwrapping and tensor\ntomography using a differentiable forward model based on Jones calculus. Our\nnon-linear model faithfully matches real captures, unlike prior linear\napproximations. We develop an experimental multi-axis polariscope setup to\ncapture 3D photoelasticity and experimentally demonstrate that NeST\nreconstructs the internal stress distribution for objects with varying shape\nand force conditions. Additionally, we showcase novel applications in stress\nanalysis, such as visualizing photoelastic fringes by virtually slicing the\nobject and viewing photoelastic fringes from unseen viewpoints. NeST paves the\nway for scalable non-destructive 3D photoelastic analysis.\n","authors":["Akshat Dave","Tianyi Zhang","Aaron Young","Ramesh Raskar","Wolfgang Heidrich","Ashok Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2406.10212v2.pdf","comment":"Project webpage: https://akshatdave.github.io/nest"},{"id":"http://arxiv.org/abs/2406.16615v1","updated":"2024-06-24T12:55:06Z","published":"2024-06-24T12:55:06Z","title":"The Championship-Winning Solution for the 5th CLVISION Challenge 2024","summary":"  In this paper, we introduce our approach to the 5th CLVision Challenge, which\npresents distinctive challenges beyond traditional class incremental learning.\nUnlike standard settings, this competition features the recurrence of\npreviously encountered classes and includes unlabeled data that may contain\nOut-of-Distribution (OOD) categories. Our approach is based on Winning\nSubnetworks to allocate independent parameter spaces for each task addressing\nthe catastrophic forgetting problem in class incremental learning and employ\nthree training strategies: supervised classification learning, unsupervised\ncontrastive learning, and pseudo-label classification learning to fully utilize\nthe information in both labeled and unlabeled data, enhancing the\nclassification performance of each subnetwork. Furthermore, during the\ninference stage, we have devised an interaction strategy between subnetworks,\nwhere the prediction for a specific class of a particular sample is the average\nlogits across different subnetworks corresponding to that class, leveraging the\nknowledge learned from different subnetworks on recurring classes to improve\nclassification accuracy. These strategies can be simultaneously applied to the\nthree scenarios of the competition, effectively solving the difficulties in the\ncompetition scenarios. Experimentally, our method ranks first in both the\npre-selection and final evaluation stages, with an average accuracy of 0.4535\nduring the preselection stage and an average accuracy of 0.4805 during the\nfinal evaluation stage.\n","authors":["Sishun Pan","Tingmin Li","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.16615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01188v4","updated":"2024-06-24T12:49:35Z","published":"2024-04-01T15:45:58Z","title":"MonoBox: Tightness-free Box-supervised Polyp Segmentation using\n  Monotonicity Constraint","summary":"  We propose MonoBox, an innovative box-supervised segmentation method\nconstrained by monotonicity to liberate its training from the user-unfriendly\nbox-tightness assumption. In contrast to conventional box-supervised\nsegmentation, where the box edges must precisely touch the target boundaries,\nMonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise\nsegmentation. The 'linchpin' is that, within the noisy zones around box edges,\nMonoBox discards the traditional misguiding multiple-instance learning loss,\nand instead optimizes a carefully-designed objective, termed monotonicity\nconstraint. Along directions transitioning from the foreground to background,\nthis new constraint steers responses to adhere to a trend of monotonically\ndecreasing values. Consequently, the originally unreliable learning within the\nnoisy zones is transformed into a correct and effective monotonicity\noptimization. Moreover, an adaptive label correction is introduced, enabling\nMonoBox to enhance the tightness of box annotations using predicted masks from\nthe previous epoch and dynamically shrink the noisy zones as training\nprogresses. We verify MonoBox in the box-supervised segmentation task of\npolyps, where satisfying box-tightness is challenging due to the vague\nboundaries between the polyp and normal tissues. Experiments on both public\nsynthetic and in-house real noisy datasets demonstrate that MonoBox exceeds\nother anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%,\nrespectively. Codes are at https://github.com/Huster-Hq/MonoBox.\n","authors":["Qiang Hu","Zhenyu Yi","Ying Zhou","Ting Li","Fan Huang","Mei Liu","Qiang Li","Zhiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.01188v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16608v1","updated":"2024-06-24T12:47:21Z","published":"2024-06-24T12:47:21Z","title":"When Invariant Representation Learning Meets Label Shift: Insufficiency\n  and Theoretical Insights","summary":"  As a crucial step toward real-world learning scenarios with changing\nenvironments, dataset shift theory and invariant representation learning\nalgorithm have been extensively studied to relax the identical distribution\nassumption in classical learning setting. Among the different assumptions on\nthe essential of shifting distributions, generalized label shift (GLS) is the\nlatest developed one which shows great potential to deal with the complex\nfactors within the shift. In this paper, we aim to explore the limitations of\ncurrent dataset shift theory and algorithm, and further provide new insights by\npresenting a comprehensive understanding of GLS. From theoretical aspect, two\ninformative generalization bounds are derived, and the GLS learner is proved to\nbe sufficiently close to optimal target model from the Bayesian perspective.\nThe main results show the insufficiency of invariant representation learning,\nand prove the sufficiency and necessity of GLS correction for generalization,\nwhich provide theoretical supports and innovations for exploring generalizable\nmodel under dataset shift. From methodological aspect, we provide a unified\nview of existing shift correction frameworks, and propose a kernel\nembedding-based correction algorithm (KECA) to minimize the generalization\nerror and achieve successful knowledge transfer. Both theoretical results and\nextensive experiment evaluations demonstrate the sufficiency and necessity of\nGLS correction for addressing dataset shift and the superiority of proposed\nalgorithm.\n","authors":["You-Wei Luo","Chuan-Xian Ren"],"pdf_url":"https://arxiv.org/pdf/2406.16608v1.pdf","comment":"Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)"},{"id":"http://arxiv.org/abs/2406.16601v1","updated":"2024-06-24T12:41:51Z","published":"2024-06-24T12:41:51Z","title":"Do As I Do: Pose Guided Human Motion Copy","summary":"  Human motion copy is an intriguing yet challenging task in artificial\nintelligence and computer vision, which strives to generate a fake video of a\ntarget person performing the motion of a source person. The problem is\ninherently challenging due to the subtle human-body texture details to be\ngenerated and the temporal consistency to be considered. Existing approaches\ntypically adopt a conventional GAN with an L1 or L2 loss to produce the target\nfake video, which intrinsically necessitates a large number of training samples\nthat are challenging to acquire. Meanwhile, current methods still have\ndifficulties in attaining realistic image details and temporal consistency,\nwhich unfortunately can be easily perceived by human observers. Motivated by\nthis, we try to tackle the issues from three aspects: (1) We constrain\npose-to-appearance generation with a perceptual loss and a theoretically\nmotivated Gromov-Wasserstein loss to bridge the gap between pose and\nappearance. (2) We present an episodic memory module in the pose-to-appearance\ngeneration to propel continuous learning that helps the model learn from its\npast poor generations. We also utilize geometrical cues of the face to optimize\nfacial details and refine each key body part with a dedicated local GAN. (3) We\nadvocate generating the foreground in a sequence-to-sequence manner rather than\na single-frame manner, explicitly enforcing temporal inconsistency. Empirical\nresults on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse\ndatasets, demonstrate that our method is capable of generating realistic target\nvideos while precisely copying motion from a source video. Our method\nsignificantly outperforms state-of-the-art approaches and gains 7.2% and 12.4%\nimprovements in PSNR and FID respectively.\n","authors":["Sifan Wu","Zhenguang Liu","Beibei Zhang","Roger Zimmermann","Zhongjie Ba","Xiaosong Zhang","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2406.16601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16593v1","updated":"2024-06-24T12:33:56Z","published":"2024-06-24T12:33:56Z","title":"Measuring the Recyclability of Electronic Components to Assist Automatic\n  Disassembly and Sorting Waste Printed Circuit Boards","summary":"  The waste of electrical and electronic equipment has been increased due to\nthe fast evolution of technology products and competition of many IT sectors.\nEvery year millions of tons of electronic waste are thrown into the environment\nwhich causes high consequences for human health. Therefore, it is crucial to\ncontrol this waste flow using technology, especially using Artificial\nIntelligence but also reclamation of critical raw materials for new production\nprocesses. In this paper, we focused on the measurement of recyclability of\nwaste electronic components (WECs) from waste printed circuit boards (WPCBs)\nusing mathematical innovation model. This innovative approach evaluates both\nthe recyclability and recycling difficulties of WECs, integrating an AI model\nfor improved disassembly and sorting. Assessing the recyclability of individual\nelectronic components present on WPCBs provides insight into the recovery\npotential of valuable materials and indicates the level of complexity involved\nin recycling in terms of economic worth and production utility. This novel\nmeasurement approach helps AI models in accurately determining the number of\nclasses to be identified and sorted during the automated disassembly of\ndiscarded PCBs. It also facilitates the model in iterative training and\nvalidation of individual electronic components.\n","authors":["Muhammad Mohsin","Xianlai Zeng","Stefano Rovetta","Francesco Masulli"],"pdf_url":"https://arxiv.org/pdf/2406.16593v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.16592v1","updated":"2024-06-24T12:33:21Z","published":"2024-06-24T12:33:21Z","title":"Toward Fairer Face Recognition Datasets","summary":"  Face recognition and verification are two computer vision tasks whose\nperformance has progressed with the introduction of deep representations.\nHowever, ethical, legal, and technical challenges due to the sensitive\ncharacter of face data and biases in real training datasets hinder their\ndevelopment. Generative AI addresses privacy by creating fictitious identities,\nbut fairness problems persist. We promote fairness by introducing a demographic\nattributes balancing mechanism in generated training datasets. We experiment\nwith an existing real dataset, three generated training datasets, and the\nbalanced versions of a diffusion-based dataset. We propose a comprehensive\nevaluation that considers accuracy and fairness equally and includes a rigorous\nregression-based statistical analysis of attributes. The analysis shows that\nbalancing reduces demographic unfairness. Also, a performance gap persists\ndespite generation becoming more accurate with time. The proposed balancing\nmethod and comprehensive verification evaluation promote fairer and transparent\nface recognition and verification.\n","authors":["Alexandre Fournier-Mongieux","Michael Soumm","Adrian Popescu","Bertrand Luvison","Hervé Le Borgne"],"pdf_url":"https://arxiv.org/pdf/2406.16592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16583v1","updated":"2024-06-24T12:16:51Z","published":"2024-06-24T12:16:51Z","title":"Personalized federated learning based on feature fusion","summary":"  Federated learning enables distributed clients to collaborate on training\nwhile storing their data locally to protect client privacy. However, due to the\nheterogeneity of data, models, and devices, the final global model may need to\nperform better for tasks on each client. Communication bottlenecks, data\nheterogeneity, and model heterogeneity have been common challenges in federated\nlearning. In this work, we considered a label distribution skew problem, a type\nof data heterogeneity easily overlooked. In the context of classification, we\npropose a personalized federated learning approach called pFedPM. In our\nprocess, we replace traditional gradient uploading with feature uploading,\nwhich helps reduce communication costs and allows for heterogeneous client\nmodels. These feature representations play a role in preserving privacy to some\nextent.\n  We use a hyperparameter $a$ to mix local and global features, which enables\nus to control the degree of personalization. We also introduced a relation\nnetwork as an additional decision layer, which provides a non-linear learnable\nclassifier to predict labels. Experimental results show that, with an\nappropriate setting of $a$, our scheme outperforms several recent FL methods on\nMNIST, FEMNIST, and CRIFAR10 datasets and achieves fewer communications.\n","authors":["Wolong Xing","Zhenkui Shi","Hongyan Peng","Xiantao Hu","Xianxian Li"],"pdf_url":"https://arxiv.org/pdf/2406.16583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07189v2","updated":"2024-06-24T12:09:46Z","published":"2024-06-11T12:01:11Z","title":"RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker","summary":"  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n","authors":["Yunfeng Li","Bo Wang","Jiuran Sun","Xueyi Wu","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2406.07189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02956v2","updated":"2024-06-24T12:07:52Z","published":"2024-02-05T12:34:03Z","title":"AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a\n  Single High-Resolution Image","summary":"  The process of estimating and counting tree density using only a single\naerial or satellite image is a difficult task in the fields of photogrammetry\nand remote sensing. However, it plays a crucial role in the management of\nforests. The huge variety of trees in varied topography severely hinders tree\ncounting models to perform well. The purpose of this paper is to propose a\nframework that is learnt from the source domain with sufficient labeled trees\nand is adapted to the target domain with only a limited number of labeled\ntrees. Our method, termed as AdaTreeFormer, contains one shared encoder with a\nhierarchical feature extraction scheme to extract robust features from the\nsource and target domains. It also consists of three subnets: two for\nextracting self-domain attention maps from source and target domains\nrespectively and one for extracting cross-domain attention maps. For the\nlatter, an attention-to-adapt mechanism is introduced to distill relevant\ninformation from different domains while generating tree density maps; a\nhierarchical cross-domain feature alignment scheme is proposed that\nprogressively aligns the features from the source and target domains. We also\nadopt adversarial learning into the framework to further reduce the gap between\nsource and target domains. Our AdaTreeFormer is evaluated on six designed\ndomain adaptation tasks using three tree counting datasets, \\ie Jiangsu,\nYosemite, and London. Experimental results show that AdaTreeFormer\nsignificantly surpasses the state of the art, \\eg in the cross domain from the\nYosemite to Jiangsu dataset, it achieves a reduction of 15.9 points in terms of\nthe absolute counting errors and an increase of 10.8\\% in the accuracy of the\ndetected trees' locations. The codes and datasets are available at\n\\emph{\\color{magenta}{https://github.com/HAAClassic/AdaTreeFormer}}.\n","authors":["Hamed Amini Amirkolaee","Miaojing Shi","Lianghua He","Mark Mulligan"],"pdf_url":"https://arxiv.org/pdf/2402.02956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00933v4","updated":"2024-06-24T12:03:00Z","published":"2023-04-03T12:45:52Z","title":"Knowledge Accumulation in Continually Learned Representations and the\n  Issue of Feature Forgetting","summary":"  Continual learning research has shown that neural networks suffer from\ncatastrophic forgetting \"at the output level\", but it is debated whether this\nis also the case at the level of learned representations. Multiple recent\nstudies ascribe representations a certain level of innate robustness against\nforgetting -- that they only forget minimally in comparison with forgetting at\nthe output level. We revisit and expand upon the experiments that revealed this\ndifference in forgetting and illustrate the coexistence of two phenomena that\naffect the quality of continually learned representations: knowledge\naccumulation and feature forgetting. Taking both aspects into account, we show\nthat, even though forgetting in the representation (i.e. feature forgetting)\ncan be small in absolute terms, when measuring relative to how much was learned\nduring a task, forgetting in the representation tends to be just as\ncatastrophic as forgetting at the output level. Next we show that this feature\nforgetting is problematic as it substantially slows down the incremental\nlearning of good general representations (i.e. knowledge accumulation).\nFinally, we study how feature forgetting and knowledge accumulation are\naffected by different types of continual learning methods.\n","authors":["Timm Hess","Eli Verwimp","Gido M. van de Ven","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2304.00933v4.pdf","comment":"TMLR 2024"},{"id":"http://arxiv.org/abs/2406.16564v1","updated":"2024-06-24T12:01:55Z","published":"2024-06-24T12:01:55Z","title":"FASTC: A Fast Attentional Framework for Semantic Traversability\n  Classification Using Point Cloud","summary":"  Producing traversability maps and understanding the surroundings are crucial\nprerequisites for autonomous navigation. In this paper, we address the problem\nof traversability assessment using point clouds. We propose a novel pillar\nfeature extraction module that utilizes PointNet to capture features from point\nclouds organized in vertical volume and a 2D encoder-decoder structure to\nconduct traversability classification instead of the widely used 3D\nconvolutions. This results in less computational cost while even better\nperformance is achieved at the same time. We then propose a new spatio-temporal\nattention module to fuse multi-frame information, which can properly handle the\nvarying density problem of LIDAR point clouds, and this makes our module able\nto assess distant areas more accurately. Comprehensive experimental results on\naugmented Semantic KITTI and RELLIS-3D datasets show that our method is able to\nachieve superior performance over existing approaches both quantitatively and\nquantitatively.\n","authors":["Yirui Chen","Pengjin Wei","Zhenhuan Liu","Bingchao Wang","Jie Yang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16564v1.pdf","comment":"Accepted to ECAI2023 Our code is publicly available at\n  [this](https://github.com/chenyirui/FASTC)"},{"id":"http://arxiv.org/abs/2308.04702v2","updated":"2024-06-24T12:01:20Z","published":"2023-08-09T04:46:16Z","title":"Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric\n  Multi-Modal Network","summary":"  State-of-the-art multimodal semantic segmentation strategies combining LiDAR\nand color data are usually designed on top of asymmetric information-sharing\nschemes and assume that both modalities are always available. This strong\nassumption may not hold in real-world scenarios, where sensors are prone to\nfailure or can face adverse conditions that make the acquired information\nunreliable. This problem is exacerbated when continual learning scenarios are\nconsidered since they have stringent data reliability constraints. In this\nwork, we re-frame the task of multimodal semantic segmentation by enforcing a\ntightly coupled feature representation and a symmetric information-sharing\nscheme, which allows our approach to work even when one of the input modalities\nis missing. We also introduce an ad-hoc class-incremental continual learning\nscheme, proving our approach's effectiveness and reliability even in\nsafety-critical settings, such as autonomous driving. We evaluate our approach\non the SemanticKITTI dataset, achieving impressive performances.\n","authors":["Francesco Barbato","Elena Camuffo","Simone Milani","Pietro Zanuttigh"],"pdf_url":"https://arxiv.org/pdf/2308.04702v2.pdf","comment":"Accepted ad ICIP 2024, 6 pages, 5 figures, 3 tables, 7 equations"},{"id":"http://arxiv.org/abs/2406.16562v1","updated":"2024-06-24T11:56:15Z","published":"2024-06-24T11:56:15Z","title":"EvalAlign: Evaluating Text-to-Image Models through Precision Alignment\n  of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations","summary":"  The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive datasets. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to\nalign closely with human evaluative judgments, resulting in a robust evaluation\nmodel. Our comprehensive tests across 24 text-to-image generation models\ndemonstrate that EvalAlign not only provides superior metric stability but also\naligns more closely with human preferences than existing metrics, confirming\nits effectiveness and utility in model assessment.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Mengping Yang","Cheng Zhang","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2406.16562v1.pdf","comment":"Github Repository: https://github.com/SAIS-FUXI/EvalAlign"},{"id":"http://arxiv.org/abs/2406.16544v1","updated":"2024-06-24T11:29:52Z","published":"2024-06-24T11:29:52Z","title":"Hierarchical B-frame Video Coding for Long Group of Pictures","summary":"  Learned video compression methods already outperform VVC in the low-delay\n(LD) case, but the random-access (RA) scenario remains challenging. Most works\non learned RA video compression either use HEVC as an anchor or compare it to\nVVC in specific test conditions, using RGB-PSNR metric instead of Y-PSNR and\navoiding comprehensive evaluation. Here, we present an end-to-end learned video\ncodec for random access that combines training on long sequences of frames,\nrate allocation designed for hierarchical coding and content adaptation on\ninference. We show that under common test conditions (JVET-CTC), it achieves\nresults comparable to VTM (VVC reference software) in terms of YUV-PSNR BD-Rate\non some classes of videos, and outperforms it on almost all test sets in terms\nof VMAF BD-Rate. On average it surpasses open LD and RA end-to-end solutions in\nterms of VMAF and YUV BD-Rates.\n","authors":["Ivan Kirillov","Denis Parkhomenko","Kirill Chernyshev","Alexander Pletnev","Yibo Shi","Kai Lin","Dmitry Babin"],"pdf_url":"https://arxiv.org/pdf/2406.16544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16540v1","updated":"2024-06-24T11:20:44Z","published":"2024-06-24T11:20:44Z","title":"Improving robustness to corruptions with multiplicative weight\n  perturbations","summary":"  Deep neural networks (DNNs) excel on clean images but struggle with corrupted\nones. Incorporating specific corruptions into the data augmentation pipeline\ncan improve robustness to those corruptions but may harm performance on clean\nimages and other types of distortion. In this paper, we introduce an\nalternative approach that improves the robustness of DNNs to a wide range of\ncorruptions without compromising accuracy on clean images. We first demonstrate\nthat input perturbations can be mimicked by multiplicative perturbations in the\nweight space. Leveraging this, we propose Data Augmentation via Multiplicative\nPerturbation (DAMP), a training method that optimizes DNNs under random\nmultiplicative weight perturbations. We also examine the recently proposed\nAdaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs\nunder adversarial multiplicative weight perturbations. Experiments on image\nclassification datasets (CIFAR-10/100, TinyImageNet and ImageNet) and neural\nnetwork architectures (ResNet50, ViT-S/16) show that DAMP enhances model\ngeneralization performance in the presence of corruptions across different\nsettings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch,\nreaching the top-1 error of 23.7% which is comparable to ResNet50 without\nextensive data augmentations.\n","authors":["Trung Trinh","Markus Heinonen","Luigi Acerbi","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2406.16540v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.16537v1","updated":"2024-06-24T11:16:37Z","published":"2024-06-24T11:16:37Z","title":"Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization","summary":"  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods\n","authors":["Yuhang Ma","Wenting Xu","Jiji Tang","Qinfeng Jin","Rongsheng Zhang","Zeng Zhao","Changjie Fan","Zhipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16531v1","updated":"2024-06-24T11:10:41Z","published":"2024-06-24T11:10:41Z","title":"GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization","summary":"  The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location(IMDL). However, the lack of a large-scale\ndata foundation makes IMDL task unattainable. In this paper, a local\nmanipulation pipeline is designed, incorporating the powerful SAM, ChatGPT and\ngenerative models. Upon this basis, We propose the GIM dataset, which has the\nfollowing advantages: 1) Large scale, including over one million pairs of\nAI-manipulated images and real images. 2) Rich Image Content, encompassing a\nbroad range of image classes 3) Diverse Generative Manipulation, manipulated\nimages with state-of-the-art generators and various manipulation tasks. The\naforementioned advantages allow for a more comprehensive evaluation of IMDL\nmethods, extending their applicability to diverse images. We introduce two\nbenchmark settings to evaluate the generalization capability and comprehensive\nperformance of baseline methods. In addition, we propose a novel IMDL\nframework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial Block (FSB), and a Multi-window Anomalous Modelling (MWAM)\nModule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nprevious state-of-the-art works significantly on two different benchmarks.\n","authors":["Yirui Chen","Xudong Huang","Quan Zhang","Wei Li","Mingjian Zhu","Qiangyu Yan","Simiao Li","Hanting Chen","Hailin Hu","Jie Yang","Wei Liu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16531v1.pdf","comment":"Code page: https://github.com/chenyirui/GIM"},{"id":"http://arxiv.org/abs/2406.01467v2","updated":"2024-06-24T11:04:08Z","published":"2024-06-03T15:56:58Z","title":"RaDe-GS: Rasterizing Depth in Gaussian Splatting","summary":"  Gaussian Splatting (GS) has proven to be highly effective in novel view\nsynthesis, achieving high-quality and real-time rendering. However, its\npotential for reconstructing detailed 3D shapes has not been fully explored.\nExisting methods often suffer from limited shape accuracy due to the discrete\nand unstructured nature of Gaussian splats, which complicates the shape\nextraction. While recent techniques like 2D GS have attempted to improve shape\nreconstruction, they often reformulate the Gaussian primitives in ways that\nreduce both rendering quality and computational efficiency. To address these\nproblems, our work introduces a rasterized approach to render the depth maps\nand surface normal maps of general 3D Gaussian splats. Our method not only\nsignificantly enhances shape reconstruction accuracy but also maintains the\ncomputational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer\ndistance error comparable to NeuraLangelo on the DTU dataset and maintains\nsimilar computational efficiency as the original 3D GS methods. Our method is a\nsignificant advancement in Gaussian Splatting and can be directly integrated\ninto existing Gaussian Splatting-based methods.\n","authors":["Baowen Zhang","Chuan Fang","Rakesh Shrestha","Yixun Liang","Xiaoxiao Long","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2406.01467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16518v1","updated":"2024-06-24T10:47:45Z","published":"2024-06-24T10:47:45Z","title":"Vision Mamba-based autonomous crack segmentation on concrete, asphalt,\n  and masonry surfaces","summary":"  Convolutional neural networks (CNNs) and Transformers have shown advanced\naccuracy in crack detection under certain conditions. Yet, the fixed local\nattention can compromise the generalisation of CNNs, and the quadratic\ncomplexity of the global self-attention restricts the practical deployment of\nTransformers. Given the emergence of the new-generation architecture of Mamba,\nthis paper proposes a Vision Mamba (VMamba)-based framework for crack\nsegmentation on concrete, asphalt, and masonry surfaces, with high accuracy,\ngeneralisation, and less computational complexity. Having 15.6% - 74.5% fewer\nparameters, the encoder-decoder network integrated with VMamba could obtain up\nto 2.8% higher mDS than representative CNN-based models while showing about the\nsame performance as Transformer-based models. Moreover, the VMamba-based\nencoder-decoder network could process high-resolution image input with up to\n90.6% lower floating-point operations.\n","authors":["Zhaohui Chen","Elyas Asadi Shamsabadi","Sheng Jiang","Luming Shen","Daniel Dias-da-Costa"],"pdf_url":"https://arxiv.org/pdf/2406.16518v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16513v1","updated":"2024-06-24T10:40:46Z","published":"2024-06-24T10:40:46Z","title":"Multi-Modal Vision Transformers for Crop Mapping from Satellite Image\n  Time Series","summary":"  Using images acquired by different satellite sensors has shown to improve\nclassification performance in the framework of crop mapping from satellite\nimage time series (SITS). Existing state-of-the-art architectures use\nself-attention mechanisms to process the temporal dimension and convolutions\nfor the spatial dimension of SITS. Motivated by the success of purely\nattention-based architectures in crop mapping from single-modal SITS, we\nintroduce several multi-modal multi-temporal transformer-based architectures.\nSpecifically, we investigate the effectiveness of Early Fusion, Cross Attention\nFusion and Synchronized Class Token Fusion within the Temporo-Spatial Vision\nTransformer (TSViT). Experimental results demonstrate significant improvements\nover state-of-the-art architectures with both convolutional and self-attention\ncomponents.\n","authors":["Theresa Follath","David Mickisch","Jan Hemmerling","Stefan Erasmi","Marcel Schwieder","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2406.16513v1.pdf","comment":"5 pages, 2 figures, 1 table. Accepted at IEEE International\n  Geoscience and Remote Sensing Symposium (IGARSS) 2024. Our code is available\n  at https://git.tu-berlin.de/rsim/mmtsvit"},{"id":"http://arxiv.org/abs/2402.08506v3","updated":"2024-06-24T10:24:14Z","published":"2024-02-13T15:02:46Z","title":"P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient\n  Pediatric Echocardiographic Left Ventricular Segmentation","summary":"  In pediatric cardiology, the accurate and immediate assessment of cardiac\nfunction through echocardiography is crucial since it can determine whether\nurgent intervention is required in many emergencies. However, echocardiography\nis characterized by ambiguity and heavy background noise interference, causing\nmore difficulty in accurate segmentation. Present methods lack efficiency and\nare prone to mistakenly segmenting some background noise areas, such as the\nleft ventricular area, due to noise disturbance. To address these issues, we\nintroduce P-Mamba, which integrates the Mixture of Experts (MoE) concept for\nefficient pediatric echocardiographic left ventricular segmentation.\nSpecifically, we utilize the recently proposed ViM layers from the vision mamba\nto enhance our model's computational and memory efficiency while modeling\nglobal dependencies.In the DWT-based Perona-Malik Diffusion (PMD) Block, we\ndevise a PMD Block for noise suppression while preserving the left ventricle's\nlocal shape cues. Consequently, our proposed P-Mamba innovatively combines the\nPMD's noise suppression and local feature extraction capabilities with Mamba's\nefficient design for global dependency modeling. We conducted segmentation\nexperiments on two pediatric ultrasound datasets and a general ultrasound\ndataset, namely Echonet-dynamic, and achieved state-of-the-art (SOTA) results.\nLeveraging the strengths of the P-Mamba block, our model demonstrates superior\naccuracy and efficiency compared to established models, including vision\ntransformers with quadratic and linear computational complexity.\n","authors":["Zi Ye","Tianxiang Chen","Fangyijie Wang","Hanwei Zhang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.08506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16538v2","updated":"2024-06-24T10:21:17Z","published":"2024-04-25T11:53:36Z","title":"OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images","summary":"  Recent open-world 3D representation learning methods using Vision-Language\nModels (VLMs) to align 3D data with image-text information have shown superior\n3D zero-shot performance. However, CAD-rendered images for this alignment often\nlack realism and texture variation, compromising alignment robustness.\nMoreover, the volume discrepancy between 3D and 2D pretraining datasets\nhighlights the need for effective strategies to transfer the representational\nabilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel\nopen-world 3D model using depth-aligned images generated from a diffusion model\nfor robust multimodal alignment. These images exhibit greater texture diversity\nthan CAD renderings due to the stochastic nature of the diffusion model. By\nrefining the depth map projection pipeline and designing depth-specific\nprompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D\nrepresentation learning with streamlined fine-tuning. Our experiments show that\nOpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks,\ndespite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In\nzero-shot classification, OpenDlign surpasses previous models by 8.0% on\nModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images\nfor multimodal alignment consistently enhances the performance of other\nstate-of-the-art models.\n","authors":["Ye Mao","Junpeng Jing","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2404.16538v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.16502v1","updated":"2024-06-24T10:12:03Z","published":"2024-06-24T10:12:03Z","title":"LOGCAN++: Local-global class-aware network for semantic segmentation of\n  remote sensing images","summary":"  Remote sensing images usually characterized by complex backgrounds, scale and\norientation variations, and large intra-class variance. General semantic\nsegmentation methods usually fail to fully investigate the above issues, and\nthus their performances on remote sensing image segmentation are limited. In\nthis paper, we propose our LOGCAN++, a semantic segmentation model customized\nfor remote sensing images, which is made up of a Global Class Awareness (GCA)\nmodule and several Local Class Awareness (LCA) modules. The GCA module captures\nglobal representations for class-level context modeling to reduce the\ninterference of background noise. The LCA module generates local class\nrepresentations as intermediate perceptual elements to indirectly associate\npixels with the global class representations, targeting at dealing with the\nlarge intra-class variance problem. In particular, we introduce affine\ntransformations in the LCA module for adaptive extraction of local class\nrepresentations to effectively tolerate scale and orientation variations in\nremotely sensed images. Extensive experiments on three benchmark datasets show\nthat our LOGCAN++ outperforms current mainstream general and remote sensing\nsemantic segmentation methods and achieves a better trade-off between speed and\naccuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.\n","authors":["Xiaowen Ma","Rongrong Lian","Zhenkai Wu","Hongbo Guo","Mengting Ma","Sensen Wu","Zhenhong Du","Siyang Song","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16502v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.16501v1","updated":"2024-06-24T10:10:03Z","published":"2024-06-24T10:10:03Z","title":"UNICAD: A Unified Approach for Attack Detection, Noise Reduction and\n  Novel Class Identification","summary":"  As the use of Deep Neural Networks (DNNs) becomes pervasive, their\nvulnerability to adversarial attacks and limitations in handling unseen classes\nposes significant challenges. The state-of-the-art offers discrete solutions\naimed to tackle individual issues covering specific adversarial attack\nscenarios, classification or evolving learning. However, real-world systems\nneed to be able to detect and recover from a wide range of adversarial attacks\nwithout sacrificing classification accuracy and to flexibly act in {\\bf unseen}\nscenarios. In this paper, UNICAD, is proposed as a novel framework that\nintegrates a variety of techniques to provide an adaptive solution.\n  For the targeted image classification, UNICAD achieves accurate image\nclassification, detects unseen classes, and recovers from adversarial attacks\nusing Prototype and Similarity-based DNNs with denoising autoencoders. Our\nexperiments performed on the CIFAR-10 dataset highlight UNICAD's effectiveness\nin adversarial mitigation and unseen class classification, outperforming\ntraditional models.\n","authors":["Alvaro Lopez Pellicer","Kittipos Giatgong","Yi Li","Neeraj Suri","Plamen Angelov"],"pdf_url":"https://arxiv.org/pdf/2406.16501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16481v1","updated":"2024-06-24T09:36:58Z","published":"2024-06-24T09:36:58Z","title":"Improving Quaternion Neural Networks with Quaternionic Activation\n  Functions","summary":"  In this paper, we propose novel quaternion activation functions where we\nmodify either the quaternion magnitude or the phase, as an alternative to the\ncommonly used split activation functions. We define criteria that are relevant\nfor quaternion activation functions, and subsequently we propose our novel\nactivation functions based on this analysis. Instead of applying a known\nactivation function like the ReLU or Tanh on the quaternion elements\nseparately, these activation functions consider the quaternion properties and\nrespect the quaternion space $\\mathbb{H}$. In particular, all quaternion\ncomponents are utilized to calculate all output components, carrying out the\nbenefit of the Hamilton product in e.g. the quaternion convolution to the\nactivation functions. The proposed activation functions can be incorporated in\narbitrary quaternion valued neural networks trained with gradient descent\ntechniques. We further discuss the derivatives of the proposed activation\nfunctions where we observe beneficial properties for the activation functions\naffecting the phase. Specifically, they prove to be sensitive on basically the\nwhole input range, thus improved gradient flow can be expected. We provide an\nelaborate experimental evaluation of our proposed quaternion activation\nfunctions including comparison with the split ReLU and split Tanh on two image\nclassification tasks using the CIFAR-10 and SVHN dataset. There, especially the\nquaternion activation functions affecting the phase consistently prove to\nprovide better performance.\n","authors":["Johannes Pöppelbaum","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2406.16481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03066v2","updated":"2024-06-24T09:35:41Z","published":"2023-06-05T17:43:50Z","title":"Of Mice and Mates: Automated Classification and Modelling of Mouse\n  Behaviour in Groups using a Single Model across Cages","summary":"  Behavioural experiments often happen in specialised arenas, but this may\nconfound the analysis. To address this issue, we provide tools to study mice in\nthe home-cage environment, equipping biologists with the possibility to capture\nthe temporal aspect of the individual's behaviour and model the interaction and\ninterdependence between cage-mates with minimal human intervention. Our main\ncontribution is the novel Group Behaviour Model (GBM) which summarises the\njoint behaviour of groups of mice across cages, using a permutation matrix to\nmatch the mouse identities in each cage to the model. In support of the above,\nwe also (a) developed the Activity Labelling Module (ALM) to automatically\nclassify mouse behaviour from video, and (b) released two datasets, ABODe for\ntraining behaviour classifiers and IMADGE for modelling behaviour.\n","authors":["Michael P. J. Camilleri","Rasneer S. Bains","Christopher K. I. Williams"],"pdf_url":"https://arxiv.org/pdf/2306.03066v2.pdf","comment":"International Journal of Computer Vision (2024)"},{"id":"http://arxiv.org/abs/2406.16477v1","updated":"2024-06-24T09:30:36Z","published":"2024-06-24T09:30:36Z","title":"DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World\n  Image Super-Resolution","summary":"  Image super-resolution pursuits reconstructing high-fidelity high-resolution\ncounterpart for low-resolution image. In recent years, diffusion-based models\nhave garnered significant attention due to their capabilities with rich prior\nknowledge. The success of diffusion models based on general text prompts has\nvalidated the effectiveness of textual control in the field of text2image.\nHowever, given the severe degradation commonly presented in low-resolution\nimages, coupled with the randomness characteristics of diffusion models,\ncurrent models struggle to adequately discern semantic and degradation\ninformation within severely degraded images. This often leads to obstacles such\nas semantic loss, visual artifacts, and visual hallucinations, which pose\nsubstantial challenges for practical use. To address these challenges, this\npaper proposes to leverage degradation-aligned language prompt for accurate,\nfine-grained, and high-fidelity image restoration. Complementary priors\nincluding semantic content descriptions and degradation prompts are explored.\nSpecifically, on one hand, image-restoration prompt alignment decoder is\nproposed to automatically discern the degradation degree of LR images, thereby\ngenerating beneficial degradation priors for image restoration. On the other\nhand, much richly tailored descriptions from pretrained multimodal large\nlanguage model elicit high-level semantic priors closely aligned with human\nperception, ensuring fidelity control for image restoration. Comprehensive\ncomparisons with state-of-the-art methods have been done on several popular\nsynthetic and real-world benchmark datasets. The quantitative and qualitative\nanalysis have demonstrated that the proposed method achieves a new\nstate-of-the-art perceptual quality level, especially in real-world cases based\non reference-free metrics.\n","authors":["Aiwen Jiang","Zhi Wei","Long Peng","Feiqiang Liu","Wenbo Li","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16476v1","updated":"2024-06-24T09:28:21Z","published":"2024-06-24T09:28:21Z","title":"ResMaster: Mastering High-Resolution Image Generation via Structural and\n  Fine-Grained Guidance","summary":"  Diffusion models excel at producing high-quality images; however, scaling to\nhigher resolutions, such as 4K, often results in over-smoothed content,\nstructural distortions, and repetitive patterns. To this end, we introduce\nResMaster, a novel, training-free method that empowers resolution-limited\ndiffusion models to generate high-quality images beyond resolution\nrestrictions. Specifically, ResMaster leverages a low-resolution reference\nimage created by a pre-trained diffusion model to provide structural and\nfine-grained guidance for crafting high-resolution images on a patch-by-patch\nbasis. To ensure a coherent global structure, ResMaster meticulously aligns the\nlow-frequency components of high-resolution patches with the low-resolution\nreference at each denoising step. For fine-grained guidance, tailored image\nprompts based on the low-resolution reference and enriched textual prompts\nproduced by a vision-language model are incorporated. This approach could\nsignificantly mitigate local pattern distortions and improve detail refinement.\nExtensive experiments validate that ResMaster sets a new benchmark for\nhigh-resolution image generation and demonstrates promising efficiency. The\nproject page is https://shuweis.github.io/ResMaster .\n","authors":["Shuwei Shi","Wenbo Li","Yuechen Zhang","Jingwen He","Biao Gong","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.16476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16473v1","updated":"2024-06-24T09:25:02Z","published":"2024-06-24T09:25:02Z","title":"Seeking Certainty In Uncertainty: Dual-Stage Unified Framework Solving\n  Uncertainty in Dynamic Facial Expression Recognition","summary":"  The contemporary state-of-the-art of Dynamic Facial Expression Recognition\n(DFER) technology facilitates remarkable progress by deriving emotional\nmappings of facial expressions from video content, underpinned by training on\nvoluminous datasets. Yet, the DFER datasets encompass a substantial volume of\nnoise data. Noise arises from low-quality captures that defy logical labeling,\nand instances that suffer from mislabeling due to annotation bias, engendering\ntwo principal types of uncertainty: the uncertainty regarding data usability\nand the uncertainty concerning label reliability. Addressing the two types of\nuncertainty, we have meticulously crafted a two-stage framework aiming at\n\\textbf{S}eeking \\textbf{C}ertain data \\textbf{I}n extensive \\textbf{U}ncertain\ndata (SCIU). This initiative aims to purge the DFER datasets of these\nuncertainties, thereby ensuring that only clean, verified data is employed in\ntraining processes. To mitigate the issue of low-quality samples, we introduce\nthe Coarse-Grained Pruning (CGP) stage, which assesses sample weights and\nprunes those deemed unusable due to their low weight. For samples with\nincorrect annotations, the Fine-Grained Correction (FGC) stage evaluates\nprediction stability to rectify mislabeled data. Moreover, SCIU is conceived as\na universally compatible, plug-and-play framework, tailored to integrate\nseamlessly with prevailing DFER methodologies. Rigorous experiments across\nprevalent DFER datasets and against numerous benchmark methods substantiates\nSCIU's capacity to markedly elevate performance metrics.\n","authors":["Haoran Wang","Xinji Mai","Zeng Tao","Xuan Tong","Junxiong Lin","Yan Wang","Jiawen Yu","Boyang Wang","Shaoqi Yan","Qing Zhao","Ziheng Zhou","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16469v1","updated":"2024-06-24T09:18:15Z","published":"2024-06-24T09:18:15Z","title":"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration","summary":"  To create culturally inclusive vision-language models (VLMs), the foremost\nrequirement is developing a test benchmark that can diagnose the models'\nability to respond to questions reflecting cultural elements. This paper\naddresses the necessity for such benchmarks, noting that existing research has\nrelied on human annotators' manual efforts, which impedes diversity and\nefficiency. We propose a semi-automated pipeline for constructing cultural VLM\nbenchmarks to enhance diversity and efficiency. This pipeline leverages\nhuman-VLM collaboration, where VLMs generate questions based on guidelines,\nhuman-annotated examples, and image-wise relevant knowledge, which are then\nreviewed by native speakers for quality and cultural relevance. The\neffectiveness of our adaptable pipeline is demonstrated through a specific\napplication: creating a dataset tailored to Korean culture, dubbed K-Viscuit.\nThe resulting benchmark features two types of questions: Type 1 questions\nmeasure visual recognition abilities, while Type 2 assess fine-grained visual\nreasoning skills. This ensures a thorough diagnosis of VLM models across\nvarious aspects. Our evaluation using K-Viscuit revealed that open-source\nmodels notably lag behind proprietary models in understanding Korean culture,\nhighlighting areas for improvement. We provided diverse analyses of VLM\nperformance across different cultural aspects. Besides, we explored the\npotential of incorporating external knowledge retrieval to enhance the\ngeneration process, suggesting future directions for improving cultural\ninterpretation ability of VLMs. Our dataset and code will be made publicly\navailable.\n","authors":["Yujin Baek","ChaeHun Park","Jaeseok Kim","Yu-Jung Heo","Du-Seong Chang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.16469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16466v1","updated":"2024-06-24T09:16:17Z","published":"2024-06-24T09:16:17Z","title":"SLOctolyzer: Fully automatic analysis toolkit for segmentation and\n  feature extracting in scanning laser ophthalmoscopy images","summary":"  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face\nretinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy\n(SLO) images.\n  Methods: SLOctolyzer includes two main modules: segmentation and measurement.\nThe segmentation module use deep learning methods to delineate retinal anatomy,\nwhile the measurement module quantifies key retinal vascular features such as\nvessel complexity, density, tortuosity, and calibre. We evaluate the\nsegmentation module using unseen data and measure its reproducibility.\n  Results: SLOctolyzer's segmentation module performed well against unseen\ninternal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,\n0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe\nretinal pathology showed decreased performance (Dice for arteries, 0.7180;\nveins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean\ndifference for fractal dimension, -0.0007; vessel density, -0.0003; vessel\ncalibre, -0.3154 $\\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a\nmacula-centred SLO image in under 20 seconds and a disc-centred SLO image in\nunder 30 seconds using a standard laptop CPU.\n  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to\nconvert raw SLO images into reproducible and clinically meaningful retinal\nvascular parameters. SLO images are captured simultaneous to optical coherence\ntomography (OCT), and we believe our software will be useful for extracting\nretinal vascular measurements from large OCT image sets and linking them to\nocular or systemic diseases. It requires no specialist knowledge or proprietary\nsoftware, and allows manual correction of segmentations and re-computing of\nvascular metrics. SLOctolyzer is freely available at\nhttps://github.com/jaburke166/SLOctolyzer.\n","authors":["Jamie Burke","Samuel Gibbon","Justin Engelmann","Adam Threlfall","Ylenia Giarratano","Charlene Hamid","Stuart King","Ian J. C. MacCormick","Tom MacGillivray"],"pdf_url":"https://arxiv.org/pdf/2406.16466v1.pdf","comment":"10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4\n  tables). Submitted for peer review at Translational Vision Science and\n  Technology"},{"id":"http://arxiv.org/abs/2406.16464v1","updated":"2024-06-24T09:13:42Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Current multi-modal sarcasm detection methods have been\nproven to struggle with biases from spurious cues, leading to a superficial\nunderstanding of the complex interactions between text and image. To address\nthese issues, we propose InterCLIP-MEP, a robust framework for multi-modal\nsarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,\nInteractive CLIP (InterCLIP), as the backbone, enhancing sample representations\nby embedding cross-modality information in each encoder. Furthermore, a novel\ntraining strategy is designed to adapt InterCLIP for a Memory-Enhanced\nPredictor (MEP). MEP uses dynamic dual-channel memory to store valuable\nhistorical knowledge of test samples and then leverages this memory as a\nnon-parametric classifier to derive the final prediction. By using InterCLIP to\nencode text-image interactions more effectively and incorporating MEP,\nInterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.\nExperiments demonstrate that InterCLIP-MEP achieves state-of-the-art\nperformance on the MMSD2.0 benchmark. Code and data are available at\n[https://github.com/CoderChen01/InterCLIP-MEP](https://github.com/CoderChen01/InterCLIP-MEP).\n","authors":["Junjie Chen","Subin Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v1.pdf","comment":"8 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2312.10251v3","updated":"2024-06-24T09:07:33Z","published":"2023-12-15T22:50:12Z","title":"Advancing Surgical VQA with Scene Graph Knowledge","summary":"  Modern operating room is becoming increasingly complex, requiring innovative\nintra-operative support systems. While the focus of surgical data science has\nlargely been on video analysis, integrating surgical computer vision with\nlanguage capabilities is emerging as a necessity. Our work aims to advance\nVisual Question Answering (VQA) in the surgical context with scene graph\nknowledge, addressing two main challenges in the current surgical VQA systems:\nremoving question-condition bias in the surgical VQA dataset and incorporating\nscene-aware reasoning in the surgical VQA model design. First, we propose a\nSurgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation\nand detection models on publicly available datasets. We build surgical scene\ngraphs using spatial and action information of instruments and anatomies. These\ngraphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA\ndataset provides a more complex, diverse, geometrically grounded, unbiased, and\nsurgical action-oriented dataset compared to existing surgical VQA datasets. We\nthen propose SSG-QA-Net, a novel surgical VQA model incorporating a lightweight\nScene-embedded Interaction Module (SIM), which integrates geometric scene\nknowledge in the VQA model design by employing cross-attention between the\ntextual and the scene features. Our comprehensive analysis of the SSG-QA\ndataset shows that SSG-QA-Net outperforms existing methods across different\nquestion types and complexities. We highlight that the primary limitation in\nthe current surgical VQA systems is the lack of scene knowledge to answer\ncomplex queries. We present a novel surgical VQA dataset and model and show\nthat results can be significantly improved by incorporating geometric scene\nfeatures in the VQA model design. The source code and the dataset will be made\npublicly available at: https://github.com/CAMMA-public/SSG-QA\n","authors":["Kun Yuan","Manasi Kattel","Joel L. Lavanchy","Nassir Navab","Vinkle Srivastav","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2312.10251v3.pdf","comment":"IPCAI 2024, Int J CARS (2024)"},{"id":"http://arxiv.org/abs/2406.16459v1","updated":"2024-06-24T08:58:43Z","published":"2024-06-24T08:58:43Z","title":"Suppressing Uncertainties in Degradation Estimation for Blind\n  Super-Resolution","summary":"  The problem of blind image super-resolution aims to recover high-resolution\n(HR) images from low-resolution (LR) images with unknown degradation modes.\nMost existing methods model the image degradation process using blur kernels.\nHowever, this explicit modeling approach struggles to cover the complex and\nvaried degradation processes encountered in the real world, such as high-order\ncombinations of JPEG compression, blur, and noise. Implicit modeling for the\ndegradation process can effectively overcome this issue, but a key challenge of\nimplicit modeling is the lack of accurate ground truth labels for the\ndegradation process to conduct supervised training. To overcome this\nlimitations inherent in implicit modeling, we propose an\n\\textbf{U}ncertainty-based degradation representation for blind\n\\textbf{S}uper-\\textbf{R}esolution framework (\\textbf{USR}). By suppressing the\nuncertainty of local degradation representations in images, USR facilitated\nself-supervised learning of degradation representations. The USR consists of\ntwo components: Adaptive Uncertainty-Aware Degradation Extraction (AUDE) and a\nfeature extraction network composed of Variable Depth Dynamic Convolution\n(VDDC) blocks. To extract Uncertainty-based Degradation Representation from LR\nimages, the AUDE utilizes the Self-supervised Uncertainty Contrast module with\nUncertainty Suppression Loss to suppress the inherent model uncertainty of the\nDegradation Extractor. Furthermore, VDDC block integrates degradation\ninformation through dynamic convolution. Rhe VDDC also employs an Adaptive\nIntensity Scaling operation that adaptively adjusts the degradation\nrepresentation according to the network hierarchy, thereby facilitating the\neffective integration of degradation information. Quantitative and qualitative\nexperiments affirm the superiority of our approach.\n","authors":["Junxiong Lin","Zeng Tao","Xuan Tong","Xinji Mai","Haoran Wang","Boyang Wang","Yan Wang","Qing Zhao","Jiawen Yu","Yuxuan Lin","Shaoqi Yan","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16449v1","updated":"2024-06-24T08:42:42Z","published":"2024-06-24T08:42:42Z","title":"Evaluating and Analyzing Relationship Hallucinations in LVLMs","summary":"  The issue of hallucinations is a prevalent concern in existing Large\nVision-Language Models (LVLMs). Previous efforts have primarily focused on\ninvestigating object hallucinations, which can be easily alleviated by\nintroducing object detectors. However, these efforts neglect hallucinations in\ninter-object relationships, which is essential for visual comprehension. In\nthis work, we introduce R-Bench, a novel benchmark for evaluating Vision\nRelationship Hallucination. R-Bench features image-level questions that focus\non the existence of relationships and instance-level questions that assess\nlocal visual comprehension. We identify three types of relationship\nco-occurrences that lead to hallucinations: relationship-relationship,\nsubject-relationship, and relationship-object. The visual instruction tuning\ndataset's long-tail distribution significantly impacts LVLMs' understanding of\nvisual relationships. Furthermore, our analysis reveals that current LVLMs tend\nto disregard visual content and overly rely on the common sense knowledge of\nLarge Language Models. They also struggle with reasoning about spatial\nrelationships based on contextual information.\n","authors":["Mingrui Wu","Jiayi Ji","Oucheng Huang","Jiale Li","Yuhang Wu","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.16449v1.pdf","comment":"ICML2024"},{"id":"http://arxiv.org/abs/2406.16442v1","updated":"2024-06-24T08:33:02Z","published":"2024-06-24T08:33:02Z","title":"EmoLLM: Multimodal Emotional Understanding Meets Large Language Models","summary":"  Multi-modal large language models (MLLMs) have achieved remarkable\nperformance on objective multimodal perception tasks, but their ability to\ninterpret subjective, emotionally nuanced multimodal content remains largely\nunexplored. Thus, it impedes their ability to effectively understand and react\nto the intricate emotions expressed by humans through multimodal media. To\nbridge this gap, we introduce EmoBench, the first comprehensive benchmark\ndesigned specifically to evaluate the emotional capabilities of MLLMs across\nfive popular emotional tasks, using a diverse dataset of 287k images and videos\npaired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a\nnovel model for multimodal emotional understanding, incorporating with two core\ntechniques. 1) Multi-perspective Visual Projection, it captures diverse\nemotional cues from visual data from multiple perspectives. 2) EmoPrompt, it\nguides MLLMs to reason about emotions in the correct direction. Experimental\nresults demonstrate that EmoLLM significantly elevates multimodal emotional\nunderstanding performance, with an average improvement of 12.1% across multiple\nfoundation models on EmoBench. Our work contributes to the advancement of MLLMs\nby facilitating a deeper and more nuanced comprehension of intricate human\nemotions, paving the way for the development of artificial emotional\nintelligence capabilities with wide-ranging applications in areas such as\nhuman-computer interaction, mental health support, and empathetic AI systems.\nCode, data, and model will be released.\n","authors":["Qu Yang","Mang Ye","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2406.16442v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.16439v1","updated":"2024-06-24T08:30:03Z","published":"2024-06-24T08:30:03Z","title":"Exploring Test-Time Adaptation for Object Detection in Continually\n  Changing Environments","summary":"  For real-world applications, neural network models are commonly deployed in\ndynamic environments, where the distribution of the target domain undergoes\ntemporal changes. Continual Test-Time Adaptation (CTTA) has recently emerged as\na promising technique to gradually adapt a source-trained model to test data\ndrawn from a continually changing target domain. Despite recent advancements in\naddressing CTTA, two critical issues remain: 1) The use of a fixed threshold\nfor pseudo-labeling in existing methodologies leads to the generation of\nlow-quality pseudo-labels, as model confidence varies across categories and\ndomains; 2) While current solutions utilize stochastic parameter restoration to\nmitigate catastrophic forgetting, their capacity to preserve critical\ninformation is undermined by its intrinsic randomness. To tackle these\nchallenges, we present CTAOD, aiming to enhance the performance of detection\nmodels in CTTA scenarios. Inspired by prior CTTA works for effective\nadaptation, CTAOD is founded on the mean-teacher framework, characterized by\nthree core components. Firstly, the object-level contrastive learning module\ntailored for object detection extracts object-level features using the\nteacher's region of interest features and optimizes them through contrastive\nlearning. Secondly, the dynamic threshold strategy updates the\ncategory-specific threshold based on predicted confidence scores to improve the\nquality of pseudo-labels. Lastly, we design a data-driven stochastic\nrestoration mechanism to selectively reset inactive parameters using the\ngradients as weights for a random mask matrix, thereby ensuring the retention\nof essential knowledge. We demonstrate the effectiveness of our approach on\nfour CTTA tasks for object detection, where CTAOD outperforms existing methods,\nespecially achieving a 3.0 mAP improvement on the Cityscapes-to-Cityscapes-C\nCTTA task.\n","authors":["Shilei Cao","Yan Liu","Juepeng Zheng","Weijia Li","Runmin Dong","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2406.16439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16434v1","updated":"2024-06-24T08:27:31Z","published":"2024-06-24T08:27:31Z","title":"Multi-threshold Deep Metric Learning for Facial Expression Recognition","summary":"  Effective expression feature representations generated by a triplet-based\ndeep metric learning are highly advantageous for facial expression recognition\n(FER). The performance of triplet-based deep metric learning is contingent upon\nidentifying the best threshold for triplet loss. Threshold validation, however,\nis tough and challenging, as the ideal threshold changes among datasets and\neven across classes within the same dataset. In this paper, we present the\nmulti-threshold deep metric learning technique, which not only avoids the\ndifficult threshold validation but also vastly increases the capacity of\ntriplet loss learning to construct expression feature representations. We find\nthat each threshold of the triplet loss intrinsically determines a distinctive\ndistribution of inter-class variations and corresponds, thus, to a unique\nexpression feature representation. Therefore, rather than selecting a single\noptimal threshold from a valid threshold range, we thoroughly sample thresholds\nacross the range, allowing the representation characteristics manifested by\nthresholds within the range to be fully extracted and leveraged for FER. To\nrealize this approach, we partition the embedding layer of the deep metric\nlearning network into a collection of slices and model training these embedding\nslices as an end-to-end multi-threshold deep metric learning problem. Each\nembedding slice corresponds to a sample threshold and is learned by enforcing\nthe corresponding triplet loss, yielding a set of distinct expression features,\none for each embedding slice. It makes the embedding layer, which is composed\nof a set of slices, a more informative and discriminative feature, hence\nenhancing the FER accuracy. Extensive evaluations demonstrate the superior\nperformance of the proposed approach on both posed and spontaneous facial\nexpression datasets.\n","authors":["Wenwu Yang","Jinyi Yu","Tuo Chen","Zhenguang Liu","Xun Wang","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2406.16434v1.pdf","comment":"accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2406.13210v2","updated":"2024-06-24T08:22:40Z","published":"2024-06-19T04:43:41Z","title":"Surgical Triplet Recognition via Diffusion Model","summary":"  Surgical triplet recognition is an essential building block to enable\nnext-generation context-aware operating rooms. The goal is to identify the\ncombinations of instruments, verbs, and targets presented in surgical video\nframes. In this paper, we propose DiffTriplet, a new generative framework for\nsurgical triplet recognition employing the diffusion model, which predicts\nsurgical triplets via iterative denoising. To handle the challenge of triplet\nassociation, two unique designs are proposed in our diffusion framework, i.e.,\nassociation learning and association guidance. During training, we optimize the\nmodel in the joint space of triplets and individual components to capture the\ndependencies among them. At inference, we integrate association constraints\ninto each update of the iterative denoising process, which refines the triplet\nprediction using the information of individual components. Experiments on the\nCholecT45 and CholecT50 datasets show the superiority of the proposed method in\nachieving a new state-of-the-art performance for surgical triplet recognition.\nOur codes will be released.\n","authors":["Daochang Liu","Axel Hu","Mubarak Shah","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2406.13210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16427v1","updated":"2024-06-24T08:20:53Z","published":"2024-06-24T08:20:53Z","title":"Dynamic Pseudo Label Optimization in Point-Supervised Nuclei\n  Segmentation","summary":"  Deep learning has achieved impressive results in nuclei segmentation, but the\nmassive requirement for pixel-wise labels remains a significant challenge. To\nalleviate the annotation burden, existing methods generate pseudo masks for\nmodel training using point labels. However, the generated masks are inevitably\ndifferent from the ground truth, and these dissimilarities are not handled\nreasonably during the network training, resulting in the subpar performance of\nthe segmentation model. To tackle this issue, we propose a framework named\nDoNuSeg, enabling \\textbf{D}ynamic pseudo label \\textbf{O}ptimization in\npoint-supervised \\textbf{Nu}clei \\textbf{Seg}mentation. Specifically, DoNuSeg\ntakes advantage of class activation maps (CAMs) to adaptively capture regions\nwith semantics similar to annotated points. To leverage semantic diversity in\nthe hierarchical feature levels, we design a dynamic selection module to choose\nthe optimal one among CAMs from different encoder blocks as pseudo masks.\nMeanwhile, a CAM-guided contrastive module is proposed to further enhance the\naccuracy of pseudo masks. In addition to exploiting the semantic information\nprovided by CAMs, we consider location priors inherent to point labels,\ndeveloping a task-decoupled structure for effectively differentiating nuclei.\nExtensive experiments demonstrate that DoNuSeg outperforms state-of-the-art\npoint-supervised methods. The code is available at\nhttps://github.com/shinning0821/MICCAI24-DoNuSeg.\n","authors":["Ziyue Wang","Ye Zhang","Yifeng Wang","Linghan Cai","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16427v1.pdf","comment":"early accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2406.15111v2","updated":"2024-06-24T08:19:00Z","published":"2024-06-21T12:59:20Z","title":"Investigating the impact of 2D gesture representation on co-speech\n  gesture generation","summary":"  Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.\n","authors":["Teo Guichoux","Laure Soulier","Nicolas Obin","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2406.15111v2.pdf","comment":"8 pages. Paper accepted at WACAI 2024"},{"id":"http://arxiv.org/abs/2403.04161v5","updated":"2024-06-24T08:18:29Z","published":"2024-03-07T02:40:42Z","title":"SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS","summary":"  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid\nresource-intensive neural network training, especially in Neural Architecture\nSearch (NAS). Recent studies show that existing training-free metrics have\nseveral limitations, such as limited correlation and poor generalisation across\ndifferent search spaces and tasks. Hence, we propose Sample-Wise Activation\nPatterns and its derivative, SWAP-Score, a novel high-performance training-free\nmetric. It measures the expressivity of networks over a batch of input samples.\nThe SWAP-Score is strongly correlated with ground-truth performance across\nvarious search spaces and tasks, outperforming 15 existing training-free\nmetrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be\nfurther enhanced by regularisation, which leads to even higher correlations in\ncell-based search space and enables model size control during the search. For\nexample, Spearman's rank correlation coefficient between regularised SWAP-Score\nand CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,\nsignificantly higher than 0.80 from the second-best metric, NWOT. When\nintegrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves\ncompetitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and\n9 minutes of GPU time respectively.\n","authors":["Yameng Peng","Andy Song","Haytham M. Fayek","Vic Ciesielski","Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2403.04161v5.pdf","comment":"ICLR2024 Spotlight"},{"id":"http://arxiv.org/abs/2406.16422v1","updated":"2024-06-24T08:14:09Z","published":"2024-06-24T08:14:09Z","title":"Exploring Cross-Domain Few-Shot Classification via Frequency-Aware\n  Prompting","summary":"  Cross-Domain Few-Shot Learning has witnessed great stride with the\ndevelopment of meta-learning. However, most existing methods pay more attention\nto learning domain-adaptive inductive bias (meta-knowledge) through\nfeature-wise manipulation or task diversity improvement while neglecting the\nphenomenon that deep networks tend to rely more on high-frequency cues to make\nthe classification decision, which thus degenerates the robustness of learned\ninductive bias since high-frequency information is vulnerable and easy to be\ndisturbed by noisy information. Hence in this paper, we make one of the first\nattempts to propose a Frequency-Aware Prompting method with mutual attention\nfor Cross-Domain Few-Shot classification, which can let networks simulate the\nhuman visual perception of selecting different frequency cues when facing new\nrecognition tasks. Specifically, a frequency-aware prompting mechanism is first\nproposed, in which high-frequency components of the decomposed source image are\nswitched either with normal distribution sampling or zeroing to get\nfrequency-aware augment samples. Then, a mutual attention module is designed to\nlearn generalizable inductive bias under CD-FSL settings. More importantly, the\nproposed method is a plug-and-play module that can be directly applied to most\noff-the-shelf CD-FLS methods. Experimental results on CD-FSL benchmarks\ndemonstrate the effectiveness of our proposed method as well as robustly\nimprove the performance of existing CD-FLS methods. Resources at\nhttps://github.com/tinkez/FAP_CDFSC.\n","authors":["Tiange Zhang","Qing Cai","Feng Gao","Lin Qi","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2406.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16384v1","updated":"2024-06-24T07:53:46Z","published":"2024-06-24T07:53:46Z","title":"High-resolution open-vocabulary object 6D pose estimation","summary":"  The generalisation to unseen objects in the 6D pose estimation task is very\nchallenging. While Vision-Language Models (VLMs) enable using natural language\ndescriptions to support 6D pose estimation of unseen objects, these solutions\nunderperform compared to model-based methods. In this work we present Horyon,\nan open-vocabulary VLM-based architecture that addresses relative pose\nestimation between two scenes of an unseen object, described by a textual\nprompt only. We use the textual prompt to identify the unseen object in the\nscenes and then obtain high-resolution multi-scale features. These features are\nused to extract cross-scene matches for registration. We evaluate our model on\na benchmark with a large variety of unseen objects across four datasets, namely\nREAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves\nstate-of-the-art performance on all datasets, outperforming by 12.6 in Average\nRecall the previous best-performing approach.\n","authors":["Jaime Corsetti","Davide Boscaini","Francesco Giuliari","Changjae Oh","Andrea Cavallaro","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2406.16384v1.pdf","comment":"Technical report. Extension of CVPR paper \"Open-vocabulary object 6D\n  pose estimation\". Project page: https://jcorsetti.github.io/oryon"},{"id":"http://arxiv.org/abs/2406.06462v2","updated":"2024-06-24T07:05:01Z","published":"2024-06-10T16:58:48Z","title":"VCR: Visual Caption Restoration","summary":"  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n","authors":["Tianyu Zhang","Suyuchen Wang","Lu Li","Ge Zhang","Perouz Taslakian","Sai Rajeswar","Jie Fu","Bang Liu","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2406.06462v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.16360v1","updated":"2024-06-24T07:00:57Z","published":"2024-06-24T07:00:57Z","title":"MIRReS: Multi-bounce Inverse Rendering using Reservoir Sampling","summary":"  We present MIRReS, a novel two-stage inverse rendering framework that jointly\nreconstructs and optimizes the explicit geometry, material, and lighting from\nmulti-view images. Unlike previous methods that rely on implicit irradiance\nfields or simplified path tracing algorithms, our method extracts an explicit\ngeometry (triangular mesh) in stage one, and introduces a more realistic\nphysically-based inverse rendering model that utilizes multi-bounce path\ntracing and Monte Carlo integration. By leveraging multi-bounce path tracing,\nour method effectively estimates indirect illumination, including\nself-shadowing and internal reflections, which improves the intrinsic\ndecomposition of shape, material, and lighting. Moreover, we incorporate\nreservoir sampling into our framework to address the noise in Monte Carlo\nintegration, enhancing convergence and facilitating gradient-based optimization\nwith low sample counts. Through qualitative and quantitative evaluation of\nseveral scenarios, especially in challenging scenarios with complex shadows, we\ndemonstrate that our method achieves state-of-the-art performance on\ndecomposition results. Additionally, our optimized explicit geometry enables\napplications such as scene editing, relighting, and material editing with\nmodern graphics engines or CAD software. The source code is available at\nhttps://brabbitdousha.github.io/MIRReS/\n","authors":["Yuxin Dai","Qi Wang","Jingsen Zhu","Dianbing Xi","Yuchi Huo","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2406.16360v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.16359v1","updated":"2024-06-24T06:57:51Z","published":"2024-06-24T06:57:51Z","title":"Improving Generative Adversarial Networks for Video Super-Resolution","summary":"  In this research, we explore different ways to improve generative adversarial\nnetworks for video super-resolution tasks from a base single image\nsuper-resolution GAN model. Our primary objective is to identify potential\ntechniques that enhance these models and to analyze which of these techniques\nyield the most significant improvements. We evaluate our results using Peak\nSignal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our\nfindings indicate that the most effective techniques include temporal\nsmoothing, long short-term memory (LSTM) layers, and a temporal loss function.\nThe integration of these methods results in an 11.97% improvement in PSNR and\nan 8% improvement in SSIM compared to the baseline video super-resolution\ngenerative adversarial network (GAN) model. This substantial improvement\nsuggests potential further applications to enhance current state-of-the-art\nmodels.\n","authors":["Daniel Wen"],"pdf_url":"https://arxiv.org/pdf/2406.16359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16346v1","updated":"2024-06-24T06:39:02Z","published":"2024-06-24T06:39:02Z","title":"Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific\n  Training Tasks","summary":"  Large language models (LLMs) and large visual language models (LVLMs) have\nbeen at the forefront of the artificial intelligence field, particularly for\ntasks like text generation, video captioning, and question-answering.\nTypically, it is more applicable to train these models on broader knowledge\nbases or datasets to increase generalizability, learn relationships between\ntopics, and recognize patterns. Instead, we propose to provide instructional\ndatasets specific to the task of each modality within a distinct domain and\nthen fine-tune the parameters of the model using LORA. With our approach, we\ncan eliminate all noise irrelevant to the given task while also ensuring that\nthe model generates with enhanced precision. For this work, we use Video-LLaVA\nto generate recipes given cooking videos without transcripts. Video-LLaVA's\nmultimodal architecture allows us to provide cooking images to its image\nencoder, cooking videos to its video encoder, and general cooking questions to\nits text encoder. Thus, we aim to remove all noise unrelated to cooking while\nimproving our model's capabilities to generate specific ingredient lists and\ndetailed instructions. As a result, our approach to fine-tuning Video-LLaVA\nleads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset.\nWhile this may seem like a marginal increase, our model trains on an image\ninstruction dataset 2.5% the size of Video-LLaVA's and a video instruction\ndataset 23.76% of Video-LLaVA's.\n","authors":["Daniel Wen","Nafisa Hussain"],"pdf_url":"https://arxiv.org/pdf/2406.16346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01368v3","updated":"2024-06-24T06:27:44Z","published":"2024-02-02T12:39:47Z","title":"LIR: A Lightweight Baseline for Image Restoration","summary":"  Recently, there have been significant advancements in Image Restoration based\non CNN and transformer. However, the inherent characteristics of the Image\nRestoration task are often overlooked in many works. They, instead, tend to\nfocus on the basic block design and stack numerous such blocks to the model,\nleading to parameters redundant and computations unnecessary. Thus, the\nefficiency of the image restoration is hindered. In this paper, we propose a\nLightweight Baseline network for Image Restoration called LIR to efficiently\nrestore the image and remove degradations. First of all, through an ingenious\nstructural design, LIR removes the degradations existing in the local and\nglobal residual connections that are ignored by modern networks. Then, a\nLightweight Adaptive Attention (LAA) Block is introduced which is mainly\ncomposed of proposed Adaptive Filters and Attention Blocks. The proposed\nAdaptive Filter is used to adaptively extract high-frequency information and\nenhance object contours in various IR tasks, and Attention Block involves a\nnovel Patch Attention module to approximate the self-attention part of the\ntransformer. On the deraining task, our LIR achieves the state-of-the-art\nStructure Similarity Index Measure (SSIM) and comparable performance to\nstate-of-the-art models on Peak Signal-to-Noise Ratio (PSNR). For denoising,\ndehazing, and deblurring tasks, LIR also achieves a comparable performance to\nstate-of-the-art models with a parameter size of about 30\\%. In addition, it is\nworth noting that our LIR produces better visual results that are more in line\nwith the human aesthetic.\n","authors":["Dongqi Fan","Ting Yue","Xin Zhao","Renjing Xu","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2402.01368v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16338v1","updated":"2024-06-24T06:21:59Z","published":"2024-06-24T06:21:59Z","title":"VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in\n  Large Video-Language Models","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs) have extended\ntheir capabilities to video understanding. Yet, these models are often plagued\nby \"hallucinations\", where irrelevant or nonsensical content is generated,\ndeviating from the actual video context. This work introduces VideoHallucer,\nthe first comprehensive benchmark for hallucination detection in large\nvideo-language models (LVLMs). VideoHallucer categorizes hallucinations into\ntwo main types: intrinsic and extrinsic, offering further subcategories for\ndetailed analysis, including object-relation, temporal, semantic detail,\nextrinsic factual, and extrinsic non-factual hallucinations. We adopt an\nadversarial binary VideoQA method for comprehensive evaluation, where pairs of\nbasic and hallucinated questions are crafted strategically. By evaluating\neleven LVLMs on VideoHallucer, we reveal that i) the majority of current models\nexhibit significant issues with hallucinations; ii) while scaling datasets and\nparameters improves models' ability to detect basic visual cues and\ncounterfactuals, it provides limited benefit for detecting extrinsic factual\nhallucinations; iii) existing models are more adept at detecting facts than\nidentifying hallucinations. As a byproduct, these analyses further instruct the\ndevelopment of our self-PEP framework, achieving an average of 5.38%\nimprovement in hallucination resistance across all model architectures.\n","authors":["Yuxuan Wang","Yueqian Wang","Dongyan Zhao","Cihang Xie","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.16338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16333v1","updated":"2024-06-24T06:12:16Z","published":"2024-06-24T06:12:16Z","title":"Prompt-Consistency Image Generation (PCIG): A Unified Framework\n  Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models","summary":"  The rapid advancement of Text-to-Image(T2I) generative models has enabled the\nsynthesis of high-quality images guided by textual descriptions. Despite this\nsignificant progress, these models are often susceptible in generating contents\nthat contradict the input text, which poses a challenge to their reliability\nand practical deployment. To address this problem, we introduce a novel\ndiffusion-based framework to significantly enhance the alignment of generated\nimages with their corresponding descriptions, addressing the inconsistency\nbetween visual output and textual input. Our framework is built upon a\ncomprehensive analysis of inconsistency phenomena, categorizing them based on\ntheir manifestation in the image. Leveraging a state-of-the-art large language\nmodule, we first extract objects and construct a knowledge graph to predict the\nlocations of these objects in potentially generated images. We then integrate a\nstate-of-the-art controllable image generation model with a visual text\ngeneration module to generate an image that is consistent with the original\nprompt, guided by the predicted object locations. Through extensive experiments\non an advanced multimodal hallucination benchmark, we demonstrate the efficacy\nof our approach in accurately generating the images without the inconsistency\nwith the original prompt. The code can be accessed via\nhttps://github.com/TruthAI-Lab/PCIG.\n","authors":["Yichen Sun","Zhixuan Chu","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2406.16333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08336v2","updated":"2024-06-24T06:09:42Z","published":"2024-06-12T15:42:21Z","title":"CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal\n  Dysarthric Speech Reconstruction","summary":"  Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech\ninto normal speech. It still suffers from low speaker similarity and poor\nprosody naturalness. In this paper, we propose a multi-modal DSR model by\nleveraging neural codec language modeling to improve the reconstruction\nresults, especially for the speaker similarity and prosody naturalness. Our\nproposed model consists of: (i) a multi-modal content encoder to extract robust\nphoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a\nspeaker codec encoder to extract and normalize the speaker-aware codecs from\nthe dysarthric speech, in order to provide original timbre and normal prosody;\n(iii) a codec language model based speech decoder to reconstruct the speech\nbased on the extracted phoneme embeddings and normalized codecs. Evaluations on\nthe commonly used UASpeech corpus show that our proposed model can achieve\nsignificant improvements in terms of speaker similarity and prosody\nnaturalness.\n","authors":["Xueyuan Chen","Dongchao Yang","Dingdong Wang","Xixin Wu","Zhiyong Wu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2406.08336v2.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.05779v2","updated":"2024-06-24T06:07:14Z","published":"2024-06-09T13:25:02Z","title":"Learning to utilize image second-order derivative information for crisp\n  edge detection","summary":"  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) leveraging the\nprecise edge pixel location characteristics of second-order image derivatives,\nand (2) alleviating the issue of imbalanced pixel distribution. We propose a\nsecond-order derivative-based multi-scale contextual enhancement module (SDMC)\nto help the model locate true edge pixels accurately and construct a hybrid\nfocal loss function (HFL) to alleviate the imbalanced distribution issue. We\ntest our method on three standard benchmarks and the experiment results\nillustrate that our method can make the output edge maps crisp and achieves a\ntop performance among several state-of-the-art methods on the BSDS500 dataset\n(ODS F-score in standard evaluation is 0.829, in crispness evaluation is\n0.720), NYUD-V2 dataset (ODS F-score in standard evaluation is 0.768, in\ncrispness evaluation is 0.546), and BIPED dataset (ODS F-score in standard\nevaluation is 0.903).\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Yuming Li","Mingyang Li","Wenlin Li","Yimeng Fan","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.05779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13393v2","updated":"2024-06-24T06:04:23Z","published":"2024-06-19T09:36:18Z","title":"Style-NeRF2NeRF: 3D Style Transfer From Style-Aligned Multi-View Images","summary":"  We propose a simple yet effective pipeline for stylizing a 3D scene,\nharnessing the power of 2D image diffusion models. Given a NeRF model\nreconstructed from a set of multi-view images, we perform 3D style transfer by\nrefining the source NeRF model using stylized images generated by a\nstyle-aligned image-to-image diffusion model. Given a target style prompt, we\nfirst generate perceptually similar multi-view images by leveraging a\ndepth-conditioned diffusion model with an attention-sharing mechanism. Next,\nbased on the stylized multi-view images, we propose to guide the style transfer\nprocess with the sliced Wasserstein loss based on the feature maps extracted\nfrom a pre-trained CNN model. Our pipeline consists of decoupled steps,\nallowing users to test various prompt ideas and preview the stylized 3D result\nbefore proceeding to the NeRF fine-tuning stage. We demonstrate that our method\ncan transfer diverse artistic styles to real-world 3D scenes with competitive\nquality. Result videos are also available on our project page:\nhttps://haruolabs.github.io/style-n2n/\n","authors":["Haruo Fujiwara","Yusuke Mukuta","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2406.13393v2.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.03662v2","updated":"2024-06-24T05:48:24Z","published":"2024-05-06T17:39:53Z","title":"Diffeomorphic Template Registration for Atmospheric Turbulence\n  Mitigation","summary":"  We describe a method for recovering the irradiance underlying a collection of\nimages corrupted by atmospheric turbulence. Since supervised data is often\ntechnically impossible to obtain, assumptions and biases have to be imposed to\nsolve this inverse problem, and we choose to model them explicitly. Rather than\ninitializing a latent irradiance (\"template\") by heuristics to estimate\ndeformation, we select one of the images as a reference, and model the\ndeformation in this image by the aggregation of the optical flow from it to\nother images, exploiting a prior imposed by Central Limit Theorem. Then with a\nnovel flow inversion module, the model registers each image TO the template but\nWITHOUT the template, avoiding artifacts related to poor template\ninitialization. To illustrate the robustness of the method, we simply (i)\nselect the first frame as the reference and (ii) use the simplest optical flow\nto estimate the warpings, yet the improvement in registration is decisive in\nthe final reconstruction, as we achieve state-of-the-art performance despite\nits simplicity. The method establishes a strong baseline that can be further\nimproved by integrating it seamlessly into more sophisticated pipelines, or\nwith domain-specific methods if so desired.\n","authors":["Dong Lao","Congli Wang","Alex Wong","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2405.03662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11494v2","updated":"2024-06-24T05:29:10Z","published":"2024-03-18T05:58:13Z","title":"CCC++: Optimized Color Classified Colorization with Segment Anything\n  Model (SAM) Empowered Object Selective Color Harmonization","summary":"  In this paper, we formulate the colorization problem into a multinomial\nclassification problem and then apply a weighted function to classes. We\npropose a set of formulas to transform color values into color classes and vice\nversa. To optimize the classes, we experiment with different bin sizes for\ncolor class transformation. Observing class appearance, standard deviation, and\nmodel parameters on various extremely large-scale real-time images in practice\nwe propose 532 color classes for our classification task. During training, we\npropose a class-weighted function based on true class appearance in each batch\nto ensure proper saturation of individual objects. We adjust the weights of the\nmajor classes, which are more frequently observed, by lowering them, while\nescalating the weights of the minor classes, which are less commonly observed.\nIn our class re-weight formula, we propose a hyper-parameter for finding the\noptimal trade-off between the major and minor appeared classes. As we apply\nregularization to enhance the stability of the minor class, occasional minor\nnoise may appear at the object's edges. We propose a novel object-selective\ncolor harmonization method empowered by the Segment Anything Model (SAM) to\nrefine and enhance these edges. We propose two new color image evaluation\nmetrics, the Color Class Activation Ratio (CCAR), and the True Activation Ratio\n(TAR), to quantify the richness of color components. We compare our proposed\nmodel with state-of-the-art models using six different dataset: Place, ADE,\nCeleba, COCO, Oxford 102 Flower, and ImageNet, in qualitative and quantitative\napproaches. The experimental results show that our proposed model outstrips\nother models in visualization, CNR and in our proposed CCAR and TAR measurement\ncriteria while maintaining satisfactory performance in regression (MSE, PSNR),\nsimilarity (SSIM, LPIPS, UIUI), and generative criteria (FID).\n","authors":["Mrityunjoy Gain","Avi Deb Raha","Rameswar Debnath"],"pdf_url":"https://arxiv.org/pdf/2403.11494v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.01476"},{"id":"http://arxiv.org/abs/2406.16322v1","updated":"2024-06-24T05:15:15Z","published":"2024-06-24T05:15:15Z","title":"Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype\n  Classification on Multi-Phase CT Scans","summary":"  Multi-phase computed tomography (CT) has been widely used for the\npreoperative diagnosis of kidney cancer due to its non-invasive nature and\nability to characterize renal lesions. However, since enhancement patterns of\nrenal lesions across CT phases are different even for the same lesion type, the\nvisual assessment by radiologists suffers from inter-observer variability in\nclinical practice. Although deep learning-based approaches have been recently\nexplored for differential diagnosis of kidney cancer, they do not explicitly\nmodel the relationships between CT phases in the network design, limiting the\ndiagnostic performance. In this paper, we propose a novel lesion-aware\ncross-phase attention network (LACPANet) that can effectively capture temporal\ndependencies of renal lesions across CT phases to accurately classify the\nlesions into five major pathological subtypes from time-series multi-phase CT\nimages. We introduce a 3D inter-phase lesion-aware attention mechanism to learn\neffective 3D lesion features that are used to estimate attention weights\ndescribing the inter-phase relations of the enhancement patterns. We also\npresent a multi-scale attention scheme to capture and aggregate temporal\npatterns of lesion features at different spatial scales for further\nimprovement. Extensive experiments on multi-phase CT scans of kidney cancer\npatients from the collected dataset demonstrate that our LACPANet outperforms\nstate-of-the-art approaches in diagnostic accuracy.\n","authors":["Kwang-Hyun Uhm","Seung-Won Jung","Sung-Hoo Hong","Sung-Jea Ko"],"pdf_url":"https://arxiv.org/pdf/2406.16322v1.pdf","comment":"This article has been accepted for publication in Computers in\n  Biology and Medicine"},{"id":"http://arxiv.org/abs/2403.02148v4","updated":"2024-06-24T05:06:56Z","published":"2024-03-04T15:57:29Z","title":"MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection","summary":"  Recently, infrared small target detection (ISTD) has made significant\nprogress, thanks to the development of basic models. Specifically, the models\ncombining CNNs with transformers can successfully extract both local and global\nfeatures. However, the disadvantage of the transformer is also inherited, i.e.,\nthe quadratic computational complexity to sequence length. Inspired by the\nrecent basic model with linear complexity for long-distance modeling, Mamba, we\nexplore the potential of this state space model for ISTD task in terms of\neffectiveness and efficiency in the paper. However, directly applying Mamba\nachieves suboptimal performances due to the insufficient harnessing of local\nfeatures, which are imperative for detecting small targets. Instead, we tailor\na nested structure, Mamba-in-Mamba (MiM-ISTD), for efficient ISTD. It consists\nof Outer and Inner Mamba blocks to adeptly capture both global and local\nfeatures. Specifically, we treat the local patches as \"visual sentences\" and\nuse the Outer Mamba to explore the global information. We then decompose each\nvisual sentence into sub-patches as \"visual words\" and use the Inner Mamba to\nfurther explore the local information among words in the visual sentence with\nnegligible computational costs. By aggregating the visual word and visual\nsentence features, our MiM-ISTD can effectively explore both global and local\ninformation. Experiments on NUAA-SIRST and IRSTD-1k show the superior accuracy\nand efficiency of our method. Specifically, MiM-ISTD is $8 \\times$ faster than\nthe SOTA method and reduces GPU memory usage by 62.2$\\%$ when testing on $2048\n\\times 2048$ images, overcoming the computation and memory constraints on\nhigh-resolution infrared images.\n","authors":["Tianxiang Chen","Zi Ye","Zhentao Tan","Tao Gong","Yue Wu","Qi Chu","Bin Liu","Nenghai Yu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2403.02148v4.pdf","comment":"The first Mamba-based model for infrared small target detection"},{"id":"http://arxiv.org/abs/2403.01422v2","updated":"2024-06-24T04:55:28Z","published":"2024-03-03T07:43:39Z","title":"MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies","summary":"  Development of multimodal models has marked a significant step forward in how\nmachines understand videos. These models have shown promise in analyzing short\nvideo clips. However, when it comes to longer formats like movies, they often\nfall short. The main hurdles are the lack of high-quality, diverse video data\nand the intensive work required to collect or annotate such data. In face of\nthese challenges, we propose MovieLLM, a novel framework designed to synthesize\nconsistent and high-quality video data for instruction tuning. The pipeline is\ncarefully designed to control the style of videos by improving textual\ninversion technique with powerful text generation capability of GPT-4. As the\nfirst framework to do such thing, our approach stands out for its flexibility\nand scalability, empowering users to create customized movies with only one\ndescription. This makes it a superior alternative to traditional data\ncollection methods. Our extensive experiments validate that the data produced\nby MovieLLM significantly improves the performance of multimodal models in\nunderstanding complex video narratives, overcoming the limitations of existing\ndatasets regarding scarcity and bias.\n","authors":["Zhende Song","Chenchen Wang","Jiamu Sheng","Chi Zhang","Gang Yu","Jiayuan Fan","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06872v5","updated":"2024-06-24T04:47:38Z","published":"2022-12-13T19:38:13Z","title":"Comparing the Decision-Making Mechanisms by Transformers and CNNs via\n  Explanation Methods","summary":"  In order to gain insights about the decision-making of different visual\nrecognition backbones, we propose two methodologies, sub-explanation counting\nand cross-testing, that systematically applies deep explanation algorithms on a\ndataset-wide basis, and compares the statistics generated from the amount and\nnature of the explanations. These methodologies reveal the difference among\nnetworks in terms of two properties called compositionality and disjunctivism.\nTransformers and ConvNeXt are found to be more compositional, in the sense that\nthey jointly consider multiple parts of the image in building their decisions,\nwhereas traditional CNNs and distilled transformers are less compositional and\nmore disjunctive, which means that they use multiple diverse but smaller set of\nparts to achieve a confident prediction. Through further experiments, we\npinpointed the choice of normalization to be especially important in the\ncompositionality of a model, in that batch normalization leads to less\ncompositionality while group and layer normalization lead to more. Finally, we\nalso analyze the features shared by different backbones and plot a landscape of\ndifferent models based on their feature-use similarity.\n","authors":["Mingqi Jiang","Saeed Khorram","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2212.06872v5.pdf","comment":"25 pages with 37 figures, to be published in CVPR24. Project Webpage:\n  https://mingqij.github.io/projects/cdmmtc/"},{"id":"http://arxiv.org/abs/2301.01955v3","updated":"2024-06-24T04:45:10Z","published":"2023-01-05T08:37:36Z","title":"Adaptively Clustering Neighbor Elements for Image-Text Generation","summary":"  We propose a novel Transformer-based image-to-text generation model termed as\n\\textbf{ACF} that adaptively clusters vision patches into object regions and\nlanguage words into phrases to implicitly learn object-phrase alignments for\nbetter visual-text coherence. To achieve this, we design a novel self-attention\nlayer that applies self-attention over the elements in a local cluster window\ninstead of the whole sequence. The window size is softly decided by a\nclustering matrix that is calculated by the current input data and thus this\nprocess is adaptive. By stacking these revised self-attention layers to\nconstruct ACF, the small clusters in the lower layers can be grouped into a\nbigger cluster, \\eg vision/language. ACF clusters small objects/phrases into\nbigger ones. In this gradual clustering process, a parsing tree is generated\nwhich embeds the hierarchical knowledge of the input sequence. As a result, by\nusing ACF to build the vision encoder and language decoder, the hierarchical\nobject-phrase alignments are embedded and then transferred from vision to\nlanguage domains in two popular image-to-text tasks: Image captioning and\nVisual Question Answering. The experiment results demonstrate the effectiveness\nof ACF, which outperforms most SOTA captioning and VQA models and achieves\ncomparable scores compared with some large-scale pre-trained models. Our code\nis available \\href{https://github.com/ZihuaEvan/ACFModel/}{[here]}.\n","authors":["Zihua Wang","Xu Yang","Hanwang Zhang","Haiyang Xu","Ming Yan","Fei Huang","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.01955v3.pdf","comment":"Compared to v1 and v2, we expanded this method to VQA. And it proved\n  that our method can be applied on more general image-text generation tasks"},{"id":"http://arxiv.org/abs/2406.16307v1","updated":"2024-06-24T04:10:28Z","published":"2024-06-24T04:10:28Z","title":"Artistic-style text detector and a new Movie-Poster dataset","summary":"  Although current text detection algorithms demonstrate effectiveness in\ngeneral scenarios, their performance declines when confronted with\nartistic-style text featuring complex structures. This paper proposes a method\nthat utilizes Criss-Cross Attention and residual dense block to address the\nincomplete and misdiagnosis of artistic-style text detection by current\nalgorithms. Specifically, our method mainly consists of a feature extraction\nbackbone, a feature enhancement network, a multi-scale feature fusion module,\nand a boundary discrimination module. The feature enhancement network\nsignificantly enhances the model's perceptual capabilities in complex\nenvironments by fusing horizontal and vertical contextual information, allowing\nit to capture detailed features overlooked in artistic-style text. We\nincorporate residual dense block into the Feature Pyramid Network to suppress\nthe effect of background noise during feature fusion. Aiming to omit the\ncomplex post-processing, we explore a boundary discrimination module that\nguides the correct generation of boundary proposals. Furthermore, given that\nmovie poster titles often use stylized art fonts, we collected a Movie-Poster\ndataset to address the scarcity of artistic-style text data. Extensive\nexperiments demonstrate that our proposed method performs superiorly on the\nMovie-Poster dataset and produces excellent results on multiple benchmark\ndatasets. The code and the Movie-Poster dataset will be available at:\nhttps://github.com/biedaxiaohua/Artistic-style-text-detection\n","authors":["Aoxiang Ning","Yiting Wei","Minglong Xue","Senming Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.16307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11217v2","updated":"2024-06-24T03:55:30Z","published":"2024-06-17T05:23:18Z","title":"WeatherQA: Can Multimodal Language Models Reason about Severe Weather?","summary":"  Severe convective weather events, such as hail, tornadoes, and thunderstorms,\noften occur quickly yet cause significant damage, costing billions of dollars\nevery year. This highlights the importance of forecasting severe weather\nthreats hours in advance to better prepare meteorologists and residents in\nat-risk areas. Can modern large foundation models perform such forecasting?\nExisting weather benchmarks typically focus only on predicting time-series\nchanges in certain weather parameters (e.g., temperature, moisture) with\ntext-only features. In this work, we introduce WeatherQA, the first multimodal\ndataset designed for machines to reason about complex combinations of weather\nparameters (a.k.a., ingredients) and predict severe weather in real-world\nscenarios. The dataset includes over 8,000 (multi-images, text) pairs for\ndiverse severe weather events. Each pair contains rich information crucial for\nforecasting -- the images describe the ingredients capturing environmental\ninstability, surface observations, and radar reflectivity, and the text\ncontains forecast analyses written by human experts. With WeatherQA, we\nevaluate state-of-the-art vision language models, including GPT4, Claude3.5,\nGemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging\ntasks: (1) multi-choice QA for predicting affected area and (2) classification\nof the development potential of severe convection. These tasks require deep\nunderstanding of domain knowledge (e.g., atmospheric dynamics) and complex\nreasoning over multimodal data (e.g., interactions between weather parameters).\nWe show a substantial gap between the strongest VLM, GPT4o, and human\nreasoning. Our comprehensive case study with meteorologists further reveals the\nweaknesses of the models, suggesting that better training and data integration\nare necessary to bridge this gap. WeatherQA link:\nhttps://github.com/chengqianma/WeatherQA.\n","authors":["Chengqian Ma","Zhanxiang Hua","Alexandra Anderson-Frey","Vikram Iyer","Xin Liu","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.11217v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16301v1","updated":"2024-06-24T03:55:25Z","published":"2024-06-24T03:55:25Z","title":"UBiSS: A Unified Framework for Bimodal Semantic Summarization of Videos","summary":"  With the surge in the amount of video data, video summarization techniques,\nincluding visual-modal(VM) and textual-modal(TM) summarization, are attracting\nmore and more attention. However, unimodal summarization inevitably loses the\nrich semantics of the video. In this paper, we focus on a more comprehensive\nvideo summarization task named Bimodal Semantic Summarization of Videos\n(BiSSV). Specifically, we first construct a large-scale dataset, BIDS, in\n(video, VM-Summary, TM-Summary) triplet format. Unlike traditional processing\nmethods, our construction procedure contains a VM-Summary extraction algorithm\naiming to preserve the most salient content within long videos. Based on BIDS,\nwe propose a Unified framework UBiSS for the BiSSV task, which models the\nsaliency information in the video and generates a TM-summary and VM-summary\nsimultaneously. We further optimize our model with a list-wise ranking-based\nobjective to improve its capacity to capture highlights. Lastly, we propose a\nmetric, $NDCG_{MS}$, to provide a joint evaluation of the bimodal summary.\nExperiments show that our unified framework achieves better performance than\nmulti-stage summarization pipelines. Code and data are available at\nhttps://github.com/MeiYutingg/UBiSS.\n","authors":["Yuting Mei","Linli Yao","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2406.16301v1.pdf","comment":"Accepted by ACM International Conference on Multimedia Retrieval\n  (ICMR'24)"},{"id":"http://arxiv.org/abs/2406.16297v1","updated":"2024-06-24T03:49:52Z","published":"2024-06-24T03:49:52Z","title":"Priorformer: A UGC-VQA Method with content and distortion priors","summary":"  User Generated Content (UGC) videos are susceptible to complicated and\nvariant degradations and contents, which prevents the existing blind video\nquality assessment (BVQA) models from good performance since the lack of the\nadapability of distortions and contents. To mitigate this, we propose a novel\nprior-augmented perceptual vision transformer (PriorFormer) for the BVQA of\nUGC, which boots its adaptability and representation capability for divergent\ncontents and distortions. Concretely, we introduce two powerful priors, i.e.,\nthe content and distortion priors, by extracting the content and distortion\nembeddings from two pre-trained feature extractors. Then we adopt these two\npowerful embeddings as the adaptive prior tokens, which are transferred to the\nvision transformer backbone jointly with implicit quality features. Based on\nthe above strategy, the proposed PriorFormer achieves state-of-the-art\nperformance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and\nYouTube-UGC.\n","authors":["Yajing Pei","Shiyu Huang","Yiting Lu","Xin Li","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16297v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2406.16289v1","updated":"2024-06-24T03:30:20Z","published":"2024-06-24T03:30:20Z","title":"Crowd-Sourced NeRF: Collecting Data from Production Vehicles for 3D\n  Street View Reconstruction","summary":"  Recently, Neural Radiance Fields (NeRF) achieved impressive results in novel\nview synthesis. Block-NeRF showed the capability of leveraging NeRF to build\nlarge city-scale models. For large-scale modeling, a mass of image data is\nnecessary. Collecting images from specially designed data-collection vehicles\ncan not support large-scale applications. How to acquire massive high-quality\ndata remains an opening problem. Noting that the automotive industry has a huge\namount of image data, crowd-sourcing is a convenient way for large-scale data\ncollection. In this paper, we present a crowd-sourced framework, which utilizes\nsubstantial data captured by production vehicles to reconstruct the scene with\nthe NeRF model. This approach solves the key problem of large-scale\nreconstruction, that is where the data comes from and how to use them. Firstly,\nthe crowd-sourced massive data is filtered to remove redundancy and keep a\nbalanced distribution in terms of time and space. Then a structure-from-motion\nmodule is performed to refine camera poses. Finally, images, as well as poses,\nare used to train the NeRF model in a certain block. We highlight that we\npresent a comprehensive framework that integrates multiple modules, including\ndata selection, sparse 3D reconstruction, sequence appearance embedding, depth\nsupervision of ground surface, and occlusion completion. The complete system is\ncapable of effectively processing and reconstructing high-quality 3D scenes\nfrom crowd-sourced data. Extensive quantitative and qualitative experiments\nwere conducted to validate the performance of our system. Moreover, we proposed\nan application, named first-view navigation, which leveraged the NeRF model to\ngenerate 3D street view and guide the driver with a synthesized video.\n","authors":["Tong Qin","Changze Li","Haoyang Ye","Shaowei Wan","Minzhen Li","Hongwei Liu","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2406.16289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03876v2","updated":"2024-06-24T03:19:39Z","published":"2024-04-05T03:51:19Z","title":"Accurately Classifying Out-Of-Distribution Data in Facial Recognition","summary":"  Standard classification theory assumes that the distribution of images in the\ntest and training sets are identical. Unfortunately, real-life scenarios\ntypically feature unseen data (\"out-of-distribution data\") which is different\nfrom data in the training distribution(\"in-distribution\"). This issue is most\nprevalent in social justice problems where data from under-represented groups\nmay appear in the test data without representing an equal proportion of the\ntraining data. This may result in a model returning confidently wrong decisions\nand predictions. We are interested in the following question: Can the\nperformance of a neural network improve on facial images of out-of-distribution\ndata when it is trained simultaneously on multiple datasets of in-distribution\ndata? We approach this problem by incorporating the Outlier Exposure model and\ninvestigate how the model's performance changes when other datasets of facial\nimages were implemented. We observe that the accuracy and other metrics of the\nmodel can be increased by applying Outlier Exposure, incorporating a trainable\nweight parameter to increase the machine's emphasis on outlier images, and by\nre-weighting the importance of different class labels. We also experimented\nwith whether sorting the images and determining outliers via image features\nwould have more of an effect on the metrics than sorting by average pixel\nvalue. Our goal was to make models not only more accurate but also more fair by\nscanning a more expanded range of images. We also tested the datasets in\nreverse order to see whether a more fair dataset with balanced features has an\neffect on the model's accuracy.\n","authors":["Gianluca Barone","Aashrit Cunchala","Rudy Nunez"],"pdf_url":"https://arxiv.org/pdf/2404.03876v2.pdf","comment":"18 pages, 6 tables, 6 figures"},{"id":"http://arxiv.org/abs/2406.16279v1","updated":"2024-06-24T03:01:08Z","published":"2024-06-24T03:01:08Z","title":"SegNet4D: Effective and Efficient 4D LiDAR Semantic Segmentation in\n  Autonomous Driving Environments","summary":"  4D LiDAR semantic segmentation, also referred to as multi-scan semantic\nsegmentation, plays a crucial role in enhancing the environmental understanding\ncapabilities of autonomous vehicles. It entails identifying the semantic\ncategory of each point in the LiDAR scan and distinguishing whether it is\ndynamic, a critical aspect in downstream tasks such as path planning and\nautonomous navigation. Existing methods for 4D semantic segmentation often rely\non computationally intensive 4D convolutions for multi-scan input, resulting in\npoor real-time performance. In this article, we introduce SegNet4D, a novel\nreal-time multi-scan semantic segmentation method leveraging a projection-based\napproach for fast motion feature encoding, showcasing outstanding performance.\nSegNet4D treats 4D semantic segmentation as two distinct tasks: single-scan\nsemantic segmentation and moving object segmentation, each addressed by\ndedicated head. These results are then fused in the proposed motion-semantic\nfusion module to achieve comprehensive multi-scan semantic segmentation.\nBesides, we propose extracting instance information from the current scan and\nincorporating it into the network for instance-aware segmentation. Our approach\nexhibits state-of-the-art performance across multiple datasets and stands out\nas a real-time multi-scan semantic segmentation method. The implementation of\nSegNet4D will be made available at\n\\url{https://github.com/nubot-nudt/SegNet4D}.\n","authors":["Neng Wang","Ruibin Guo","Chenghao Shi","Hui Zhang","Huimin Lu","Zhiqiang Zheng","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16279v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.16273v1","updated":"2024-06-24T02:40:26Z","published":"2024-06-24T02:40:26Z","title":"YouDream: Generating Anatomically Controllable Consistent Text-to-3D\n  Animals","summary":"  3D generation guided by text-to-image diffusion models enables the creation\nof visually compelling assets. However previous methods explore generation\nbased on image or text. The boundaries of creativity are limited by what can be\nexpressed through words or the images that can be sourced. We present YouDream,\na method to generate high-quality anatomically controllable animals. YouDream\nis guided using a text-to-image diffusion model controlled by 2D views of a 3D\npose prior. Our method generates 3D animals that are not possible to create\nusing previous text-to-3D generative methods. Additionally, our method is\ncapable of preserving anatomic consistency in the generated animals, an area\nwhere prior text-to-3D approaches often struggle. Moreover, we design a fully\nautomated pipeline for generating commonly found animals. To circumvent the\nneed for human intervention to create a 3D pose, we propose a multi-agent LLM\nthat adapts poses from a limited library of animal 3D poses to represent the\ndesired animal. A user study conducted on the outcomes of YouDream demonstrates\nthe preference of the animal models generated by our method over others.\nTurntable results and code are released at https://youdream3d.github.io/\n","authors":["Sandeep Mishra","Oindrila Saha","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2406.16273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16272v1","updated":"2024-06-24T02:38:30Z","published":"2024-06-24T02:38:30Z","title":"Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via\n  Attention-Guided Feature Enhancement","summary":"  Text-to-Image Diffusion Models (T2I DMs) have garnered significant attention\nfor their ability to generate high-quality images from textual descriptions.\nHowever, these models often produce images that do not fully align with the\ninput prompts, resulting in semantic inconsistencies. The most prominent issue\namong these semantic inconsistencies is catastrophic-neglect, where the images\ngenerated by T2I DMs miss key objects mentioned in the prompt. We first conduct\nan empirical study on this issue, exploring the prevalence of\ncatastrophic-neglect, potential mitigation strategies with feature enhancement,\nand the insights gained. Guided by the empirical findings, we propose an\nautomated repair approach named Patcher to address catastrophic-neglect in T2I\nDMs. Specifically, Patcher first determines whether there are any neglected\nobjects in the prompt, and then applies attention-guided feature enhancement to\nthese neglected objects, resulting in a repaired prompt. Experimental results\non three versions of Stable Diffusion demonstrate that Patcher effectively\nrepairs the issue of catastrophic-neglect, achieving 10.1%-16.3% higher Correct\nRate in image generation compared to baselines.\n","authors":["Zhiyuan Chang","Mingyang Li","Junjie Wang","Yi Liu","Qing Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16272v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.16271v1","updated":"2024-06-24T02:33:46Z","published":"2024-06-24T02:33:46Z","title":"Feature-prompting GBMSeg: One-Shot Reference Guided Training-Free Prompt\n  Engineering for Glomerular Basement Membrane Segmentation","summary":"  Assessment of the glomerular basement membrane (GBM) in transmission electron\nmicroscopy (TEM) is crucial for diagnosing chronic kidney disease (CKD). The\nlack of domain-independent automatic segmentation tools for the GBM\nnecessitates an AI-based solution to automate the process. In this study, we\nintroduce GBMSeg, a training-free framework designed to automatically segment\nthe GBM in TEM images guided only by a one-shot annotated reference.\nSpecifically, GBMSeg first exploits the robust feature matching capabilities of\nthe pretrained foundation model to generate initial prompt points, then\nintroduces a series of novel automatic prompt engineering techniques across the\nfeature and physical space to optimize the prompt scheme. Finally, GBMSeg\nemploys a class-agnostic foundation segmentation model with the generated\nprompt scheme to obtain accurate segmentation results. Experimental results on\nour collected 2538 TEM images confirm that GBMSeg achieves superior\nsegmentation performance with a Dice similarity coefficient (DSC) of 87.27%\nusing only one labeled reference image in a training-free manner, outperforming\nrecently proposed one-shot or few-shot methods. In summary, GBMSeg introduces a\ndistinctive automatic prompt framework that facilitates robust\ndomain-independent segmentation performance without training, particularly\nadvancing the automatic prompting of foundation segmentation models for medical\nimages. Future work involves automating the thickness measurement of segmented\nGBM and quantifying pathological indicators, holding significant potential for\nadvancing pathology assessments in clinical applications. The source code is\navailable on https://github.com/SnowRain510/GBMSeg\n","authors":["Xueyu Liu","Guangze Shi","Rui Wang","Yexin Lai","Jianan Zhang","Lele Sun","Quan Yang","Yongfei Wu","MIng Li","Weixia Han","Wen Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.16271v1.pdf","comment":"Accepted for MICCAI2024"},{"id":"http://arxiv.org/abs/2401.12900v5","updated":"2024-06-24T02:30:09Z","published":"2024-01-23T16:40:47Z","title":"PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation\n  with 3D Gaussian Splatting","summary":"  Despite much progress, achieving real-time high-fidelity head avatar\nanimation is still difficult and existing methods have to trade-off between\nspeed and quality. 3DMM based methods often fail to model non-facial structures\nsuch as eyeglasses and hairstyles, while neural implicit models suffer from\ndeformation inflexibility and rendering inefficiency. Although 3D Gaussian has\nbeen demonstrated to possess promising capability for geometry representation\nand radiance field reconstruction, applying 3D Gaussian in head avatar creation\nremains a major challenge since it is difficult for 3D Gaussian to model the\nhead shape variations caused by changing poses and expressions. In this paper,\nwe introduce PSAvatar, a novel framework for animatable head avatar creation\nthat utilizes discrete geometric primitive to create a parametric morphable\nshape model and employs 3D Gaussian for fine detail representation and high\nfidelity rendering. The parametric morphable shape model is a Point-based\nMorphable Shape Model (PMSM) which uses points instead of meshes for 3D\nrepresentation to achieve enhanced representation flexibility. The PMSM first\nconverts the FLAME mesh to points by sampling on the surfaces as well as off\nthe meshes to enable the reconstruction of not only surface-like structures but\nalso complex geometries such as eyeglasses and hairstyles. By aligning these\npoints with the head shape in an analysis-by-synthesis manner, the PMSM makes\nit possible to utilize 3D Gaussian for fine detail representation and\nappearance modeling, thus enabling the creation of high-fidelity avatars. We\nshow that PSAvatar can reconstruct high-fidelity head avatars of a variety of\nsubjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a\nresolution of 512 $\\times$ 512 ).\n","authors":["Zhongyuan Zhao","Zhenyu Bao","Qing Li","Guoping Qiu","Kanglin Liu"],"pdf_url":"https://arxiv.org/pdf/2401.12900v5.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2308.07269v3","updated":"2024-06-24T02:17:57Z","published":"2023-08-14T16:52:42Z","title":"EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models","summary":"  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.\n","authors":["Peng Wang","Ningyu Zhang","Bozhong Tian","Zekun Xi","Yunzhi Yao","Ziwen Xu","Mengru Wang","Shengyu Mao","Xiaohan Wang","Siyuan Cheng","Kangwei Liu","Yuansheng Ni","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.07269v3.pdf","comment":"ACL 2024 System Demonstrations; Code:\n  https://github.com/zjunlp/EasyEdit HF Demo:\n  https://huggingface.co/spaces/zjunlp/EasyEdit Video:\n  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit"},{"id":"http://arxiv.org/abs/2406.04341v2","updated":"2024-06-24T02:14:18Z","published":"2024-06-06T17:59:52Z","title":"Interpreting the Second-Order Effects of Neurons in CLIP","summary":"  We interpret the function of individual neurons in CLIP by automatically\ndescribing them using text. Analyzing the direct effects (i.e. the flow from a\nneuron through the residual stream to the output) or the indirect effects\n(overall contribution) fails to capture the neurons' function in CLIP.\nTherefore, we present the \"second-order lens\", analyzing the effect flowing\nfrom a neuron through the later attention heads, directly to the output. We\nfind that these effects are highly selective: for each neuron, the effect is\nsignificant for <2% of the images. Moreover, each effect can be approximated by\na single direction in the text-image space of CLIP. We describe neurons by\ndecomposing these directions into sparse sets of text representations. The sets\nreveal polysemantic behavior - each neuron corresponds to multiple, often\nunrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we\nmass-produce \"semantic\" adversarial examples by generating images with concepts\nspuriously correlated to the incorrect class. Additionally, we use the\nsecond-order effects for zero-shot segmentation and attribute discovery in\nimages. Our results indicate that a scalable understanding of neurons can be\nused for model deception and for introducing new model capabilities.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.04341v2.pdf","comment":"project page:\n  https://yossigandelsman.github.io/clip_neurons/index.html"},{"id":"http://arxiv.org/abs/2406.16260v1","updated":"2024-06-24T01:56:12Z","published":"2024-06-24T01:56:12Z","title":"Video-Infinity: Distributed Long Video Generation","summary":"  Diffusion models have recently achieved remarkable results for video\ngeneration. Despite the encouraging performances, the generated videos are\ntypically constrained to a small number of frames, resulting in clips lasting\nmerely a few seconds. The primary challenges in producing longer videos include\nthe substantial memory requirements and the extended processing time required\non a single GPU. A straightforward solution would be to split the workload\nacross multiple GPUs, which, however, leads to two issues: (1) ensuring all\nGPUs communicate effectively to share timing and context information, and (2)\nmodifying existing video diffusion models, which are usually trained on short\nsequences, to create longer videos without additional training. To tackle\nthese, in this paper we introduce Video-Infinity, a distributed inference\npipeline that enables parallel processing across multiple GPUs for long-form\nvideo generation. Specifically, we propose two coherent mechanisms: Clip\nparallelism and Dual-scope attention. Clip parallelism optimizes the gathering\nand sharing of context information across GPUs which minimizes communication\noverhead, while Dual-scope attention modulates the temporal self-attention to\nbalance local and global contexts efficiently across the devices. Together, the\ntwo mechanisms join forces to distribute the workload and enable the fast\ngeneration of long videos. Under an 8 x Nvidia 6000 Ada GPU (48G) setup, our\nmethod generates videos up to 2,300 frames in approximately 5 minutes, enabling\nlong video generation at a speed 100 times faster than the prior methods.\n","authors":["Zhenxiong Tan","Xingyi Yang","Songhua Liu","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17137v2","updated":"2024-06-24T01:42:55Z","published":"2023-11-28T18:59:02Z","title":"Intrinsic LoRA: A Generalist Approach for Discovering Knowledge in\n  Generative Models","summary":"  Generative models excel at creating images that closely mimic real scenes,\nsuggesting they inherently encode scene representations. We introduce Intrinsic\nLoRA (I-LoRA), a general approach that uses Low-Rank Adaptation (LoRA) to\ndiscover scene intrinsics such as normals, depth, albedo, and shading from a\nwide array of generative models. I-LoRA is lightweight, adding minimally to the\nmodel's parameters and requiring very small datasets for this knowledge\ndiscovery. Our approach, applicable to Diffusion models, GANs, and\nAutoregressive models alike, generates intrinsics using the same output head as\nthe original images. Through control experiments, we establish a correlation\nbetween the generative model's quality and the extracted intrinsics' accuracy.\nFinally, scene intrinsics obtained by our method with just hundreds to\nthousands of labeled images, perform on par with those from supervised methods\ntrained on millions of labeled examples.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v2.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2405.04309v2","updated":"2024-06-24T01:30:48Z","published":"2024-05-07T13:33:50Z","title":"Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment\n  and Spatially-variant Deformation Modeling","summary":"  Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively\nstudied and great progress has been made, there are still key challenges that\nhinder their broad real-world applications: 1) the inherent motion/rotation\nambiguity requires either explicit camera motion recovery with extra constraint\nor complex Procrustean Alignment; 2) existing low-rank modeling of the global\nshape can over-penalize drastic deformations in the 3D shape sequence. This\npaper proposes to resolve the above issues from a spatial-temporal modeling\nperspective. First, we propose a novel Temporally-smooth Procrustean Alignment\nmodule that estimates 3D deforming shapes and adjusts the camera motion by\naligning the 3D shape sequence consecutively. Our new alignment module remedies\nthe requirement of complex reference 3D shape during alignment, which is more\nconductive to non-isotropic deformation modeling. Second, we propose a\nspatial-weighted approach to enforce the low-rank constraint adaptively at\ndifferent locations to accommodate drastic spatially-variant deformation\nreconstruction better. Our modeling outperform existing low-rank based methods,\nand extensive experiments across different datasets validate the effectiveness\nof our method.\n","authors":["Jiawei Shi","Hui Deng","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2405.04309v2.pdf","comment":"Accepted by CVPR 2024; V2 adds new experiments"},{"id":"http://arxiv.org/abs/2402.09801v2","updated":"2024-06-24T00:50:58Z","published":"2024-02-15T08:58:03Z","title":"EFUF: Efficient Fine-grained Unlearning Framework for Mitigating\n  Hallucinations in Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have attracted increasing attention\nin the past few years, but they may still generate descriptions that include\nobjects not present in the corresponding images, a phenomenon known as object\nhallucination. To eliminate hallucinations, existing methods manually annotate\npaired responses with and without hallucinations, and then employ various\nalignment algorithms to improve the alignment capability between images and\ntext. However, they not only demand considerable computation resources during\nthe finetuning stage but also require expensive human annotation to construct\npaired data needed by the alignment algorithms. To address these issues, we\nborrow the idea of unlearning and propose an efficient fine-grained unlearning\nframework (EFUF), which can eliminate hallucinations without the need for\npaired data. Extensive experiments show that our method consistently reduces\nhallucinations while preserving the generation quality with modest\ncomputational overhead. Our code and datasets will be publicly available.\n","authors":["Shangyu Xing","Fei Zhao","Zhen Wu","Tuo An","Weihao Chen","Chunhui Li","Jianbing Zhang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2402.09801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04929v2","updated":"2024-06-24T00:37:16Z","published":"2024-02-07T14:56:13Z","title":"Source-Free Domain Adaptation with Diffusion-Guided Source Data\n  Generation","summary":"  This paper introduces a novel approach to leverage the generalizability of\nDiffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed\nDMSFDA method involves fine-tuning a pre-trained text-to-image diffusion model\nto generate source domain images using features from the target images to guide\nthe diffusion process. Specifically, the pre-trained diffusion model is\nfine-tuned to generate source samples that minimize entropy and maximize\nconfidence for the pre-trained source model. We then use a diffusion\nmodel-based image mixup strategy to bridge the domain gap between the source\nand target domains. We validate our approach through comprehensive experiments\nacross a range of datasets, including Office-31 [39], Office-Home [48], and\nVisDA [35]. The results demonstrate significant improvements in SFDA\nperformance, highlighting the potential of diffusion models in generating\ncontextually relevant, domain-specific images.\n","authors":["Shivang Chopra","Suraj Kothawade","Houda Aynaou","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.04929v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.01701"},{"id":"http://arxiv.org/abs/2406.17183v1","updated":"2024-06-24T23:43:08Z","published":"2024-06-24T23:43:08Z","title":"POPCat: Propagation of particles for complex annotation tasks","summary":"  Novel dataset creation for all multi-object tracking, crowd-counting, and\nindustrial-based videos is arduous and time-consuming when faced with a unique\nclass that densely populates a video sequence. We propose a time efficient\nmethod called POPCat that exploits the multi-target and temporal features of\nvideo data to produce a semi-supervised pipeline for segmentation or box-based\nvideo annotation. The method retains the accuracy level associated with human\nlevel annotation while generating a large volume of semi-supervised annotations\nfor greater generalization. The method capitalizes on temporal features through\nthe use of a particle tracker to expand the domain of human-provided target\npoints. This is done through the use of a particle tracker to reassociate the\ninitial points to a set of images that follow the labeled frame. A YOLO model\nis then trained with this generated data, and then rapidly infers on the target\nvideo. Evaluations are conducted on GMOT-40, AnimalTrack, and Visdrone-2019\nbenchmarks. These multi-target video tracking/detection sets contain multiple\nsimilar-looking targets, camera movements, and other features that would\ncommonly be seen in \"wild\" situations. We specifically choose these difficult\ndatasets to demonstrate the efficacy of the pipeline and for comparison\npurposes. The method applied on GMOT-40, AnimalTrack, and Visdrone shows a\nmargin of improvement on recall/mAP50/mAP over the best results by a value of\n24.5%/9.6%/4.8%, -/43.1%/27.8%, and 7.5%/9.4%/7.5% where metrics were\ncollected.\n","authors":["Adam Srebrnjak Yang","Dheeraj Khanna","John S. Zelek"],"pdf_url":"https://arxiv.org/pdf/2406.17183v1.pdf","comment":"10 pages, 5 figures, Accepted in \"Conference on Robots and Vision\n  2024\""},{"id":"http://arxiv.org/abs/2403.19651v2","updated":"2024-06-24T23:41:29Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v2.pdf","comment":"ICML 2024 (Oral); Project Website:\n  https://open-vision-language.github.io/MagicLens/"},{"id":"http://arxiv.org/abs/2406.17173v1","updated":"2024-06-24T23:23:18Z","published":"2024-06-24T23:23:18Z","title":"Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT\n  Classification with Transformer Networks","summary":"  The manifestation of symptoms associated with lung diseases can vary in\ndifferent depths for individual patients, highlighting the significance of 3D\ninformation in CT scans for medical image classification. While Vision\nTransformer has shown superior performance over convolutional neural networks\nin image classification tasks, their effectiveness is often demonstrated on\nsufficiently large 2D datasets and they easily encounter overfitting issues on\nsmall medical image datasets. To address this limitation, we propose a\nDiffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent\nspace of the Diffusion model to form the slice sequence for 3D analysis and\nincorporates clustering attention into ViT to aggregate repetitive information\nwithin 3D CT scans, thereby harnessing the power of the advanced transformer in\n3D classification tasks on small datasets. Our method exhibits improved\nperformance on two different scales of small datasets of 3D lung CT scans,\nsurpassing the state of the art 3D methods and other transformer-based\napproaches that emerged during the COVID-19 pandemic, demonstrating its robust\nand superior performance across different scales of data. Experimental results\nunderscore the superiority of our proposed method, indicating its potential for\nenhancing medical image classification tasks in real-world scenarios.\n","authors":["Zihao Jin","Yingying Fang","Jiahao Huang","Caiwen Xu","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17173v1.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2404.03118v3","updated":"2024-06-24T22:45:20Z","published":"2024-04-03T23:57:34Z","title":"LVLM-Interpret: An Interpretability Tool for Large Vision-Language\n  Models","summary":"  In the rapidly evolving landscape of artificial intelligence, multi-modal\nlarge language models are emerging as a significant area of interest. These\nmodels, which combine various forms of data input, are becoming increasingly\npopular. However, understanding their internal mechanisms remains a complex\ntask. Numerous advancements have been made in the field of explainability tools\nand mechanisms, yet there is still much to explore. In this work, we present a\nnovel interactive application aimed towards understanding the internal\nmechanisms of large vision-language models. Our interface is designed to\nenhance the interpretability of the image patches, which are instrumental in\ngenerating an answer, and assess the efficacy of the language model in\ngrounding its output in the image. With our application, a user can\nsystematically investigate the model and uncover system limitations, paving the\nway for enhancements in system capabilities. Finally, we present a case study\nof how our application can aid in understanding failure mechanisms in a popular\nlarge multi-modal model: LLaVA.\n","authors":["Gabriela Ben Melech Stan","Estelle Aflalo","Raanan Yehezkel Rohekar","Anahita Bhiwandiwalla","Shao-Yen Tseng","Matthew Lyle Olson","Yaniv Gurwicz","Chenfei Wu","Nan Duan","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2404.03118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17162v1","updated":"2024-06-24T22:29:30Z","published":"2024-06-24T22:29:30Z","title":"Virtual Mines -- Component-level recycling of printed circuit boards\n  using deep learning","summary":"  This contribution gives an overview of an ongoing project using machine\nlearning and computer vision components for improving the electronic waste\nrecycling process. In circular economy, the \"virtual mines\" concept refers to\nproduction cycles where interesting raw materials are reclaimed in an efficient\nand cost-effective manner from end-of-life items. In particular, the growth of\ne-waste, due to the increasingly shorter life cycle of hi-tech goods, is a\nglobal problem. In this paper, we describe a pipeline based on deep learning\nmodel to recycle printed circuit boards at the component level. A pre-trained\nYOLOv5 model is used to analyze the results of the locally developed dataset.\nWith a different distribution of class instances, YOLOv5 managed to achieve\nsatisfactory precision and recall, with the ability to optimize with large\ncomponent instances.\n","authors":["Muhammad Mohsin","Stefano Rovetta","Francesco Masulli","Alberto Cabri"],"pdf_url":"https://arxiv.org/pdf/2406.17162v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.17148v1","updated":"2024-06-24T21:38:36Z","published":"2024-06-24T21:38:36Z","title":"Unambiguous Recognition Should Not Rely Solely on Natural Language\n  Training","summary":"  In LaTeX text recognition using Transformer-based architectures, this paper\nidentifies certain \"bias\" issues. For instance, $e-t$ is frequently\nmisrecognized as $e^{-t}$. This bias stems from the inherent characteristics of\nthe dataset. To mitigate this bias, we propose a LaTeX printed text recognition\nmodel trained on a mixed dataset of pseudo-formulas and pseudo-text. The model\nemploys a Swin Transformer as the encoder and a RoBERTa model as the decoder.\nExperimental results demonstrate that this approach reduces \"bias\", enhancing\nthe accuracy and robustness of text recognition. For clear images, the model\nstrictly adheres to the image content; for blurred images, it integrates both\nimage and contextual information to produce reasonable recognition results.\n","authors":["Renqing Luo","Yuhan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02424v2","updated":"2024-06-24T21:37:45Z","published":"2024-04-03T03:27:01Z","title":"Rethinking Pruning for Vision-Language Models: Strategies for Effective\n  Sparsity and Performance Restoration","summary":"  Vision-Language Models (VLMs) integrate information from multiple modalities\nand have shown remarkable success across various tasks. However, deploying\nlarge-scale VLMs in resource-constrained scenarios is challenging. Pruning\nfollowed by finetuning offers a potential solution but remains underexplored\nfor VLMs. This study addresses two key questions: how to distribute sparsity\nacross different modality-specific models, and how to restore the performance\nof pruned sparse VLMs. Our preliminary studies identified two effective pruning\nsettings: applying the same sparsity to both vision and language models, and\npruning only the language models. While LoRA finetuning aims to restore sparse\nmodels, it faces challenges due to incompatibility with sparse models,\ndisrupting the pruned sparsity. To overcome these issues, we propose\nSparseLoRA, which applies sparsity directly to LoRA weights. Our experimental\nresults demonstrate significant improvements, including an 11.3\\% boost under\n2:4 sparsity and a 47.6\\% enhancement under unstructured 70\\% sparsity. Code is\nreleased at: \\url{https://github.com/Shwai-He/VLM-Compression}.\n","authors":["Shwai He","Ang Li","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17146v1","updated":"2024-06-24T21:36:01Z","published":"2024-06-24T21:36:01Z","title":"Vastextures: Vast repository of textures and PBR materials extracted\n  from real-world images using unsupervised methods","summary":"  Vastextures is a vast repository of 500,000 textures and PBR materials\nextracted from real-world images using an unsupervised process. The extracted\nmaterials and textures are extremely diverse and cover a vast range of\nreal-world patterns, but at the same time less refined compared to existing\nrepositories. The repository is composed of 2D textures cropped from natural\nimages and SVBRDF/PBR materials generated from these textures. Textures and PBR\nmaterials are essential for CGI. Existing materials repositories focus on\ngames, animation, and arts, that demand a limited amount of high-quality\nassets. However, virtual worlds and synthetic data are becoming increasingly\nimportant for training A.I systems for computer vision. This application\ndemands a huge amount of diverse assets but at the same time less affected by\nnoisy and unrefined assets. Vastexture aims to address this need by creating a\nfree, huge, and diverse assets repository that covers as many real-world\nmaterials as possible. The materials are automatically extracted from natural\nimages in two steps: 1) Automatically scanning a giant amount of images to\nidentify and crop regions with uniform textures. This is done by splitting the\nimage into a grid of cells and identifying regions in which all of the cells\nshare a similar statistical distribution. 2) Extracting the properties of the\nPBR material from the cropped texture. This is done by randomly guessing every\ncorrelation between the properties of the texture image and the properties of\nthe PBR material. The resulting PBR materials exhibit a vast amount of\nreal-world patterns as well as unexpected emergent properties. Neutral nets\ntrained on this repository outperformed nets trained using handcrafted assets.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2406.17146v1.pdf","comment":"Vastexture was published as part of Learning Zero-Shot Material\n  States Segmentation, by Implanting Natural Image Patterns in Synthetic Data,\n  refer to this work in citations. This document gives a more detailed and\n  technical discussion of this repository"},{"id":"http://arxiv.org/abs/2406.13875v2","updated":"2024-06-24T20:31:00Z","published":"2024-06-19T22:37:42Z","title":"WATT: Weight Average Test-Time Adaptation of CLIP","summary":"  Vision-Language Models (VLMs) such as CLIP have yielded unprecedented\nperformance for zero-shot image classification, yet their generalization\ncapability may still be seriously challenged when confronted to domain shifts.\nIn response, we present Weight Average Test-Time Adaptation (WATT) of CLIP, a\npioneering approach facilitating full test-time adaptation (TTA) of this VLM.\nOur method employs a diverse set of templates for text prompts, augmenting the\nexisting framework of CLIP. Predictions are utilized as pseudo labels for model\nupdates, followed by weight averaging to consolidate the learned information\nglobally. Furthermore, we introduce a text ensemble strategy, enhancing overall\ntest performance by aggregating diverse textual cues. Our findings underscore\nthe efficacy of WATT in enhancing performance across diverse datasets,\nincluding CIFAR-10-C, CIFAR-10.1, CIFAR-100-C, VisDA-C, and several other\nchallenging datasets, effectively covering a wide range of domain shifts.\nNotably, these enhancements are achieved without necessitating additional model\ntransformations or trainable modules. Moreover, compared to other Test-Time\nAdaptation methods, our approach can operate effectively with just a single\nimage. Highlighting the potential of innovative test-time strategies, this\nresearch emphasizes their role in fortifying the adaptability of VLMs. The\nimplementation is available at:\n\\url{https://github.com/Mehrdad-Noori/WATT.git}.\n","authors":["David Osowiechi","Mehrdad Noori","Gustavo Adolfo Vargas Hakim","Moslem Yazdanpanah","Ali Bahri","Milad Cheraghalikhani","Sahar Dastani","Farzad Beizaee","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2406.13875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09457v4","updated":"2024-06-24T20:29:38Z","published":"2023-10-14T00:32:11Z","title":"UCM-Net: A Lightweight and Efficient Solution for Skin Lesion\n  Segmentation using MLP and CNN","summary":"  Skin cancer poses a significant public health challenge, necessitating\nefficient diagnostic tools. We introduce UCM-Net, a novel skin lesion\nsegmentation model combining Multi-Layer Perceptrons (MLP) and Convolutional\nNeural Networks (CNN). This lightweight, efficient architecture, deviating from\ntraditional UNet designs, dramatically reduces computational demands, making it\nideal for mobile health applications. Evaluated on PH2, ISIC 2017, and ISIC\n2018 datasets, UCM-Net demonstrates robust performance with fewer than 50KB\nparameters and requires less than 0.05 Giga Operations Per Second (GLOPs).\nMoreover, its minimal memory requirement is just 1.19MB in CPU environment\npositions. It is a potential benchmark for efficiency in skin lesion\nsegmentation, suitable for deployment in resource-constrained settings. In\norder to facilitate accessibility and further research in the field, the\nUCM-Net source code is https://github.com/chunyuyuan/UCM-Net.\n","authors":["Chunyu Yuan","Dongfang Zhao","Sos S. Agaian"],"pdf_url":"https://arxiv.org/pdf/2310.09457v4.pdf","comment":"17 pages, accepted by Journal of Biomedical Signal Processing and\n  Control"},{"id":"http://arxiv.org/abs/2406.17126v1","updated":"2024-06-24T20:29:16Z","published":"2024-06-24T20:29:16Z","title":"MM-SpuBench: Towards Better Understanding of Spurious Biases in\n  Multimodal LLMs","summary":"  Spurious bias, a tendency to use spurious correlations between non-essential\ninput attributes and target variables for predictions, has revealed a severe\nrobustness pitfall in deep learning models trained on single modality data.\nMultimodal Large Language Models (MLLMs), which integrate both vision and\nlanguage models, have demonstrated strong capability in joint vision-language\nunderstanding. However, whether spurious biases are prevalent in MLLMs remains\nunder-explored. We mitigate this gap by analyzing the spurious biases in a\nmultimodal setting, uncovering the specific test data patterns that can\nmanifest this problem when biases in the vision model cascade into the\nalignment between visual and text tokens in MLLMs. To better understand this\nproblem, we introduce MM-SpuBench, a comprehensive visual question-answering\n(VQA) benchmark designed to evaluate MLLMs' reliance on nine distinct\ncategories of spurious correlations from five open-source image datasets. The\nVQA dataset is built from human-understandable concept information\n(attributes). Leveraging this benchmark, we conduct a thorough evaluation of\ncurrent state-of-the-art MLLMs. Our findings illuminate the persistence of the\nreliance on spurious correlations from these models and underscore the urge for\nnew methodologies to mitigate spurious biases. To support the MLLM robustness\nresearch, we release our VQA benchmark at\nhttps://huggingface.co/datasets/mmbench/MM-SpuBench.\n","authors":["Wenqian Ye","Guangtao Zheng","Yunsheng Ma","Xu Cao","Bolin Lai","James M. Rehg","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17119v1","updated":"2024-06-24T20:13:23Z","published":"2024-06-24T20:13:23Z","title":"Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier\n  Neural Operator with U-Net Backbone","summary":"  Prolonged contact between a corrosive liquid and metal alloys can cause\nprogressive dealloying. For such liquid-metal dealloying (LMD) process, phase\nfield models have been developed. However, the governing equations often\ninvolve coupled non-linear partial differential equations (PDE), which are\nchallenging to solve numerically. In particular, stiffness in the PDEs requires\nan extremely small time steps (e.g. $10^{-12}$ or smaller). This computational\nbottleneck is especially problematic when running LMD simulation until a late\ntime horizon is required. This motivates the development of surrogate models\ncapable of leaping forward in time, by skipping several consecutive time steps\nat-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators\n(U-AFNO), a machine learning (ML) model inspired by recent advances in neural\noperator learning. U-AFNO employs U-Nets for extracting and reconstructing\nlocal features within the physical fields, and passes the latent space through\na vision transformer (ViT) implemented in the Fourier space (AFNO). We use\nU-AFNOs to learn the dynamics mapping the field at a current time step into a\nlater time step. We also identify global quantities of interest (QoI)\ndescribing the corrosion process (e.g. the deformation of the liquid-metal\ninterface) and show that our proposed U-AFNO model is able to accurately\npredict the field dynamics, in-spite of the chaotic nature of LMD. Our model\nreproduces the key micro-structure statistics and QoIs with a level of accuracy\non-par with the high-fidelity numerical solver. We also investigate the\nopportunity of using hybrid simulations, in which we alternate forward leap in\ntime using the U-AFNO with high-fidelity time stepping. We demonstrate that\nwhile advantageous for some surrogate model design choices, our proposed U-AFNO\nmodel in fully auto-regressive settings consistently outperforms hybrid\nschemes.\n","authors":["Christophe Bonneville","Nathan Bieberdorf","Arun Hegde","Mark Asta","Habib N. Najm","Laurent Capolungo","Cosmin Safta"],"pdf_url":"https://arxiv.org/pdf/2406.17119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17117v1","updated":"2024-06-24T20:11:46Z","published":"2024-06-24T20:11:46Z","title":"Speeding Up Image Classifiers with Little Companions","summary":"  Scaling up neural networks has been a key recipe to the success of large\nlanguage and vision models. However, in practice, up-scaled models can be\ndisproportionately costly in terms of computations, providing only marginal\nimprovements in performance; for example, EfficientViT-L3-384 achieves <2%\nimprovement on ImageNet-1K accuracy over the base L1-224 model, while requiring\n$14\\times$ more multiply-accumulate operations (MACs). In this paper, we\ninvestigate scaling properties of popular families of neural networks for image\nclassification, and find that scaled-up models mostly help with \"difficult\"\nsamples. Decomposing the samples by difficulty, we develop a simple\nmodel-agnostic two-pass Little-Big algorithm that first uses a light-weight\n\"little\" model to make predictions of all samples, and only passes the\ndifficult ones for the \"big\" model to solve. Good little companion achieve\ndrastic MACs reduction for a wide variety of model families and scales. Without\nloss of accuracy or modification of existing models, our Little-Big models\nachieve MACs reductions of 76% for EfficientViT-L3-384, 81% for\nEfficientNet-B7-600, 71% for DeiT3-L-384 on ImageNet-1K. Little-Big also speeds\nup the InternImage-G-512 model by 62% while achieving 90% ImageNet-1K top-1\naccuracy, serving both as a strong baseline and as a simple practical method\nfor large model compression.\n","authors":["Yang Liu","Kowshik Thopalli","Jayaraman Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2406.17117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17115v1","updated":"2024-06-24T20:08:07Z","published":"2024-06-24T20:08:07Z","title":"Evaluating the Quality of Hallucination Benchmarks for Large\n  Vision-Language Models","summary":"  Despite the rapid progress and outstanding performance of Large\nVision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the\nissue of hallucination, i.e., LVLMs tend to generate responses that are\ninconsistent with the corresponding visual inputs. To evaluate the degree of\nhallucination in LVLMs, previous works have proposed a series of benchmarks\nfeaturing different types of tasks and evaluation metrics. However, we find\nthat the quality of the existing hallucination benchmarks varies, with some\nsuffering from problems, e.g., inconsistent evaluation results under repeated\ntests, and misalignment with human evaluation. To this end, we propose a\nHallucination benchmark Quality Measurement framework (HQM), which leverages\nvarious indicators to assess the reliability and validity of existing\nhallucination benchmarks separately. Specifically, for reliability we explore\ntest-retest reliability and parallel-forms reliability, while for validity we\nexamine criterion validity and coverage of hallucination types. Furthermore,\nbased on the results of our quality measurement, we construct a High-Quality\nHallucination Benchmark (HQH) for LVLMs. We conduct an extensive evaluation of\nover 10 representative LVLMs, including GPT-4o and Gemini-Vision-Pro, to\nprovide an in-depth analysis of the hallucination issues in existing models.\nOur benchmark is publicly available at https://github.com/HQHBench/HQHBench.\n","authors":["Bei Yan","Jie Zhang","Zheng Yuan","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17109v1","updated":"2024-06-24T19:52:27Z","published":"2024-06-24T19:52:27Z","title":"GMT: Guided Mask Transformer for Leaf Instance Segmentation","summary":"  Leaf instance segmentation is a challenging multi-instance segmentation task,\naiming to separate and delineate each leaf in an image of a plant. The\ndelineation of each leaf is a necessary prerequisite task for several\nbiology-related applications such as the fine-grained monitoring of plant\ngrowth, and crop yield estimation. The task is challenging because\nself-similarity of instances is high (similar shape and colour) and instances\nvary greatly in size under heavy occulusion.\n  We believe that the key to overcoming the aforementioned challenges lies in\nthe specific spatial patterns of leaf distribution. For example, leaves\ntypically grow around the plant's center, with smaller leaves clustering and\noverlapped near this central point. In this paper, we propose a novel approach\nnamed Guided Mask Transformer (GMT), which contains three key components,\nnamely Guided Positional Encoding (GPE), Guided Embedding Fusion Module (GEFM)\nand Guided Dynamic Positional Queries (GDPQ), to extend the meta-architecture\nof Mask2Former and incorporate with a set of harmonic guide functions. These\nguide functions are tailored to the pixel positions of instances and trained to\nseparate distinct instances in an embedding space. The proposed GMT\nconsistently outperforms State-of-the-Art models on three public plant\ndatasets.\n","authors":["Feng Chen","Sotirios A. Tsaftaris","Mario Valerio Giuffrida"],"pdf_url":"https://arxiv.org/pdf/2406.17109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17100v1","updated":"2024-06-24T19:39:59Z","published":"2024-06-24T19:39:59Z","title":"Fine-tuning Diffusion Models for Enhancing Face Quality in Text-to-image\n  Generation","summary":"  Diffusion models (DMs) have achieved significant success in generating\nimaginative images given textual descriptions. However, they are likely to fall\nshort when it comes to real-life scenarios with intricate details.The\nlow-quality, unrealistic human faces in text-to-image generation are one of the\nmost prominent issues, hindering the wide application of DMs in practice.\nTargeting addressing such an issue, we first assess the face quality of\ngenerations from popular pre-trained DMs with the aid of human annotators and\nthen evaluate the alignment between existing metrics such as ImageReward, Human\nPreference Score, Aesthetic Score Predictor, and Face Quality Assessment, with\nhuman judgments. Observing that existing metrics can be unsatisfactory for\nquantifying face quality, we develop a novel metric named Face Score (FS) by\nfine-tuning ImageReward on a dataset of (good, bad) face pairs cheaply crafted\nby an inpainting pipeline of DMs. Extensive studies reveal that FS enjoys a\nsuperior alignment with humans. On the other hand, FS opens up the door for\nrefining DMs for better face generation. To achieve this, we incorporate a\nguidance loss on the denoising trajectories of the aforementioned face pairs\nfor fine-tuning pre-trained DMs such as Stable Diffusion V1.5 and Realistic\nVision V5.1. Intuitively, such a loss pushes the trajectory of bad faces toward\nthat of good ones. Comprehensive experiments verify the efficacy of our\napproach for improving face quality while preserving general capability.\n","authors":["Zhenyi Liao","Qingsong Xie","Chen Chen","Hannan Lu","Zhijie Deng"],"pdf_url":"https://arxiv.org/pdf/2406.17100v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.11837v2","updated":"2024-06-24T19:28:08Z","published":"2024-05-20T07:25:09Z","title":"Improving the Explain-Any-Concept by Introducing Nonlinearity to the\n  Trainable Surrogate Model","summary":"  In the evolving field of Explainable AI (XAI), interpreting the decisions of\ndeep neural networks (DNNs) in computer vision tasks is an important process.\nWhile pixel-based XAI methods focus on identifying significant pixels, existing\nconcept-based XAI methods use pre-defined or human-annotated concepts. The\nrecently proposed Segment Anything Model (SAM) achieved a significant step\nforward to prepare automatic concept sets via comprehensive instance\nsegmentation. Building upon this, the Explain Any Concept (EAC) model emerged\nas a flexible method for explaining DNN decisions. EAC model is based on using\na surrogate model which has one trainable linear layer to simulate the target\nmodel. In this paper, by introducing an additional nonlinear layer to the\noriginal surrogate model, we show that we can improve the performance of the\nEAC model. We compare our proposed approach to the original EAC model and\nreport improvements obtained on both ImageNet and MS COCO datasets.\n","authors":["Mounes Zaval","Sedat Ozer"],"pdf_url":"https://arxiv.org/pdf/2405.11837v2.pdf","comment":"This paper is accepted for publication at IEEE SIU conference, 2024"},{"id":"http://arxiv.org/abs/2406.17080v1","updated":"2024-06-24T19:09:20Z","published":"2024-06-24T19:09:20Z","title":"Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net)\n  for 3D Medical Image Segmentation and Visualization","summary":"  Vision Transformers have shown superior performance to the traditional\nconvolutional-based frameworks in many vision applications, including but not\nlimited to the segmentation of 3D medical images. To further advance this area,\nthis study introduces the Multi-Aperture Fusion of Transformer-Convolutional\nNetwork (MFTC-Net), which integrates the output of Swin Transformers and their\ncorresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture\nincorporates each image patch at its original resolutions with its pyramid\nrepresentation to better preserve minute details. The proposed architecture has\ndemonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the\nSynapse multi-organs dataset an improvement over the published results. The\nimproved performance also comes with the added benefits of the reduced\ncomplexity of approximately 40 million parameters. Our code is available at\nhttps://github.com/Siyavashshabani/MFTC-Net\n","authors":["Siyavash Shabani","Muhammad Sohaib","Sahar A. Mohammed","Bahram Parvin"],"pdf_url":"https://arxiv.org/pdf/2406.17080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17074v1","updated":"2024-06-24T19:01:44Z","published":"2024-06-24T19:01:44Z","title":"Reducing the Memory Footprint of 3D Gaussian Splatting","summary":"  3D Gaussian splatting provides excellent visual quality for novel view\nsynthesis, with fast training and real-time rendering; unfortunately, the\nmemory requirements of this method for storing and transmission are\nunreasonably high. We first analyze the reasons for this, identifying three\nmain areas where storage can be reduced: the number of 3D Gaussian primitives\nused to represent a scene, the number of coefficients for the spherical\nharmonics used to represent directional radiance, and the precision required to\nstore Gaussian primitive attributes. We present a solution to each of these\nissues. First, we propose an efficient, resolution-aware primitive pruning\napproach, reducing the primitive count by half. Second, we introduce an\nadaptive adjustment method to choose the number of coefficients used to\nrepresent directional radiance for each Gaussian primitive, and finally a\ncodebook-based quantization method, together with a half-float representation\nfor further memory reduction. Taken together, these three components result in\na 27 reduction in overall size on disk on the standard datasets we tested,\nalong with a 1.7 speedup in rendering speed. We demonstrate our method on\nstandard datasets and show how our solution results in significantly reduced\ndownload times when using the method on a mobile device.\n","authors":["Panagiotis Papantonakis","Georgios Kopanas","Bernhard Kerbl","Alexandre Lanvin","George Drettakis"],"pdf_url":"https://arxiv.org/pdf/2406.17074v1.pdf","comment":"Project website: https://repo-sam.inria.fr/fungraph/reduced_3dgs/"},{"id":"http://arxiv.org/abs/2406.17051v1","updated":"2024-06-24T18:13:09Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17047v1","updated":"2024-06-24T18:08:19Z","published":"2024-06-24T18:08:19Z","title":"Enhancing Scientific Figure Captioning Through Cross-modal Learning","summary":"  Scientific charts are essential tools for effectively communicating research\nfindings, serving as a vital medium for conveying information and revealing\ndata patterns. With the rapid advancement of science and technology, coupled\nwith the advent of the big data era, the volume and diversity of scientific\nresearch data have surged, leading to an increase in the number and variety of\ncharts. This trend presents new challenges for researchers, particularly in\nefficiently and accurately generating appropriate titles for these charts to\nbetter convey their information and results. Automatically generated chart\ntitles can enhance information retrieval systems by providing precise data for\ndetailed chart classification. As research in image captioning and text\nsummarization matures, the automatic generation of scientific chart titles has\ngained significant attention. By leveraging natural language processing,\nmachine learning, and multimodal techniques, it is possible to automatically\nextract key information from charts and generate accurate, concise titles that\nbetter serve the needs of researchers. This paper presents a novel approach to\nscientific chart title generation, demonstrating its effectiveness in improving\nthe clarity and accessibility of research data.\n","authors":["Mateo Alejandro Rojas","Rafael Carranza"],"pdf_url":"https://arxiv.org/pdf/2406.17047v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2405.05944v2","updated":"2024-06-24T18:05:06Z","published":"2024-05-09T17:33:09Z","title":"MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure\n  Segmentation Tool for T1-weighted Abdominal MRI","summary":"  Background: Segmentation of organs and structures in abdominal MRI is useful\nfor many clinical applications, such as disease diagnosis and radiotherapy.\nCurrent approaches have focused on delineating a limited set of abdominal\nstructures (13 types). To date, there is no publicly available abdominal MRI\ndataset with voxel-level annotations of multiple organs and structures.\nConsequently, a segmentation tool for multi-structure segmentation is also\nunavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting\nof 195 patients who underwent imaging at National Institutes of Health (NIH)\nClinical Center. The dataset comprises of axial pre-contrast T1, arterial,\nvenous, and delayed phases for each patient, thereby amounting to a total of\n780 series (69,248 2D slices). Each series contains voxel-level annotations of\n62 abdominal organs and structures. A 3D nnUNet model, dubbed as\nMRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,\nand evaluation was conducted on an internal test set and two large external\ndatasets: AMOS22 and Duke Liver. The predicted segmentations were compared\nagainst the ground-truth using the Dice Similarity Coefficient (DSC) and\nNormalized Surface Distance (NSD). Findings: MRISegmentator achieved an average\nDSC of 0.861$\\pm$0.170 and a NSD of 0.924$\\pm$0.163 in the internal test set.\nOn the AMOS22 dataset, MRISegmentator attained an average DSC of\n0.829$\\pm$0.133 and a NSD of 0.908$\\pm$0.067. For the Duke Liver dataset, an\naverage DSC of 0.933$\\pm$0.015 and a NSD of 0.929$\\pm$0.021 was obtained.\nInterpretation: The proposed MRISegmentator provides automatic, accurate, and\nrobust segmentations of 62 organs and structures in T1-weighted abdominal MRI\nsequences. The tool has the potential to accelerate research on various\nclinical topics, such as abnormality detection, radiotherapy, disease\nclassification among others.\n","authors":["Yan Zhuang","Tejas Sudharshan Mathai","Pritam Mukherjee","Brandon Khoury","Boah Kim","Benjamin Hou","Nusrat Rabbee","Abhinav Suri","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2405.05944v2.pdf","comment":"We made the segmentation model publicly available"},{"id":"http://arxiv.org/abs/2406.17032v1","updated":"2024-06-24T18:00:11Z","published":"2024-06-24T18:00:11Z","title":"Dwarf: Disease-weighted network for attention map refinement","summary":"  The interpretability of deep learning is crucial for evaluating the\nreliability of medical imaging models and reducing the risks of inaccurate\npatient recommendations. This study addresses the \"human out of the loop\" and\n\"trustworthiness\" issues in medical image analysis by integrating medical\nprofessionals into the interpretability process. We propose a disease-weighted\nattention map refinement network (Dwarf) that leverages expert feedback to\nenhance model relevance and accuracy. Our method employs cyclic training to\niteratively improve diagnostic performance, generating precise and\ninterpretable feature maps. Experimental results demonstrate significant\nimprovements in interpretability and diagnostic accuracy across multiple\nmedical imaging datasets. This approach fosters effective collaboration between\nAI systems and healthcare professionals, ultimately aiming to improve patient\noutcomes\n","authors":["Haozhe Luo","Aurélie Pahud de Mortanges","Oana Inel","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2406.17032v1.pdf","comment":null}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2406.16835v1","updated":"2024-06-24T17:44:17Z","published":"2024-06-24T17:44:17Z","title":"Preserving Real-World Finger Dexterity Using a Lightweight Fingertip\n  Haptic Device for Virtual Dexterous Manipulation","summary":"  This study presents a lightweight, wearable fingertip haptic device that\nprovides physics-based haptic feedback for dexterous manipulation in virtual\nenvironments without hindering real-world interactions. The device's design\nutilizes thin strings and actuators attached to the fingernails, minimizing the\nweight (1.76g each finger) while preserving finger flexibility. Multiple types\nof haptic feedback are simulated by integrating the software with a physics\nengine. Experiments evaluate the device's performance in pressure perception,\nslip feedback, and typical dexterous manipulation tasks. and daily operations,\nwhile subjective assessments gather user experiences. Results demonstrate that\nparticipants can perceive and respond to pressure and vibration feedback. These\nlimited haptic cues are crucial as they significantly enhance efficiency in\nvirtual dexterous manipulation tasks. The device's ability to preserve tactile\nsensations and minimize hindrance to real-world operations is a key advantage\nover glove-type haptic devices. This research offers a potential solution for\ndesigning haptic interfaces that balance lightweight, haptic feedback for\ndexterous manipulation and daily wearability.\n","authors":["Yunxiu XU","Siyu Wang","Shoichi Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2406.16835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16737v1","updated":"2024-06-24T15:44:55Z","published":"2024-06-24T15:44:55Z","title":"A Digital Human Model for Symptom Progression of Vestibular Motion\n  Sickness based on Subjective Vertical Conflict Theory","summary":"  Digital human models of motion sickness have been actively developed, among\nwhich models based on subjective vertical conflict (SVC) theory are the most\nactively studied. These models facilitate the prediction of motion sickness in\nvarious scenarios such as riding in a car. Most SVC theory models predict the\nmotion sickness incidence (MSI), which is defined as the percentage of people\nwho would vomit with the given specific motion stimulus. However, no model has\nbeen developed to describe milder forms of discomfort or specific symptoms of\nmotion sickness, even though predicting milder symptoms is important for\napplications in automobiles and daily use vehicles. Therefore, the purpose of\nthis study was to build a computational model of symptom progression of\nvestibular motion sickness based on SVC theory. We focused on a model of\nvestibular motion sickness with six degrees-of-freedom (6DoF) head motions. The\nmodel was developed by updating the output part of the state-of-the-art SVC\nmodel, termed the 6DoF-SVC (IN1) model, from MSI to the MIsery SCale (MISC),\nwhich is a subjective rating scale for symptom progression. We conducted an\nexperiment to measure the progression of motion sickness during a straight\nfore-aft motion. It was demonstrated that our proposed method, with the\nparameters of the output parts optimized by the experimental results, fits well\nwith the observed MISC.\n","authors":["Shota Inoue","Hailong Liu","Takahiro Wada"],"pdf_url":"https://arxiv.org/pdf/2406.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13857v2","updated":"2024-06-24T13:04:31Z","published":"2024-05-22T17:32:04Z","title":"What Do Privacy Advertisements Communicate to Consumers?","summary":"  When companies release marketing materials aimed at promoting their privacy\npractices or highlighting specific privacy features, what do they actually\ncommunicate to consumers? In this paper, we explore the impact of privacy\nmarketing on: (1) consumers' attitudes toward the organizations providing the\ncampaigns, (2) overall privacy awareness, and (3) the actionability of\nsuggested privacy advice. To this end, we investigated the impact of four\nprivacy advertising videos and one privacy game published by five different\ntechnology companies. We conducted 24 semi-structured interviews with\nparticipants randomly assigned to view one or two of the videos or play the\ngame. Our findings suggest that awareness of privacy features can contribute to\npositive perceptions of a company or its products. The ads we tested were more\nsuccessful in communicating the advertised privacy features than the game we\ntested. We observed that advertising a single privacy feature using a single\nmetaphor in a short ad increased awareness of the advertised feature. The game\nfailed to communicate privacy features or motivate study participants to use\nthe features. Our results also suggest that privacy campaigns can be useful for\nraising awareness about privacy features and improving brand image, but may not\nbe the most effective way to teach viewers how to use privacy features.\n","authors":["Xiaoxin Shen","Eman Alashwali","Lorrie Faith Cranor"],"pdf_url":"https://arxiv.org/pdf/2405.13857v2.pdf","comment":"This document is the author's manuscript for a paper to appear in\n  Proceedings on Privacy Enhancing Technologies 2024(4)"},{"id":"http://arxiv.org/abs/2406.16572v1","updated":"2024-06-24T12:09:34Z","published":"2024-06-24T12:09:34Z","title":"ChatGPT's financial discrimination between rich and poor -- misaligned\n  with human behavior and expectations","summary":"  ChatGPT disrupted the application of machine-learning methods and drastically\nreduced the usage barrier. Chatbots are now widely used in a lot of different\nsituations. They provide advice, assist in writing source code, or assess and\nsummarize information from various sources. However, their scope is not only\nlimited to aiding humans; they can also be used to take on tasks like\nnegotiating or bargaining. To understand the implications of Chatbot usage on\nbargaining situations, we conduct a laboratory experiment with the ultimatum\ngame. In the ultimatum game, two human players interact: The receiver decides\non accepting or rejecting a monetary offer from the proposer. To shed light on\nthe new bargaining situation, we let ChatGPT provide an offer to a human\nplayer. In the novel design, we vary the wealth of the receivers. Our results\nindicate that humans have the same beliefs about other humans and chatbots.\nHowever, our results contradict these beliefs in an important point: Humans\nfavor poor receivers as correctly anticipated by the humans, but ChatGPT favors\nrich receivers which the humans did not expect to happen. These results imply\nthat ChatGPT's answers are not aligned with those of humans and that humans do\nnot anticipate this difference.\n","authors":["Dmitri Bershadskyy","Florian E. Sachs","Joachim Weimann"],"pdf_url":"https://arxiv.org/pdf/2406.16572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14965v4","updated":"2024-06-24T11:53:02Z","published":"2024-04-23T12:20:14Z","title":"Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction","summary":"  The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.\n","authors":["Yuchong Zhang","Yong Ma","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2404.14965v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13813v2","updated":"2024-06-24T08:18:29Z","published":"2024-06-19T20:20:28Z","title":"The Efficacy of Conversational Artificial Intelligence in Rectifying the\n  Theory of Mind and Autonomy Biases: Comparative Analysis","summary":"  The study evaluates the efficacy of Conversational Artificial Intelligence\n(CAI) in rectifying cognitive biases and recognizing affect in human-AI\ninteractions, which is crucial for digital mental health interventions.\nCognitive biases (systematic deviations from normative thinking) affect mental\nhealth, intensifying conditions like depression and anxiety. Therapeutic\nchatbots can make cognitive-behavioral therapy (CBT) more accessible and\naffordable, offering scalable and immediate support. The research employs a\nstructured methodology with clinical-based virtual case scenarios simulating\ntypical user-bot interactions. Performance and affect recognition were assessed\nacross two categories of cognitive biases: theory of mind biases\n(anthropomorphization of AI, overtrust in AI, attribution to AI) and autonomy\nbiases (illusion of control, fundamental attribution error, just-world\nhypothesis). A qualitative feedback mechanism was used with an ordinal scale to\nquantify responses based on accuracy, therapeutic quality, and adherence to CBT\nprinciples. Therapeutic bots (Wysa, Youper) and general-use LLMs (GTP 3.5, GTP\n4, Gemini Pro) were evaluated through scripted interactions, double-reviewed by\ncognitive scientists and a clinical psychologist. Statistical analysis showed\ntherapeutic bots were consistently outperformed by non-therapeutic bots in bias\nrectification and in 4 out of 6 biases in affect recognition. The data suggests\nthat non-therapeutic chatbots are more effective in addressing some cognitive\nbiases.\n","authors":["Marcin Rządeczka","Anna Sterna","Julia Stolińska","Paulina Kaczyńska","Marcin Moskalewicz"],"pdf_url":"https://arxiv.org/pdf/2406.13813v2.pdf","comment":"28 pages, 5 tables, 6 figures"},{"id":"http://arxiv.org/abs/2404.14869v2","updated":"2024-06-24T08:02:17Z","published":"2024-04-23T09:51:24Z","title":"EEGEncoder: Advancing BCI with Transformer-Based Motor Imagery\n  Classification","summary":"  Brain-computer interfaces (BCIs) harness electroencephalographic signals for\ndirect neural control of devices, offering a significant benefit for\nindividuals with motor impairments. Traditional machine learning methods for\nEEG-based motor imagery (MI) classification encounter challenges such as manual\nfeature extraction and susceptibility to noise.This paper introduces\nEEGEncoder, a deep learning framework that employs modified transformers and\nTCNs to surmount these limitations. We innovatively propose a fusion\narchitecture, namely Dual-Stream Temporal-Spatial Block (DSTS), to capture\ntemporal and spatial features, improving the accuracy of Motor Imagery\nclassification task. Additionally, we use multiple parallel structures to\nenhance the performance of the model. When tested on the BCI Competition IV-2a\ndataset, our model results outperform current state-of-the-art techniques.\n","authors":["Wangdan Liao","Weidong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.14869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16388v1","updated":"2024-06-24T07:59:34Z","published":"2024-06-24T07:59:34Z","title":"PenSLR: Persian end-to-end Sign Language Recognition Using Ensembling","summary":"  Sign Language Recognition (SLR) is a fast-growing field that aims to fill the\ncommunication gaps between the hearing-impaired and people without hearing\nloss. Existing solutions for Persian Sign Language (PSL) are limited to\nword-level interpretations, underscoring the need for more advanced and\ncomprehensive solutions. Moreover, previous work on other languages mainly\nfocuses on manipulating the neural network architectures or hardware\nconfigurations instead of benefiting from the aggregated results of multiple\nmodels. In this paper, we introduce PenSLR, a glove-based sign language system\nconsisting of an Inertial Measurement Unit (IMU) and five flexible sensors\npowered by a deep learning framework capable of predicting variable-length\nsequences. We achieve this in an end-to-end manner by leveraging the\nConnectionist Temporal Classification (CTC) loss function, eliminating the need\nfor segmentation of input signals. To further enhance its capabilities, we\npropose a novel ensembling technique by leveraging a multiple sequence\nalignment algorithm known as Star Alignment. Furthermore, we introduce a new\nPSL dataset, including 16 PSL signs with more than 3000 time-series samples in\ntotal. We utilize this dataset to evaluate the performance of our system based\non four word-level and sentence-level metrics. Our evaluations show that PenSLR\nachieves a remarkable word accuracy of 94.58% and 96.70% in subject-independent\nand subject-dependent setups, respectively. These achievements are attributable\nto our ensembling algorithm, which not only boosts the word-level performance\nby 0.51% and 1.32% in the respective scenarios but also yields significant\nenhancements of 1.46% and 4.00%, respectively, in sentence-level accuracy.\n","authors":["Amirparsa Salmankhah","Amirreza Rajabi","Negin Kheirmand","Ali Fadaeimanesh","Amirreza Tarabkhah","Amirreza Kazemzadeh","Hamed Farbeh"],"pdf_url":"https://arxiv.org/pdf/2406.16388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09538v3","updated":"2024-06-24T02:11:44Z","published":"2023-11-16T03:28:43Z","title":"Reducing Privacy Risks in Online Self-Disclosures with Language Models","summary":"  Self-disclosure, while being common and rewarding in social media\ninteraction, also poses privacy risks. In this paper, we take the initiative to\nprotect the user-side privacy associated with online self-disclosure through\ndetection and abstraction. We develop a taxonomy of 19 self-disclosure\ncategories and curate a large corpus consisting of 4.8K annotated disclosure\nspans. We then fine-tune a language model for detection, achieving over 65%\npartial span F$_1$. We further conduct an HCI user study, with 82% of\nparticipants viewing the model positively, highlighting its real-world\napplicability. Motivated by the user feedback, we introduce the task of\nself-disclosure abstraction, which is rephrasing disclosures into less specific\nterms while preserving their utility, e.g., \"Im 16F\" to \"I'm a teenage girl\".\nWe explore various fine-tuning strategies, and our best model can generate\ndiverse abstractions that moderately reduce privacy risks while maintaining\nhigh utility according to human evaluation. To help users in deciding which\ndisclosures to abstract, we present a task of rating their importance for\ncontext understanding. Our fine-tuned model achieves 80% accuracy, on-par with\nGPT-3.5. Given safety and privacy considerations, we will only release our\ncorpus and models to researcher who agree to the ethical guidelines outlined in\nEthics Statement.\n","authors":["Yao Dou","Isadora Krsek","Tarek Naous","Anubha Kabra","Sauvik Das","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2311.09538v3.pdf","comment":"Accepted at ACL 2024"},{"id":"http://arxiv.org/abs/2402.03049v4","updated":"2024-06-24T02:10:23Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v4.pdf","comment":"ACL 2024 System Demonstrations; Project website:\n  https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2406.17181v1","updated":"2024-06-24T23:40:19Z","published":"2024-06-24T23:40:19Z","title":"FacePsy: An Open-Source Affective Mobile Sensing System -- Analyzing\n  Facial Behavior and Head Gesture for Depression Detection in Naturalistic\n  Settings","summary":"  Depression, a prevalent and complex mental health issue affecting millions\nworldwide, presents significant challenges for detection and monitoring. While\nfacial expressions have shown promise in laboratory settings for identifying\ndepression, their potential in real-world applications remains largely\nunexplored due to the difficulties in developing efficient mobile systems. In\nthis study, we aim to introduce FacePsy, an open-source mobile sensing system\ndesigned to capture affective inferences by analyzing sophisticated features\nand generating real-time data on facial behavior landmarks, eye movements, and\nhead gestures -- all within the naturalistic context of smartphone usage with\n25 participants. Through rigorous development, testing, and optimization, we\nidentified eye-open states, head gestures, smile expressions, and specific\nAction Units (2, 6, 7, 12, 15, and 17) as significant indicators of depressive\nepisodes (AUROC=81%). Our regression model predicting PHQ-9 scores achieved\nmoderate accuracy, with a Mean Absolute Error of 3.08. Our findings offer\nvaluable insights and implications for enhancing deployable and usable mobile\naffective sensing systems, ultimately improving mental health monitoring,\nprediction, and just-in-time adaptive interventions for researchers and\ndevelopers in healthcare.\n","authors":["Rahul Islam","Sang Won Bae"],"pdf_url":"https://arxiv.org/pdf/2406.17181v1.pdf","comment":"Accepted to ACM International Conference on Mobile Human-Computer\n  Interaction (MobileHCI 2024)"},{"id":"http://arxiv.org/abs/2406.17156v1","updated":"2024-06-24T22:02:10Z","published":"2024-06-24T22:02:10Z","title":"Toward Ubiquitous 3D Object Digitization: A Wearable Computing Framework\n  for Non-Invasive Physical Property Acquisition","summary":"  Accurately digitizing physical objects is central to many applications,\nincluding virtual/augmented reality, industrial design, and e-commerce. Prior\nresearch has demonstrated efficient and faithful reconstruction of objects'\ngeometric shapes and visual appearances, which suffice for digitally\nrepresenting rigid objects. In comparison, physical properties, such as\nelasticity and pressure, are also indispensable to the behavioral fidelity of\ndigitized deformable objects. However, existing approaches to acquiring these\nquantities either rely on invasive specimen collection or expensive/bulky\nlaboratory setups, making them inapplicable to consumer-level usage.\n  To fill this gap, we propose a wearable and non-invasive computing framework\nthat allows users to conveniently estimate the material elasticity and internal\npressure of deformable objects through finger touches. This is achieved by\nmodeling their local surfaces as pressurized elastic shells and analytically\nderiving the two physical properties from finger-induced wrinkling patterns.\nTogether with photogrammetry-reconstructed geometry and textures, the two\nestimated physical properties enable us to faithfully replicate the motion and\ndeformation behaviors of several deformable objects. For the pressure\nestimation, our model achieves a relative error of 3.5%. In the interaction\nexperiments, the virtual-physical deformation discrepancy measures less than\n10.1%. Generalization to objects of irregular shape further demonstrates the\npotential of our approach in practical applications. We envision this work to\nprovide insights for and motivate research toward democratizing the ubiquitous\nand pervasive digitization of our physical surroundings in daily, industrial,\nand scientific scenarios.\n","authors":["Yunxiang Zhang","Xin Sun","Dengfeng Li","Xinge Yu","Qi Sun"],"pdf_url":"https://arxiv.org/pdf/2406.17156v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.17097v1","updated":"2024-06-24T19:35:41Z","published":"2024-06-24T19:35:41Z","title":"Lower Quantity, Higher Quality: Auditing News Content and User\n  Perceptions on Twitter/X Algorithmic versus Chronological Timelines","summary":"  Social media personalization algorithms increasingly influence the flow of\ncivic information through society, resulting in concerns about \"filter\nbubbles\", \"echo chambers\", and other ways they might exacerbate ideological\nsegregation and fan the spread of polarizing content. To address these\nconcerns, we designed and conducted a sociotechnical audit (STA) to investigate\nhow Twitter/X's timeline algorithm affects news curation while also tracking\nhow user perceptions change in response. We deployed a custom-built system\nthat, over the course of three weeks, passively tracked all tweets loaded in\nusers' browsers in the first week, then in the second week enacted an\nintervention to users' Twitter/X homepage to restrict their view to only the\nalgorithmic or chronological timeline (randomized). We flipped this condition\nfor each user in the third week. We ran our audit in late 2023, collecting\nuser-centered metrics (self-reported survey measures) and platform-centered\nmetrics (views, clicks, likes) for 243 users, along with over 800,000 tweets.\nUsing the STA framework, our results are two-fold: (1) Our algorithm audit\nfinds that Twitter/X's algorithmic timeline resulted in a lower quantity but\nhigher quality of news -- less ideologically congruent, less extreme, and\nslightly more reliable -- compared to the chronological timeline. (2) Our user\naudit suggests that although our timeline intervention had significant effects\non users' behaviors, it had little impact on their overall perceptions of the\nplatform. Our paper discusses these findings and their broader implications in\nthe context of algorithmic news curation, user-centric audits, and avenues for\nindependent social science research.\n","authors":["Stephanie Wang","Shengchun Huang","Alvin Zhou","Danaë Metaxa"],"pdf_url":"https://arxiv.org/pdf/2406.17097v1.pdf","comment":"24 pages, 5 figures, Computer-Supported Cooperative Work"}]},"2024-06-23T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.03626v2","updated":"2024-06-23T23:50:59Z","published":"2023-12-06T17:13:15Z","title":"TokenCompose: Text-to-Image Diffusion with Token-level Supervision","summary":"  We present TokenCompose, a Latent Diffusion Model for text-to-image\ngeneration that achieves enhanced consistency between user-specified text\nprompts and model-generated images. Despite its tremendous success, the\nstandard denoising process in the Latent Diffusion Model takes text prompts as\nconditions only, absent explicit constraint for the consistency between the\ntext prompts and the image contents, leading to unsatisfactory results for\ncomposing multiple object categories. TokenCompose aims to improve\nmulti-category instance composition by introducing the token-wise consistency\nterms between the image content and object segmentation maps in the finetuning\nstage. TokenCompose can be applied directly to the existing training pipeline\nof text-conditioned diffusion models without extra human labeling information.\nBy finetuning Stable Diffusion, the model exhibits significant improvements in\nmulti-category instance composition and enhanced photorealism for its generated\nimages. Project link: https://mlpc-ucsd.github.io/TokenCompose\n","authors":["Zirui Wang","Zhizhou Sha","Zheng Ding","Yilin Wang","Zhuowen Tu"],"pdf_url":"https://arxiv.org/pdf/2312.03626v2.pdf","comment":"CVPR 2024, 21 pages, 17 figures"},{"id":"http://arxiv.org/abs/2406.08929v2","updated":"2024-06-23T23:18:07Z","published":"2024-06-13T08:58:45Z","title":"Step-by-Step Diffusion: An Elementary Tutorial","summary":"  We present an accessible first course on diffusion models and flow matching\nfor machine learning, aimed at a technical audience with no diffusion\nexperience. We try to simplify the mathematical details as much as possible\n(sometimes heuristically), while retaining enough precision to derive correct\nalgorithms.\n","authors":["Preetum Nakkiran","Arwen Bradley","Hattie Zhou","Madhu Advani"],"pdf_url":"https://arxiv.org/pdf/2406.08929v2.pdf","comment":"35 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.16231v1","updated":"2024-06-23T22:05:52Z","published":"2024-06-23T22:05:52Z","title":"Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental\n  Learning Method","summary":"  Domain incremental learning (DIL) poses a significant challenge in real-world\nscenarios, as models need to be sequentially trained on diverse domains over\ntime, all the while avoiding catastrophic forgetting. Mitigating representation\ndrift, which refers to the phenomenon of learned representations undergoing\nchanges as the model adapts to new tasks, can help alleviate catastrophic\nforgetting. In this study, we propose a novel DIL method named DARE, featuring\na three-stage training process: Divergence, Adaptation, and REfinement. This\nprocess gradually adapts the representations associated with new tasks into the\nfeature space spanned by samples from previous tasks, simultaneously\nintegrating task-specific decision boundaries. Additionally, we introduce a\nnovel strategy for buffer sampling and demonstrate the effectiveness of our\nproposed method, combined with this sampling strategy, in reducing\nrepresentation drift within the feature encoder. This contribution effectively\nalleviates catastrophic forgetting across multiple DIL benchmarks. Furthermore,\nour approach prevents sudden representation drift at task boundaries, resulting\nin a well-calibrated DIL model that maintains the performance on previous\ntasks.\n","authors":["Kishaan Jeeveswaran","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2406.16231v1.pdf","comment":"Accepted at 41st International Conference on Machine Learning (ICML\n  2024)"},{"id":"http://arxiv.org/abs/2406.07741v2","updated":"2024-06-23T21:54:26Z","published":"2024-06-11T21:55:20Z","title":"Back to the Color: Learning Depth to Specific Color Transformation for\n  Unsupervised Depth Estimation","summary":"  Virtual engines have the capability to generate dense depth maps for various\nsynthetic scenes, making them invaluable for training depth estimation models.\nHowever, synthetic colors often exhibit significant discrepancies compared to\nreal-world colors, thereby posing challenges for depth estimation in real-world\nscenes, particularly in complex and uncertain environments encountered in\nunsupervised monocular depth estimation tasks. To address this issue, we\npropose Back2Color, a framework that predicts realistic colors from depth\nutilizing a model trained on real-world data, thus facilitating the\ntransformation of synthetic colors into real-world counterparts. Additionally,\nby employing the Syn-Real CutMix method for joint training with both real-world\nunsupervised and synthetic supervised depth samples, we achieve improved\nperformance in monocular depth estimation for real-world scenes. Moreover, to\ncomprehensively address the impact of non-rigid motions on depth estimation, we\npropose an auto-learning uncertainty temporal-spatial fusion method\n(Auto-UTSF), which integrates the benefits of unsupervised learning in both\ntemporal and spatial dimensions. Furthermore, we design a depth estimation\nnetwork (VADepth) based on the Vision Attention Network. Our Back2Color\nframework demonstrates state-of-the-art performance, as evidenced by\nimprovements in performance metrics and the production of fine-grained details\nin our predictions, particularly on challenging datasets such as Cityscapes for\nunsupervised depth estimation.\n","authors":["Yufan Zhu","Chongzhi Ran","Mingtao Feng","Weisheng Dong","Antonio M. López","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2406.07741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16220v1","updated":"2024-06-23T21:25:06Z","published":"2024-06-23T21:25:06Z","title":"Learning Run-time Safety Monitors for Machine Learning Components","summary":"  For machine learning components used as part of autonomous systems (AS) in\ncarrying out critical tasks it is crucial that assurance of the models can be\nmaintained in the face of post-deployment changes (such as changes in the\noperating environment of the system). A critical part of this is to be able to\nmonitor when the performance of the model at runtime (as a result of changes)\nposes a safety risk to the system. This is a particularly difficult challenge\nwhen ground truth is unavailable at runtime. In this paper we introduce a\nprocess for creating safety monitors for ML components through the use of\ndegraded datasets and machine learning. The safety monitor that is created is\ndeployed to the AS in parallel to the ML component to provide a prediction of\nthe safety risk associated with the model output. We demonstrate the viability\nof our approach through some initial experiments using publicly available speed\nsign datasets.\n","authors":["Ozan Vardal","Richard Hawkins","Colin Paterson","Chiara Picardi","Daniel Omeiza","Lars Kunze","Ibrahim Habli"],"pdf_url":"https://arxiv.org/pdf/2406.16220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08477v2","updated":"2024-06-23T20:51:21Z","published":"2024-03-13T12:46:03Z","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts","summary":"  Recent successes suggest that parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-distribution (OOD) tasks.\nIn this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by\nsparse mixture-of-experts approaches and trained to isolate subsets of\npre-trained parameters automatically for meta-tuning on each task. SMAT\nsuccessfully overcomes OOD sensitivity and delivers on the promise of enhancing\nthe transfer abilities of vision foundation models beyond parameter-efficient\nfine-tuning. We establish new state-of-the-art results on a challenging\ncombination of Meta-Dataset augmented with additional OOD tasks in both\nzero-shot and gradient-based adaptation settings. In addition, we provide a\nthorough analysis of the superiority of learned over hand-designed sparsity\npatterns for sparse expert methods and the pivotal importance of the sparsity\nlevel in balancing between in-distribution and out-of-distribution\ngeneralization. Our code is publicly available.\n","authors":["Shengzhuang Chen","Jihoon Tack","Yunqiao Yang","Yee Whye Teh","Jonathan Richard Schwarz","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2403.08477v2.pdf","comment":"The Forty-first International Conference on Machine Learning, 2024"},{"id":"http://arxiv.org/abs/2406.16204v1","updated":"2024-06-23T20:00:20Z","published":"2024-06-23T20:00:20Z","title":"Breaking the Frame: Image Retrieval by Visual Overlap Prediction","summary":"  We propose a novel visual place recognition approach, VOP, that efficiently\naddresses occlusions and complex scenes by shifting from traditional reliance\non global image similarities and local features to image overlap prediction.\nThe proposed method enables the identification of visible image sections\nwithout requiring expensive feature detection and matching. By focusing on\nobtaining patch-level embeddings by a Vision Transformer backbone and\nestablishing patch-to-patch correspondences, our approach uses a voting\nmechanism to assess overlap scores for potential database images, thereby\nproviding a nuanced image retrieval metric in challenging scenarios. VOP leads\nto more accurate relative pose estimation and localization results on the\nretrieved image pairs than state-of-the-art baselines on a number of\nlarge-scale, real-world datasets. The code is available at\nhttps://github.com/weitong8591/vop.\n","authors":["Tong Wei","Philipp Lindenberger","Jiri Matas","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2406.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00736v3","updated":"2024-06-23T19:32:56Z","published":"2024-01-01T12:25:57Z","title":"Diffusion Models, Image Super-Resolution And Everything: A Survey","summary":"  Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field\nand further closed the gap between image quality and human perceptual\npreferences. They are easy to train and can produce very high-quality samples\nthat exceed the realism of those produced by previous generative methods.\nDespite their promising results, they also come with new challenges that need\nfurther research: high computational demands, comparability, lack of\nexplainability, color shifts, and more. Unfortunately, entry into this field is\noverwhelming because of the abundance of publications. To address this, we\nprovide a unified recount of the theoretical foundations underlying DMs applied\nto image SR and offer a detailed analysis that underscores the unique\ncharacteristics and methodologies within this domain, distinct from broader\nexisting reviews in the field. This survey articulates a cohesive understanding\nof DM principles and explores current research avenues, including alternative\ninput domains, conditioning techniques, guidance mechanisms, corruption spaces,\nand zero-shot learning approaches. By offering a detailed examination of the\nevolution and current trends in image SR through the lens of DMs, this survey\nsheds light on the existing challenges and charts potential future directions,\naiming to inspire further innovation in this rapidly advancing area.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Federico Raue","Stanislav Frolov","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2401.00736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12834v2","updated":"2024-06-23T19:10:29Z","published":"2024-06-18T17:54:17Z","title":"GroPrompt: Efficient Grounded Prompting and Adaptation for Referring\n  Video Object Segmentation","summary":"  Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence throughout the entire video. Most existing\nmethods require end-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we aim to efficiently\nadapt foundation segmentation models for addressing RVOS from weak supervision\nwith the proposed Grounded Prompting (GroPrompt) framework. More specifically,\nwe propose Text-Aware Prompt Contrastive Learning (TAP-CL) to enhance the\nassociation between the position prompts and the referring sentences with only\nbox supervisions, including Text-Contrastive Prompt Learning (TextCon) and\nModality-Contrastive Prompt Learning (ModalCon) at frame level and video level,\nrespectively. With the proposed TAP-CL, our GroPrompt framework can generate\ntemporal-consistent yet text-aware position prompts describing locations and\nmovements for the referred object from the video. The experimental results in\nthe standard RVOS benchmarks (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, and\nJHMDB-Sentences) demonstrate the competitive performance of our proposed\nGroPrompt framework given only bounding box weak supervisions.\n","authors":["Ci-Siang Lin","I-Jieh Liu","Min-Hung Chen","Chien-Yi Wang","Sifei Liu","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12834v2.pdf","comment":"CVPR Workshop (CVinW) 2024. Project page:\n  https://jack24658735.github.io/groprompt/"},{"id":"http://arxiv.org/abs/2406.16192v1","updated":"2024-06-23T19:04:13Z","published":"2024-06-23T19:04:13Z","title":"HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image\n  Analysis","summary":"  Spatial transcriptomics (ST) enables interrogating the molecular composition\nof tissue with ever-increasing resolution, depth, and sensitivity. However,\ncosts, rapidly evolving technology, and lack of standards have constrained\ncomputational methods in ST to narrow tasks and small cohorts. In addition, the\nunderlying tissue morphology as reflected by H&E-stained whole slide images\n(WSIs) encodes rich information often overlooked in ST studies. Here, we\nintroduce HEST-1k, a collection of 1,108 spatial transcriptomic profiles, each\nlinked to a WSI and metadata. HEST-1k was assembled using HEST-Library from 131\npublic and internal cohorts encompassing 25 organs, two species (Homo Sapiens\nand Mus Musculus), and 320 cancer samples from 25 cancer types. HEST-1k\nprocessing enabled the identification of 1.5 million expression--morphology\npairs and 60 million nuclei. HEST-1k is tested on three use cases: (1)\nbenchmarking foundation models for histopathology (HEST-Benchmark), (2)\nbiomarker identification, and (3) multimodal representation learning. HEST-1k,\nHEST-Library, and HEST-Benchmark can be freely accessed via\nhttps://github.com/mahmoodlab/hest.\n","authors":["Guillaume Jaume","Paul Doucet","Andrew H. Song","Ming Y. Lu","Cristina Almagro-Pérez","Sophia J. Wagner","Anurag J. Vaidya","Richard J. Chen","Drew F. K. Williamson","Ahrong Kim","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2406.16192v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.16189v1","updated":"2024-06-23T18:47:51Z","published":"2024-06-23T18:47:51Z","title":"Fuzzy Attention-based Border Rendering Network for Lung Organ\n  Segmentation","summary":"  Automatic lung organ segmentation on CT images is crucial for lung disease\ndiagnosis. However, the unlimited voxel values and class imbalance of lung\norgans can lead to false-negative/positive and leakage issues in advanced\nmethods. Additionally, some slender lung organs are easily lost during the\nrecycled down/up-sample procedure, e.g., bronchioles & arterioles, causing\nsevere discontinuity issue. Inspired by these, this paper introduces an\neffective lung organ segmentation method called Fuzzy Attention-based Border\nRendering (FABR) network. Since fuzzy logic can handle the uncertainty in\nfeature extraction, hence the fusion of deep networks and fuzzy sets should be\na viable solution for better performance. Meanwhile, unlike prior top-tier\nmethods that operate on all regular dense points, our FABR depicts lung organ\nregions as cube-trees, focusing only on recycle-sampled border vulnerable\npoints, rendering the severely discontinuous, false-negative/positive organ\nregions with a novel Global-Local Cube-tree Fusion (GLCF) module. All\nexperimental results, on four challenging datasets of airway & artery,\ndemonstrate that our method can achieve the favorable performance\nsignificantly.\n","authors":["Sheng Zhang","Yang Nan","Yingying Fang","Shiyi Wang","Xiaodan Xing","Zhifan Gao","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.16189v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16187v1","updated":"2024-06-23T18:43:46Z","published":"2024-06-23T18:43:46Z","title":"Evaluation and Comparison of Emotionally Evocative Image Augmentation\n  Methods","summary":"  Experiments in affective computing are based on stimulus datasets that, in\nthe process of standardization, receive metadata describing which emotions each\nstimulus evokes. In this paper, we explore an approach to creating stimulus\ndatasets for affective computing using generative adversarial networks (GANs).\nTraditional dataset preparation methods are costly and time consuming,\nprompting our investigation of alternatives. We conducted experiments with\nvarious GAN architectures, including Deep Convolutional GAN, Conditional GAN,\nAuxiliary Classifier GAN, Progressive Augmentation GAN, and Wasserstein GAN,\nalongside data augmentation and transfer learning techniques. Our findings\nhighlight promising advances in the generation of emotionally evocative\nsynthetic images, suggesting significant potential for future research and\nimprovements in this domain.\n","authors":["Jan Ignatowicz","Krzysztof Kutt","Grzegorz J. Nalepa"],"pdf_url":"https://arxiv.org/pdf/2406.16187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12289v4","updated":"2024-06-23T17:48:42Z","published":"2024-02-19T17:04:04Z","title":"DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models","summary":"  A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of reasoning\nmodules for scene description, scene analysis, and hierarchical planning.\nFurthermore, recognizing the limitations of VLMs in spatial reasoning and heavy\ncomputational requirements, we propose DriveVLM-Dual, a hybrid system that\nsynergizes the strengths of DriveVLM with the traditional autonomous driving\npipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset\ndemonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and\nunpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a\nproduction vehicle, verifying it is effective in real-world autonomous driving\nenvironments.\n","authors":["Xiaoyu Tian","Junru Gu","Bailin Li","Yicheng Liu","Yang Wang","Zhiyong Zhao","Kun Zhan","Peng Jia","Xianpeng Lang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12289v4.pdf","comment":"Project Page: https://tsinghua-mars-lab.github.io/DriveVLM/"},{"id":"http://arxiv.org/abs/2402.07894v2","updated":"2024-06-23T16:11:19Z","published":"2024-02-12T18:56:53Z","title":"MODIPHY: Multimodal Obscured Detection for IoT using PHantom\n  Convolution-Enabled Faster YOLO","summary":"  Low-light conditions and occluded scenarios impede object detection in\nreal-world Internet of Things (IoT) applications like autonomous vehicles and\nsecurity systems. While advanced machine learning models strive for accuracy,\ntheir computational demands clash with the limitations of resource-constrained\ndevices, hampering real-time performance. In our current research, we tackle\nthis challenge, by introducing ``YOLO Phantom\", one of the smallest YOLO models\never conceived. YOLO Phantom utilizes the novel Phantom Convolution block,\nachieving comparable accuracy to the latest YOLOv8n model while simultaneously\nreducing both parameters and model size by 43\\%, resulting in a significant\n19\\% reduction in Giga Floating-Point Operations (GFLOPs). YOLO Phantom\nleverages transfer learning on our multimodal RGB-infrared dataset to address\nlow-light and occlusion issues, equipping it with robust vision under adverse\nconditions. Its real-world efficacy is demonstrated on an IoT platform with\nadvanced low-light and RGB cameras, seamlessly connecting to an AWS-based\nnotification endpoint for efficient real-time object detection. Benchmarks\nreveal a substantial boost of 17\\% and 14\\% in frames per second (FPS) for\nthermal and RGB detection, respectively, compared to the baseline YOLOv8n\nmodel. For community contribution, both the code and the multimodal dataset are\navailable on GitHub.\n","authors":["Shubhabrata Mukherjee","Cory Beard","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2402.07894v2.pdf","comment":"This paper has been accepted for publication at the IEEE\n  International Conference on Image Processing (ICIP) 2024"},{"id":"http://arxiv.org/abs/2406.16150v1","updated":"2024-06-23T16:09:21Z","published":"2024-06-23T16:09:21Z","title":"Intensity Confusion Matters: An Intensity-Distance Guided Loss for\n  Bronchus Segmentation","summary":"  Automatic segmentation of the bronchial tree from CT imaging is important, as\nit provides structural information for disease diagnosis. Despite the merits of\nprevious automatic bronchus segmentation methods, they have paied less\nattention to the issue we term as \\textit{Intensity Confusion}, wherein the\nintensity values of certain background voxels approach those of the foreground\nvoxels within bronchi. Conversely, the intensity values of some foreground\nvoxels are nearly identical to those of background voxels. This proximity in\nintensity values introduces significant challenges to neural network\nmethodologies. To address the issue, we introduce a novel Intensity-Distance\nGuided loss function, which assigns adaptive weights to different image voxels\nfor mining hard samples that cause the intensity confusion. The proposed loss\nestimates the voxel-level hardness of samples, on the basis of the following\nintensity and distance priors. We regard a voxel as a hard sample if it is in:\n(1) the background and has an intensity value close to the bronchus region; (2)\nthe bronchus region and is of higher intensity than most voxels inside the\nbronchus; (3) the background region and at a short distance from the bronchus.\nExtensive experiments not only show the superiority of our method compared with\nthe state-of-the-art methods, but also verify that tackling the intensity\nconfusion issue helps to significantly improve bronchus segmentation. Project\npage: https://github.com/lhaof/ICM.\n","authors":["Haifan Gong","Wenhao Huang","Huan Zhang","Yu Wang","Xiang Wan","Hong Shen","Guanbin Li","Haofeng Li"],"pdf_url":"https://arxiv.org/pdf/2406.16150v1.pdf","comment":"IEEE International Conference on Multimedia & Expo (ICME) 2024"},{"id":"http://arxiv.org/abs/2406.06374v2","updated":"2024-06-23T16:04:10Z","published":"2024-06-10T15:36:23Z","title":"Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual\n  Localization and Navigation","summary":"  This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.\n","authors":["Shenghao Li","Luchao Pang","Xianglong Hu"],"pdf_url":"https://arxiv.org/pdf/2406.06374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16143v1","updated":"2024-06-23T15:45:32Z","published":"2024-06-23T15:45:32Z","title":"Review of Zero-Shot and Few-Shot AI Algorithms in The Medical Domain","summary":"  In this paper, different techniques of few-shot, zero-shot, and regular\nobject detection have been investigated. The need for few-shot learning and\nzero-shot learning techniques is crucial and arises from the limitations and\nchallenges in traditional machine learning, deep learning, and computer vision\nmethods where they require large amounts of data, plus the poor generalization\nof those traditional methods.\n  Those techniques can give us prominent results by using only a few training\nsets reducing the required amounts of data and improving the generalization.\n  This survey will highlight the recent papers of the last three years that\nintroduce the usage of few-shot learning and zero-shot learning techniques in\naddressing the challenges mentioned earlier. In this paper we reviewed the\nZero-shot, few-shot and regular object detection methods and categorized them\nin an understandable manner. Based on the comparison made within each category.\nIt been found that the approaches are quite impressive.\n  This integrated review of diverse papers on few-shot, zero-shot, and regular\nobject detection reveals a shared focus on advancing the field through novel\nframeworks and techniques. A noteworthy observation is the scarcity of detailed\ndiscussions regarding the difficulties encountered during the development\nphase. Contributions include the introduction of innovative models, such as\nZSD-YOLO and GTNet, often showcasing improvements with various metrics such as\nmean average precision (mAP),Recall@100 (RE@100), the area under the receiver\noperating characteristic curve (AUROC) and precision. These findings underscore\na collective move towards leveraging vision-language models for versatile\napplications, with potential areas for future research including a more\nthorough exploration of limitations and domain-specific adaptations.\n","authors":["Maged Badawi","Mohammedyahia Abushanab","Sheethal Bhat","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2406.16143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16141v1","updated":"2024-06-23T15:28:07Z","published":"2024-06-23T15:28:07Z","title":"Multimodal Multilabel Classification by CLIP","summary":"  Multimodal multilabel classification (MMC) is a challenging task that aims to\ndesign a learning algorithm to handle two data sources, the image and text, and\nlearn a comprehensive semantic feature presentation across the modalities. In\nthis task, we review the extensive number of state-of-the-art approaches in MMC\nand leverage a novel technique that utilises the Contrastive Language-Image\nPre-training (CLIP) as the feature extractor and fine-tune the model by\nexploring different classification heads, fusion methods and loss functions.\nFinally, our best result achieved more than 90% F_1 score in the public Kaggle\ncompetition leaderboard. This paper provides detailed descriptions of novel\ntraining methods and quantitative analysis through the experimental results.\n","authors":["Yanming Guo"],"pdf_url":"https://arxiv.org/pdf/2406.16141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16137v1","updated":"2024-06-23T15:18:30Z","published":"2024-06-23T15:18:30Z","title":"MLPHand: Real Time Multi-View 3D Hand Mesh Reconstruction via MLP\n  Modeling","summary":"  Multi-view hand mesh reconstruction is a critical task for applications in\nvirtual reality and human-computer interaction, but it remains a formidable\nchallenge. Although existing multi-view hand reconstruction methods achieve\nremarkable accuracy, they typically come with an intensive computational burden\nthat hinders real-time inference. To this end, we propose MLPHand, a novel\nmethod designed for real-time multi-view single hand reconstruction. MLP Hand\nconsists of two primary modules: (1) a lightweight MLP-based Skeleton2Mesh\nmodel that efficiently recovers hand meshes from hand skeletons, and (2) a\nmulti-view geometry feature fusion prediction module that enhances the\nSkeleton2Mesh model with detailed geometric information from multiple views.\nExperiments on three widely used datasets demonstrate that MLPHand can reduce\ncomputational complexity by 90% while achieving comparable reconstruction\naccuracy to existing state-of-the-art baselines.\n","authors":["Jian Yang","Jiakun Li","Guoming Li","Zhen Shen","Huai-Yu Wu","Zhaoxin Fan","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16129v1","updated":"2024-06-23T15:03:35Z","published":"2024-06-23T15:03:35Z","title":"UDHF2-Net: An Uncertainty-diffusion-model-based High-Frequency\n  TransFormer Network for High-accuracy Interpretation of Remotely Sensed\n  Imagery","summary":"  Remotely sensed image high-accuracy interpretation (RSIHI), including tasks\nsuch as semantic segmentation and change detection, faces the three major\nproblems: (1) complementarity problem of spatially\nstationary-and-non-stationary frequency; (2) edge uncertainty problem caused by\ndown-sampling in the encoder step and intrinsic edge noises; and (3) false\ndetection problem caused by imagery registration error in change detection. To\nsolve the aforementioned problems, an uncertainty-diffusion-model-based\nhigh-Frequency TransFormer network (UDHF2-Net) is the proposed for RSIHI, the\nsuperiority of which is as following: (1) a\nspatially-stationary-and-non-stationary high-frequency connection paradigm\n(SHCP) is proposed to enhance the interaction of spatially stationary and\nnon-stationary frequency features to yield high-fidelity edge extraction\nresult. Inspired by HRFormer, SHCP remains the high-frequency stream through\nthe whole encoder-decoder process with parallel high-to-low frequency streams\nand reduces the edge loss by a downsampling operation; (2) a\nmask-and-geo-knowledge-based uncertainty diffusion module (MUDM) is proposed to\nimprove the robustness and edge noise resistance. MUDM could further optimize\nthe uncertain region to improve edge extraction result by gradually removing\nthe multiple geo-knowledge-based noises; (3) a semi-pseudo-Siamese UDHF2-Net\nfor change detection task is proposed to reduce the pseudo change by\nregistration error. It adopts semi-pseudo-Siamese architecture to extract above\ncomplemental frequency features for adaptively reducing registration\ndifferencing, and MUDM to recover the uncertain region by gradually reducing\nthe registration error besides above edge noises. Comprehensive experiments\nwere performed to demonstrate the superiority of UDHF2-Net. Especially ablation\nexperiments indicate the effectiveness of UDHF2-Net.\n","authors":["Pengfei Zhang","Chang Li","Yongjun Zhang","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2406.16129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06589v2","updated":"2024-06-23T14:54:11Z","published":"2024-04-09T19:33:05Z","title":"Leveraging Latents for Efficient Thermography Classification and\n  Segmentation","summary":"  Breast cancer is a prominent health concern worldwide, currently being the\nsecondmost common and second-deadliest type of cancer in women. While current\nbreast cancer diagnosis mainly relies on mammography imaging, in recent years\nthe use of thermography for breast cancer imaging has been garnering growing\npopularity. Thermographic imaging relies on infrared cameras to capture\nbody-emitted heat distributions. While these heat signatures have proven useful\nfor computer-vision systems for accurate breast cancer segmentation and\nclassification, prior work often relies on handcrafted feature engineering or\ncomplex architectures, potentially limiting the comparability and applicability\nof these methods. In this work, we present a novel algorithm for both breast\ncancer classification and segmentation. Rather than focusing efforts on manual\nfeature and architecture engineering, our algorithm focuses on leveraging an\ninformative, learned feature space, thus making our solution simpler to use and\nextend to other frameworks and downstream tasks, as well as more applicable to\ndata-scarce settings. Our classification produces SOTA results, while we are\nthe first work to produce segmentation regions studied in this paper.\n","authors":["Tamir Shor","Chaim Baskin","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2404.06589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11735v4","updated":"2024-06-23T14:08:35Z","published":"2024-03-18T12:43:38Z","title":"LSKNet: A Foundation Lightweight Backbone for Remote Sensing","summary":"  Remote sensing images pose distinct challenges for downstream tasks due to\ntheir inherent complexity. While a considerable amount of research has been\ndedicated to remote sensing classification, object detection and semantic\nsegmentation, most of these studies have overlooked the valuable prior\nknowledge embedded within remote sensing scenarios. Such prior knowledge can be\nuseful because remote sensing objects may be mistakenly recognized without\nreferencing a sufficiently long-range context, which can vary for different\nobjects. This paper considers these priors and proposes a lightweight Large\nSelective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its\nlarge spatial receptive field to better model the ranging context of various\nobjects in remote sensing scenarios. To our knowledge, large and selective\nkernel mechanisms have not been previously explored in remote sensing images.\nWithout bells and whistles, our lightweight LSKNet sets new state-of-the-art\nscores on standard remote sensing classification, object detection and semantic\nsegmentation benchmarks. Our comprehensive analysis further validated the\nsignificance of the identified priors and the effectiveness of LSKNet. The code\nis available at https://github.com/zcablii/LSKNet.\n","authors":["Yuxuan Li","Xiang Li","Yimian Dai","Qibin Hou","Li Liu","Yongxiang Liu","Ming-Ming Cheng","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.11735v4.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.09030"},{"id":"http://arxiv.org/abs/2406.16111v1","updated":"2024-06-23T13:59:31Z","published":"2024-06-23T13:59:31Z","title":"Multi-Scale Temporal Difference Transformer for Video-Text Retrieval","summary":"  Currently, in the field of video-text retrieval, there are many\ntransformer-based methods. Most of them usually stack frame features and\nregrade frames as tokens, then use transformers for video temporal modeling.\nHowever, they commonly neglect the inferior ability of the transformer modeling\nlocal temporal information. To tackle this problem, we propose a transformer\nvariant named Multi-Scale Temporal Difference Transformer (MSTDT). MSTDT mainly\naddresses the defects of the traditional transformer which has limited ability\nto capture local temporal information. Besides, in order to better model the\ndetailed dynamic information, we make use of the difference feature between\nframes, which practically reflects the dynamic movement of a video. We extract\nthe inter-frame difference feature and integrate the difference and frame\nfeature by the multi-scale temporal transformer. In general, our proposed MSTDT\nconsists of a short-term multi-scale temporal difference transformer and a\nlong-term temporal transformer. The former focuses on modeling local temporal\ninformation, the latter aims at modeling global temporal information. At last,\nwe propose a new loss to narrow the distance of similar samples. Extensive\nexperiments show that backbone, such as CLIP, with MSTDT has attained a new\nstate-of-the-art result.\n","authors":["Ni Wang","Dongliang Liao","Xing Xu"],"pdf_url":"https://arxiv.org/pdf/2406.16111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16109v1","updated":"2024-06-23T13:53:35Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v1.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2406.11548v2","updated":"2024-06-23T12:58:01Z","published":"2024-06-17T13:44:53Z","title":"AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation","summary":"  The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects.Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly.However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions.\nSpecifically, AIC MLLM is initially fine-tuned to acquire both pose prediction\nand feedback prompt comprehension abilities.We carefully design two types of\nprompt instructions through interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2)textual descriptions\nto indicate potential directions for rotation correction.During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration.Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts.Real-world demonstration can be\nfound at https://sites.google.com/view/aic-mllm\n","authors":["Chuyan Xiong","Chengyu Shen","Xiaoqi Li","Kaichen Zhou","Jiaming Liu","Ruiping Wang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2406.11548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16093v1","updated":"2024-06-23T12:14:37Z","published":"2024-06-23T12:14:37Z","title":"Towards Natural Language-Driven Assembly Using Foundation Models","summary":"  Large Language Models (LLMs) and strong vision models have enabled rapid\nresearch and development in the field of Vision-Language-Action models that\nenable robotic control. The main objective of these methods is to develop a\ngeneralist policy that can control robots with various embodiments. However, in\nindustrial robotic applications such as automated assembly and disassembly,\nsome tasks, such as insertion, demand greater accuracy and involve intricate\nfactors like contact engagement, friction handling, and refined motor skills.\nImplementing these skills using a generalist policy is challenging because\nthese policies might integrate further sensory data, including force or torque\nmeasurements, for enhanced precision. In our method, we present a global\ncontrol policy based on LLMs that can transfer the control policy to a finite\nset of skills that are specifically trained to perform high-precision tasks\nthrough dynamic context switching. The integration of LLMs into this framework\nunderscores their significance in not only interpreting and processing language\ninputs but also in enriching the control mechanisms for diverse and intricate\nrobotic operations.\n","authors":["Omkar Joglekar","Tal Lancewicki","Shir Kozlovsky","Vladimir Tchuiev","Zohar Feldman","Dotan Di Castro"],"pdf_url":"https://arxiv.org/pdf/2406.16093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v1","updated":"2024-06-23T12:02:17Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16085v1","updated":"2024-06-23T11:57:08Z","published":"2024-06-23T11:57:08Z","title":"A Simple Framework for Open-Vocabulary Zero-Shot Segmentation","summary":"  Zero-shot classification capabilities naturally arise in models trained\nwithin a vision-language contrastive framework. Despite their classification\nprowess, these models struggle in dense tasks like zero-shot open-vocabulary\nsegmentation. This deficiency is often attributed to the absence of\nlocalization cues in captions and the intertwined nature of the learning\nprocess, which encompasses both image representation learning and\ncross-modality alignment. To tackle these issues, we propose SimZSS, a Simple\nframework for open-vocabulary Zero-Shot Segmentation. The method is founded on\ntwo key principles: i) leveraging frozen vision-only models that exhibit\nspatial awareness while exclusively aligning the text encoder and ii)\nexploiting the discrete nature of text and linguistic knowledge to pinpoint\nlocal concepts within captions. By capitalizing on the quality of the visual\nrepresentations, our method requires only image-caption pairs datasets and\nadapts to both small curated and large-scale noisy datasets. When trained on\nCOCO Captions across 8 GPUs, SimZSS achieves state-of-the-art results on 7 out\nof 8 benchmark datasets in less than 15 minutes.\n","authors":["Thomas Stegmüller","Tim Lebailly","Nikola Dukic","Behzad Bozorgtabar","Jean-Philippe Thiran","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2406.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16083v1","updated":"2024-06-23T11:28:08Z","published":"2024-06-23T11:28:08Z","title":"Mamba-based Light Field Super-Resolution with Efficient Subspace\n  Scanning","summary":"  Transformer-based methods have demonstrated impressive performance in 4D\nlight field (LF) super-resolution by effectively modeling long-range\nspatial-angular correlations, but their quadratic complexity hinders the\nefficient processing of high resolution 4D inputs, resulting in slow inference\nspeed and high memory cost. As a compromise, most prior work adopts a\npatch-based strategy, which fails to leverage the full information from the\nentire input LFs. The recently proposed selective state-space model, Mamba, has\ngained popularity for its efficient long-range sequence modeling. In this\npaper, we propose a Mamba-based Light Field Super-Resolution method, named\nMLFSR, by designing an efficient subspace scanning strategy. Specifically, we\ntokenize 4D LFs into subspace sequences and conduct bi-directional scanning on\neach subspace. Based on our scanning strategy, we then design the Mamba-based\nGlobal Interaction (MGI) module to capture global information and the local\nSpatial- Angular Modulator (SAM) to complement local details. Additionally, we\nintroduce a Transformer-to-Mamba (T2M) loss to further enhance overall\nperformance. Extensive experiments on public benchmarks demonstrate that MLFSR\nsurpasses CNN-based models and rivals Transformer-based methods in performance\nwhile maintaining higher efficiency. With quicker inference speed and reduced\nmemory demand, MLFSR facilitates full-image processing of high-resolution 4D\nLFs with enhanced performance.\n","authors":["Ruisheng Gao","Zeyu Xiao","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.16083v1.pdf","comment":"17 pages,7 figures"},{"id":"http://arxiv.org/abs/2406.16077v1","updated":"2024-06-23T11:09:21Z","published":"2024-06-23T11:09:21Z","title":"Detecting Abnormal Operations in Concentrated Solar Power Plants from\n  Irregular Sequences of Thermal Images","summary":"  Concentrated Solar Power (CSP) plants store energy by heating a storage\nmedium with an array of mirrors that focus sunlight onto solar receivers atop a\ncentral tower. Operating at high temperatures these receivers face risks such\nas freezing, deformation, and corrosion, leading to operational failures,\ndowntime, or costly equipment damage. We study the problem of anomaly detection\n(AD) in sequences of thermal images collected over a year from an operational\nCSP plant. These images are captured at irregular intervals ranging from one to\nfive minutes throughout the day by infrared cameras mounted on solar receivers.\nOur goal is to develop a method to extract useful representations from\nhigh-dimensional thermal images for AD. It should be able to handle temporal\nfeatures of the data, which include irregularity, temporal dependency between\nimages and non-stationarity due to a strong daily seasonal pattern. The\nco-occurrence of low-temperature anomalies that resemble normal images from the\nstart and the end of the operational cycle with high-temperature anomalies\nposes an additional challenge. We first evaluate state-of-the-art deep\nimage-based AD methods, which have been shown to be effective in deriving\nmeaningful image representations for the detection of anomalies. Then, we\nintroduce a forecasting-based AD method that predicts future thermal images\nfrom past sequences and timestamps via a deep sequence model. This method\neffectively captures specific temporal data features and distinguishes between\ndifficult-to-detect temperature-based anomalies. Our experiments demonstrate\nthe effectiveness of our approach compared to multiple SOTA baselines across\nmultiple evaluation metrics. We have also successfully deployed our solution on\nfive months of unseen data, providing critical insights for the maintenance of\nthe CSP plant. Our code is available at: https://tinyurl.com/ForecastAD\n","authors":["Sukanya Patra","Nicolas Sournac","Souhaib Ben Taieb"],"pdf_url":"https://arxiv.org/pdf/2406.16077v1.pdf","comment":"Accepted in KDD 2024"},{"id":"http://arxiv.org/abs/2406.16074v1","updated":"2024-06-23T10:50:22Z","published":"2024-06-23T10:50:22Z","title":"CAVM: Conditional Autoregressive Vision Model for Contrast-Enhanced\n  Brain Tumor MRI Synthesis","summary":"  Contrast-enhanced magnetic resonance imaging (MRI) is pivotal in the pipeline\nof brain tumor segmentation and analysis. Gadolinium-based contrast agents, as\nthe most commonly used contrast agents, are expensive and may have potential\nside effects, and it is desired to obtain contrast-enhanced brain tumor MRI\nscans without the actual use of contrast agents. Deep learning methods have\nbeen applied to synthesize virtual contrast-enhanced MRI scans from\nnon-contrast images. However, as this synthesis problem is inherently\nill-posed, these methods fall short in producing high-quality results. In this\nwork, we propose Conditional Autoregressive Vision Model (CAVM) for improving\nthe synthesis of contrast-enhanced brain tumor MRI. As the enhancement of image\nintensity grows with a higher dose of contrast agents, we assume that it is\nless challenging to synthesize a virtual image with a lower dose, where the\ndifference between the contrast-enhanced and non-contrast images is smaller.\nThus, CAVM gradually increases the contrast agent dosage and produces\nhigher-dose images based on previous lower-dose ones until the final desired\ndose is achieved. Inspired by the resemblance between the gradual dose increase\nand the Chain-of-Thought approach in natural language processing, CAVM uses an\nautoregressive strategy with a decomposition tokenizer and a decoder.\nSpecifically, the tokenizer is applied to obtain a more compact image\nrepresentation for computational efficiency, and it decomposes the image into\ndose-variant and dose-invariant tokens. Then, a masked self-attention mechanism\nis developed for autoregression that gradually increases the dose of the\nvirtual image based on the dose-variant tokens. Finally, the updated\ndose-variant tokens corresponding to the desired dose are decoded together with\ndose-invariant tokens to produce the final contrast-enhanced MRI.\n","authors":["Lujun Gui","Chuyang Ye","Tianyi Yan"],"pdf_url":"https://arxiv.org/pdf/2406.16074v1.pdf","comment":"The work has been accepted by MICCAI 2024"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2403.03312v2","updated":"2024-06-23T20:46:18Z","published":"2024-03-05T20:32:10Z","title":"Beyond the Dashboard: Investigating Distracted Driver Communication\n  Preferences for ADAS","summary":"  Distracted driving is a major cause of road fatalities. With improvements in\ndriver (in)attention detection, these distracted situations can be caught early\nto alert drivers and improve road safety and comfort. However, drivers may have\ndiffering preferences for the modes of such communication based on the driving\nscenario and their current distraction state. To this end, we present an\n(N=147) where videos of simulated driving scenarios were utilized to learn\ndrivers preferences for modes of communication and their evolution with the\ndrivers changing attention. The survey queried participants preferred modes of\ncommunication for scenarios such as collisions or stagnation at a green light.\nthat inform the future of communication between drivers and their vehicles. We\nshowcase the different driver preferences based on the nature of the driving\nscenario and also show that they evolve as the drivers distraction state\nchanges\n","authors":["Aamir Hasan","D. Livingston McPherson","Melissa Miles","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.03312v2.pdf","comment":"10 pages, 6 figures. All materials associated with the study can be\n  found at https://sites.google.com/illinois.edu/driver-preference-for-modes"},{"id":"http://arxiv.org/abs/2401.07115v2","updated":"2024-06-23T19:53:33Z","published":"2024-01-13T16:41:40Z","title":"Open Models, Closed Minds? On Agents Capabilities in Mimicking Human\n  Personalities through Open Large Language Models","summary":"  The emergence of unveiling human-like behaviors in Large Language Models\n(LLMs) has led to a closer connection between NLP and human psychology.\nScholars have been studying the inherent personalities exhibited by LLMs and\nattempting to incorporate human traits and behaviors into them. However, these\nefforts have primarily focused on commercially-licensed LLMs, neglecting the\nwidespread use and notable advancements seen in Open LLMs. This work aims to\naddress this gap by employing a set of 12 LLM Agents based on the most\nrepresentative Open models and subject them to a series of assessments\nconcerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five\nInventory (BFI) test. Our approach involves evaluating the intrinsic\npersonality traits of Open LLM agents and determining the extent to which these\nagents can mimic human personalities when conditioned by specific personalities\nand roles. Our findings unveil that $(i)$ each Open LLM agent showcases\ndistinct human personalities; $(ii)$ personality-conditioned prompting produces\nvarying effects on the agents, with only few successfully mirroring the imposed\npersonality, while most of them being ``closed-minded'' (i.e., they retain\ntheir intrinsic traits); and $(iii)$ combining role and personality\nconditioning can enhance the agents' ability to mimic human personalities. Our\nwork represents a step up in understanding the dense relationship between NLP\nand human psychology through the lens of Open LLMs.\n","authors":["Lucio La Cava","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2401.07115v2.pdf","comment":"Enhanced methodology and evaluation based on BFI in addition to MBTI,\n  with expanded set of LLM agents. Author list changed w.r.t. the previous\n  version (v1), see Acknowledgements"},{"id":"http://arxiv.org/abs/2406.16177v1","updated":"2024-06-23T18:09:49Z","published":"2024-06-23T18:09:49Z","title":"Flowy: Supporting UX Design Decisions Through AI-Driven Pattern\n  Annotation in Multi-Screen User Flows","summary":"  Many recent AI-powered UX design tools focus on generating individual static\nUI screens from natural language. However, they overlook the crucial aspect of\ninteractions and user experiences across multiple screens. Through formative\nstudies with UX professionals, we identified limitations of these tools in\nsupporting realistic UX design workflows. In response, we designed and\ndeveloped Flowy, an app that augments designers' information foraging process\nin ideation by supplementing specific user flow examples with distilled design\npattern knowledge. Flowy utilizes large multimodal AI models and a high-quality\nuser flow dataset to help designers identify and understand relevant abstract\ndesign patterns in the design space for multi-screen user flows. Our user study\nwith professional UX designers demonstrates how Flowy supports realistic UX\ntasks. Our design considerations in Flowy, such as representations with\nappropriate levels of abstraction and assisted navigation through the solution\nspace, are generalizable to other creative tasks and embody a human-centered,\nintelligence augmentation approach to using AI in UX design.\n","authors":["Yuwen Lu","Ziang Tong","Qinyi Zhao","Yewon Oh","Bryan Wang","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2406.16177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16173v1","updated":"2024-06-23T17:53:10Z","published":"2024-06-23T17:53:10Z","title":"Crepe: A Mobile Screen Data Collector Using Graph Query","summary":"  Collecting mobile datasets remains challenging for academic researchers due\nto limited data access and technical barriers. Commercial organizations often\npossess exclusive access to mobile data, leading to a \"data monopoly\" that\nrestricts the independence of academic research. Existing open-source mobile\ndata collection frameworks primarily focus on mobile sensing data rather than\nscreen content, which is crucial for various research studies. We present\nCrepe, a no-code Android app that enables researchers to collect information\ndisplayed on screen through simple demonstrations of target data. Crepe\nutilizes a novel Graph Query technique which augments the structures of mobile\nUI screens to support flexible identification, location, and collection of\nspecific data pieces. The tool emphasizes participants' privacy and agency by\nproviding full transparency over collected data and allowing easy opt-out. We\ndesigned and built Crepe for research purposes only and in scenarios where\nresearchers obtain explicit consent from participants. Code for Crepe will be\nopen-sourced to support future academic research data collection.\n","authors":["Yuwen Lu","Meng Chen","Qi Zhao","Victor Cox","Yang Yang","Meng Jiang","Jay Brockman","Tamara Kay","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2406.16173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16066v2","updated":"2024-06-23T14:39:13Z","published":"2024-04-20T16:36:28Z","title":"Social Media Use is Predictable from App Sequences: Using LSTM and\n  Transformer Neural Networks to Model Habitual Behavior","summary":"  The present paper introduces a novel approach to studying social media habits\nthrough predictive modeling of sequential smartphone user behaviors. While much\nof the literature on media and technology habits has relied on self-report\nquestionnaires and simple behavioral frequency measures, we examine an\nimportant yet understudied aspect of media and technology habits: their\nembeddedness in repetitive behavioral sequences. Leveraging Long Short-Term\nMemory (LSTM) and transformer neural networks, we show that (i) social media\nuse is predictable at the within and between-person level and that (ii) there\nare robust individual differences in the predictability of social media use. We\nexamine the performance of several modeling approaches, including (i) global\nmodels trained on the pooled data from all participants, (ii) idiographic\nperson-specific models, and (iii) global models fine-tuned on person-specific\ndata. Neither person-specific modeling nor fine-tuning on person-specific data\nsubstantially outperformed the global models, indicating that the global models\nwere able to represent a variety of idiosyncratic behavioral patterns.\nAdditionally, our analyses reveal that the person-level predictability of\nsocial media use is not substantially related to the frequency of smartphone\nuse in general or the frequency of social media use, indicating that our\napproach captures an aspect of habits that is distinct from behavioral\nfrequency. Implications for habit modeling and theoretical development are\ndiscussed.\n","authors":["Heinrich Peters","Joseph B. Bayer","Sandra C. Matz","Yikun Chi","Sumer S. Vaid","Gabriella M. Harari"],"pdf_url":"https://arxiv.org/pdf/2404.16066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11461v2","updated":"2024-06-23T10:38:22Z","published":"2024-04-17T15:09:31Z","title":"Using Game Engines and Machine Learning to Create Synthetic Satellite\n  Imagery for a Tabletop Verification Exercise","summary":"  Satellite imagery is regarded as a great opportunity for citizen-based\nmonitoring of activities of interest. Relevant imagery may however not be\navailable at sufficiently high resolution, quality, or cadence -- let alone be\nuniformly accessible to open-source analysts. This limits an assessment of the\ntrue long-term potential of citizen-based monitoring of nuclear activities\nusing publicly available satellite imagery. In this article, we demonstrate how\nmodern game engines combined with advanced machine-learning techniques can be\nused to generate synthetic imagery of sites of interest with the ability to\nchoose relevant parameters upon request; these include time of day, cloud\ncover, season, or level of activity onsite. At the same time, resolution and\noff-nadir angle can be adjusted to simulate different characteristics of the\nsatellite. While there are several possible use-cases for synthetic imagery,\nhere we focus on its usefulness to support tabletop exercises in which simple\nmonitoring scenarios can be examined to better understand verification\ncapabilities enabled by new satellite constellations and very short revisit\ntimes.\n","authors":["Johannes Hoster","Sara Al-Sayed","Felix Biessmann","Alexander Glaser","Kristian Hildebrand","Igor Moric","Tuong Vy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.11461v2.pdf","comment":"Annual Meeting of the Institute of Nuclear Materials Management\n  (INMM), Vienna"},{"id":"http://arxiv.org/abs/2406.15991v1","updated":"2024-06-23T02:58:30Z","published":"2024-06-23T02:58:30Z","title":"TikTok Engagement Traces Over Time and Health Risky Behaviors: Combining\n  Data Linkage and Computational Methods","summary":"  Digital technologies and social algorithms are revolutionizing the media\nlandscape, altering how we select and consume health information. Extending the\nselectivity paradigm with research on social media engagement, the convergence\nperspective, and algorithmic impact, this study investigates how individuals'\nliked TikTok videos on various health-risk topics are associated with their\nvaping and drinking behaviors. Methodologically, we relied on data linkage to\nobjectively measure selective engagement on social media, which involves\ncombining survey self-reports with digital traces from TikTok interactions for\nthe consented respondents (n = 166). A computational analysis of 13,724\nhealth-related videos liked by these respondents from 2020 to 2023 was\nconducted. Our findings indicate that users who initially liked\ndrinking-related content on TikTok are inclined to favor more of such videos\nover time, with their likes on smoking, drinking, and fruit and vegetable\nvideos influencing their self-reported vaping and drinking behaviors. Our study\nhighlights the methodological value of combining digital traces, computational\nanalysis, and self-reported data for a more objective examination of social\nmedia consumption and engagement, as well as a more ecologically valid\nunderstanding of social media's behavioral impact.\n","authors":["Xinyan Zhao","Chau-Wai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.15991v1.pdf","comment":"12 pages. Under review"},{"id":"http://arxiv.org/abs/2406.15963v1","updated":"2024-06-23T00:04:07Z","published":"2024-06-23T00:04:07Z","title":"Effectiveness of ChatGPT in explaining complex medical reports to\n  patients","summary":"  Electronic health records contain detailed information about the medical\ncondition of patients, but they are difficult for patients to understand even\nif they have access to them. We explore whether ChatGPT (GPT 4) can help\nexplain multidisciplinary team (MDT) reports to colorectal and prostate cancer\npatients. These reports are written in dense medical language and assume\nclinical knowledge, so they are a good test of the ability of ChatGPT to\nexplain complex medical reports to patients. We asked clinicians and lay people\n(not patients) to review explanations and responses of ChatGPT. We also ran\nthree focus groups (including cancer patients, caregivers, computer scientists,\nand clinicians) to discuss output of ChatGPT. Our studies highlighted issues\nwith inaccurate information, inappropriate language, limited personalization,\nAI distrust, and challenges integrating large language models (LLMs) into\nclinical workflow. These issues will need to be resolved before LLMs can be\nused to explain complex personal medical information to patients.\n","authors":["Mengxuan Sun","Ehud Reiter","Anne E Kiltie","George Ramsay","Lisa Duncan","Peter Murchie","Rosalind Adam"],"pdf_url":"https://arxiv.org/pdf/2406.15963v1.pdf","comment":"under review"}]},"2024-06-25T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.17777v1","updated":"2024-06-25T17:59:41Z","published":"2024-06-25T17:59:41Z","title":"Text-Animator: Controllable Visual Text Video Generation","summary":"  Video generation is a challenging yet pivotal task in various industries,\nsuch as gaming, e-commerce, and advertising. One significant unresolved aspect\nwithin T2V is the effective visualization of text within generated videos.\nDespite the progress achieved in Text-to-Video~(T2V) generation, current\nmethods still cannot effectively visualize texts in videos directly, as they\nmainly focus on summarizing semantic scene information, understanding, and\ndepicting actions. While recent advances in image-level visual text generation\nshow promise, transitioning these techniques into the video domain faces\nproblems, notably in preserving textual fidelity and motion coherence. In this\npaper, we propose an innovative approach termed Text-Animator for visual text\nvideo generation. Text-Animator contains a text embedding injection module to\nprecisely depict the structures of visual text in generated videos. Besides, we\ndevelop a camera control module and a text refinement module to improve the\nstability of generated visual text by controlling the camera movement as well\nas the motion of visualized text. Quantitative and qualitative experimental\nresults demonstrate the superiority of our approach to the accuracy of\ngenerated visual text over state-of-the-art video generation methods. The\nproject page can be found at https://laulampaul.github.io/text-animator.html.\n","authors":["Lin Liu","Quande Liu","Shengju Qian","Yuan Zhou","Wengang Zhou","Houqiang Li","Lingxi Xie","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2406.17777v1.pdf","comment":"Project Page: https://laulampaul.github.io/text-animator.html"},{"id":"http://arxiv.org/abs/2406.17774v1","updated":"2024-06-25T17:59:06Z","published":"2024-06-25T17:59:06Z","title":"Fast and Uncertainty-Aware SVBRDF Recovery from Multi-View Capture using\n  Frequency Domain Analysis","summary":"  Relightable object acquisition is a key challenge in simplifying digital\nasset creation. Complete reconstruction of an object typically requires\ncapturing hundreds to thousands of photographs under controlled illumination,\nwith specialized equipment. The recent progress in differentiable rendering\nimproved the quality and accessibility of inverse rendering optimization.\nNevertheless, under uncontrolled illumination and unstructured viewpoints,\nthere is no guarantee that the observations contain enough information to\nreconstruct the appearance properties of the captured object.\n  We thus propose to consider the acquisition process from a signal-processing\nperspective. Given an object's geometry and a lighting environment, we estimate\nthe properties of the materials on the object's surface in seconds. We do so by\nleveraging frequency domain analysis, considering the recovery of material\nproperties as a deconvolution, enabling fast error estimation. We then quantify\nthe uncertainty of the estimation, based on the available data, highlighting\nthe areas for which priors or additional samples would be required for improved\nacquisition quality. We compare our approach to previous work and\nquantitatively evaluate our results, showing similar quality as previous work\nin a fraction of the time, and providing key information about the certainty of\nthe results.\n","authors":["Ruben Wiersma","Julien Philip","Miloš Hašan","Krishna Mullia","Fujun Luan","Elmar Eisemann","Valentin Deschaintre"],"pdf_url":"https://arxiv.org/pdf/2406.17774v1.pdf","comment":"Project page: https://brdf-uncertainty.github.io"},{"id":"http://arxiv.org/abs/2403.16494v2","updated":"2024-06-25T17:56:21Z","published":"2024-03-25T07:22:22Z","title":"CT-Bound: Robust Boundary Detection From Noisy Images Via Hybrid\n  Convolution and Transformer Neural Networks","summary":"  We present CT-Bound, a robust and fast boundary detection method for very\nnoisy images using a hybrid Convolution and Transformer neural network. The\nproposed architecture decomposes boundary estimation into two tasks: local\ndetection and global regularization. During the local detection, the model uses\na convolutional architecture to predict the boundary structure of each image\npatch in the form of a pre-defined local boundary representation, the\nfield-of-junctions (FoJ). Then, it uses a feed-forward transformer architecture\nto globally refine the boundary structures of each patch to generate an edge\nmap and a smoothed color map simultaneously. Our quantitative analysis shows\nthat CT-Bound outperforms the previous best algorithms in edge detection on\nvery noisy images. It also increases the edge detection accuracy of FoJ-based\nmethods while having a 3-time speed improvement. Finally, we demonstrate that\nCT-Bound can produce boundary and color maps on real captured images without\nextra fine-tuning and real-time boundary map and color map videos at ten frames\nper second.\n","authors":["Wei Xu","Junjie Luo","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.16494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12289v5","updated":"2024-06-25T17:55:35Z","published":"2024-02-19T17:04:04Z","title":"DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models","summary":"  A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of reasoning\nmodules for scene description, scene analysis, and hierarchical planning.\nFurthermore, recognizing the limitations of VLMs in spatial reasoning and heavy\ncomputational requirements, we propose DriveVLM-Dual, a hybrid system that\nsynergizes the strengths of DriveVLM with the traditional autonomous driving\npipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset\ndemonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and\nunpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a\nproduction vehicle, verifying it is effective in real-world autonomous driving\nenvironments.\n","authors":["Xiaoyu Tian","Junru Gu","Bailin Li","Yicheng Liu","Yang Wang","Zhiyong Zhao","Kun Zhan","Peng Jia","Xianpeng Lang","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12289v5.pdf","comment":"Project Page: https://tsinghua-mars-lab.github.io/DriveVLM/"},{"id":"http://arxiv.org/abs/2406.17770v1","updated":"2024-06-25T17:55:11Z","published":"2024-06-25T17:55:11Z","title":"MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning","summary":"  Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.\n","authors":["Xiangyu Zhao","Xiangtai Li","Haodong Duan","Haian Huang","Yining Li","Kai Chen","Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17763v1","updated":"2024-06-25T17:48:24Z","published":"2024-06-25T17:48:24Z","title":"DiffusionPDE: Generative PDE-Solving Under Partial Observation","summary":"  We introduce a general framework for solving partial differential equations\n(PDEs) using generative diffusion models. In particular, we focus on the\nscenarios where we do not have the full knowledge of the scene necessary to\napply classical solvers. Most existing forward or inverse PDE approaches\nperform poorly when the observations on the data or the underlying coefficients\nare incomplete, which is a common assumption for real-world measurements. In\nthis work, we propose DiffusionPDE that can simultaneously fill in the missing\ninformation and solve a PDE by modeling the joint distribution of the solution\nand coefficient spaces. We show that the learned generative priors lead to a\nversatile framework for accurately solving a wide range of PDEs under partial\nobservation, significantly outperforming the state-of-the-art methods for both\nforward and inverse directions.\n","authors":["Jiahe Huang","Guandao Yang","Zichen Wang","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2406.17763v1.pdf","comment":"Project page: https://jhhuangchloe.github.io/Diffusion-PDE/"},{"id":"http://arxiv.org/abs/2406.17758v1","updated":"2024-06-25T17:42:25Z","published":"2024-06-25T17:42:25Z","title":"MotionBooth: Motion-Aware Customized Text-to-Video Generation","summary":"  In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth\n","authors":["Jianzong Wu","Xiangtai Li","Yanhong Zeng","Jiangning Zhang","Qianyu Zhou","Yining Li","Yunhai Tong","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17758v1.pdf","comment":"Project page at https://jianzongwu.github.io/projects/motionbooth"},{"id":"http://arxiv.org/abs/2405.13285v2","updated":"2024-06-25T17:40:35Z","published":"2024-05-22T01:54:51Z","title":"Enhancing Active Learning for Sentinel 2 Imagery through Contrastive\n  Learning and Uncertainty Estimation","summary":"  In this paper, we introduce a novel method designed to enhance label\nefficiency in satellite imagery analysis by integrating semi-supervised\nlearning (SSL) with active learning strategies. Our approach utilizes\ncontrastive learning together with uncertainty estimations via Monte Carlo\nDropout (MC Dropout), with a particular focus on Sentinel-2 imagery analyzed\nusing the Eurosat dataset. We explore the effectiveness of our method in\nscenarios featuring both balanced and unbalanced class distributions. Our\nresults show that the proposed method performs better than several other\npopular methods in this field, enabling significant savings in labeling effort\nwhile maintaining high classification accuracy. These findings highlight the\npotential of our approach to facilitate scalable and cost-effective satellite\nimage analysis, particularly advantageous for extensive environmental\nmonitoring and land use classification tasks.\n","authors":["David Pogorzelski","Peter Arlinghaus","Wenyan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.13285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17749v1","updated":"2024-06-25T17:34:52Z","published":"2024-06-25T17:34:52Z","title":"Benchmarking Deep Learning Models on NVIDIA Jetson Nano for Real-Time\n  Systems: An Empirical Investigation","summary":"  The proliferation of complex deep learning (DL) models has revolutionized\nvarious applications, including computer vision-based solutions, prompting\ntheir integration into real-time systems. However, the resource-intensive\nnature of these models poses challenges for deployment on low-computational\npower and low-memory devices, like embedded and edge devices. This work\nempirically investigates the optimization of such complex DL models to analyze\ntheir functionality on an embedded device, particularly on the NVIDIA Jetson\nNano. It evaluates the effectiveness of the optimized models in terms of their\ninference speed for image classification and video action detection. The\nexperimental results reveal that, on average, optimized models exhibit a 16.11%\nspeed improvement over their non-optimized counterparts. This not only\nemphasizes the critical need to consider hardware constraints and environmental\nsustainability in model development and deployment but also underscores the\npivotal role of model optimization in enabling the widespread deployment of\nAI-assisted technologies on resource-constrained computational systems. It also\nserves as proof that prioritizing hardware-specific model optimization leads to\nefficient and scalable solutions that substantially decrease energy consumption\nand carbon footprint.\n","authors":["Tushar Prasanna Swaminathan","Christopher Silver","Thangarajah Akilan"],"pdf_url":"https://arxiv.org/pdf/2406.17749v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.09384v2","updated":"2024-06-25T17:33:31Z","published":"2024-01-17T17:55:06Z","title":"Diverse Part Synthesis for 3D Shape Creation","summary":"  Methods that use neural networks for synthesizing 3D shapes in the form of a\npart-based representation have been introduced over the last few years. These\nmethods represent shapes as a graph or hierarchy of parts and enable a variety\nof applications such as shape sampling and reconstruction. However, current\nmethods do not allow easily regenerating individual shape parts according to\nuser preferences. In this paper, we investigate techniques that allow the user\nto generate multiple, diverse suggestions for individual parts. Specifically,\nwe experiment with multimodal deep generative models that allow sampling\ndiverse suggestions for shape parts and focus on models which have not been\nconsidered in previous work on shape synthesis. To provide a comparative study\nof these techniques, we introduce a method for synthesizing 3D shapes in a\npart-based representation and evaluate all the part suggestion techniques\nwithin this synthesis method. In our method, which is inspired by previous\nwork, shapes are represented as a set of parts in the form of implicit\nfunctions which are then positioned in space to form the final shape. Synthesis\nin this representation is enabled by a neural network architecture based on an\nimplicit decoder and a spatial transformer. We compare the various multimodal\ngenerative models by evaluating their performance in generating part\nsuggestions. Our contribution is to show with qualitative and quantitative\nevaluations which of the new techniques for multimodal part generation perform\nthe best and that a synthesis method based on the top-performing techniques\nallows the user to more finely control the parts that are generated in the 3D\nshapes while maintaining high shape fidelity when reconstructing shapes.\n","authors":["Yanran Guan","Oliver van Kaick"],"pdf_url":"https://arxiv.org/pdf/2401.09384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17741v1","updated":"2024-06-25T17:28:03Z","published":"2024-06-25T17:28:03Z","title":"Point-SAM: Promptable 3D Segmentation Model for Point Clouds","summary":"  The development of 2D foundation models for image segmentation has been\nsignificantly advanced by the Segment Anything Model (SAM). However, achieving\nsimilar success in 3D models remains a challenge due to issues such as\nnon-unified data formats, lightweight models, and the scarcity of labeled data\nwith diverse masks. To this end, we propose a 3D promptable segmentation model\n(Point-SAM) focusing on point clouds. Our approach utilizes a transformer-based\nmethod, extending SAM to the 3D domain. We leverage part-level and object-level\nannotations and introduce a data engine to generate pseudo labels from SAM,\nthereby distilling 2D knowledge into our 3D model. Our model outperforms\nstate-of-the-art models on several indoor and outdoor benchmarks and\ndemonstrates a variety of applications, such as 3D annotation. Codes and demo\ncan be found at https://github.com/zyc00/Point-SAM.\n","authors":["Yuchen Zhou","Jiayuan Gu","Tung Yen Chiang","Fanbo Xiang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2406.17741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17740v1","updated":"2024-06-25T17:26:05Z","published":"2024-06-25T17:26:05Z","title":"Structured Unrestricted-Rank Matrices for Parameter Efficient\n  Fine-tuning","summary":"  Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.\n","authors":["Arijit Sehanobish","Avinava Dubey","Krzysztof Choromanski","Somnath Basu Roy Chowdhury","Deepali Jain","Vikas Sindhwani","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.17740v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.17720v1","updated":"2024-06-25T17:09:54Z","published":"2024-06-25T17:09:54Z","title":"Arboretum: A Large Multimodal Dataset Enabling AI for Biodiversity","summary":"  We introduce Arboretum, the largest publicly accessible dataset designed to\nadvance AI for biodiversity applications. This dataset, curated from the\niNaturalist community science platform and vetted by domain experts to ensure\naccuracy, includes 134.6 million images, surpassing existing datasets in scale\nby an order of magnitude. The dataset encompasses image-language paired data\nfor a diverse set of species from birds (Aves), spiders/ticks/mites\n(Arachnida), insects (Insecta), plants (Plantae), fungus/mushrooms (Fungi),\nsnails (Mollusca), and snakes/lizards (Reptilia), making it a valuable resource\nfor multimodal vision-language AI models for biodiversity assessment and\nagriculture research. Each image is annotated with scientific names, taxonomic\ndetails, and common names, enhancing the robustness of AI model training.\n  We showcase the value of Arboretum by releasing a suite of CLIP models\ntrained using a subset of 40 million captioned images. We introduce several new\nbenchmarks for rigorous assessment, report accuracy for zero-shot learning, and\nevaluations across life stages, rare species, confounding species, and various\nlevels of the taxonomic hierarchy.\n  We anticipate that Arboretum will spur the development of AI models that can\nenable a variety of digital tools ranging from pest control strategies, crop\nmonitoring, and worldwide biodiversity assessment and environmental\nconservation. These advancements are critical for ensuring food security,\npreserving ecosystems, and mitigating the impacts of climate change. Arboretum\nis publicly available, easily accessible, and ready for immediate use.\n  Please see the \\href{https://baskargroup.github.io/Arboretum/}{project\nwebsite} for links to our data, models, and code.\n","authors":["Chih-Hsuan Yang","Benjamin Feuer","Zaki Jubery","Zi K. Deng","Andre Nakkab","Md Zahid Hasan","Shivani Chiranjeevi","Kelly Marshall","Nirmal Baishnab","Asheesh K Singh","Arti Singh","Soumik Sarkar","Nirav Merchant","Chinmay Hegde","Baskar Ganapathysubramanian"],"pdf_url":"https://arxiv.org/pdf/2406.17720v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2312.03806v2","updated":"2024-06-25T17:01:54Z","published":"2023-12-06T16:23:26Z","title":"XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies","summary":"  We present XCube (abbreviated as $\\mathcal{X}^3$), a novel generative model\nfor high-resolution sparse 3D voxel grids with arbitrary attributes. Our model\ncan generate millions of voxels with a finest effective resolution of up to\n$1024^3$ in a feed-forward fashion without time-consuming test-time\noptimization. To achieve this, we employ a hierarchical voxel latent diffusion\nmodel which generates progressively higher resolution grids in a coarse-to-fine\nmanner using a custom framework built on the highly efficient VDB data\nstructure. Apart from generating high-resolution objects, we demonstrate the\neffectiveness of XCube on large outdoor scenes at scales of 100m$\\times$100m\nwith a voxel size as small as 10cm. We observe clear qualitative and\nquantitative improvements over past approaches. In addition to unconditional\ngeneration, we show that our model can be used to solve a variety of tasks such\nas user-guided editing, scene completion from a single scan, and text-to-3D.\nThe source code and more results can be found at\nhttps://research.nvidia.com/labs/toronto-ai/xcube/.\n","authors":["Xuanchi Ren","Jiahui Huang","Xiaohui Zeng","Ken Museth","Sanja Fidler","Francis Williams"],"pdf_url":"https://arxiv.org/pdf/2312.03806v2.pdf","comment":"CVPR 2024 Highlight. Code: https://github.com/nv-tlabs/XCube/\n  Website: https://research.nvidia.com/labs/toronto-ai/xcube/"},{"id":"http://arxiv.org/abs/2404.15275v3","updated":"2024-06-25T16:57:27Z","published":"2024-04-23T17:59:43Z","title":"ID-Animator: Zero-Shot Identity-Preserving Human Video Generation","summary":"  Generating high-fidelity human video with specified identities has attracted\nsignificant attention in the content generation community. However, existing\ntechniques struggle to strike a balance between training efficiency and\nidentity preservation, either requiring tedious case-by-case fine-tuning or\nusually missing identity details in the video generation process. In this\nstudy, we present \\textbf{ID-Animator}, a zero-shot human-video generation\napproach that can perform personalized video generation given a single\nreference facial image without further training. ID-Animator inherits existing\ndiffusion-based video generation backbones with a face adapter to encode the\nID-relevant embeddings from learnable facial latent queries. To facilitate the\nextraction of identity information in video generation, we introduce an\nID-oriented dataset construction pipeline that incorporates unified human\nattributes and action captioning techniques from a constructed facial image\npool. Based on this pipeline, a random reference training strategy is further\ndevised to precisely capture the ID-relevant embeddings with an ID-preserving\nloss, thus improving the fidelity and generalization capacity of our model for\nID-specific video generation. Extensive experiments demonstrate the superiority\nof ID-Animator to generate personalized human videos over previous models.\nMoreover, our method is highly compatible with popular pre-trained T2V models\nlike animatediff and various community backbone models, showing high\nextendability in real-world applications for video generation where identity\npreservation is highly desired. Our codes and checkpoints are released at\nhttps://github.com/ID-Animator/ID-Animator.\n","authors":["Xuanhua He","Quande Liu","Shengju Qian","Xin Wang","Tao Hu","Ke Cao","Keyu Yan","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.15275v3.pdf","comment":"Project Page: https://id-animator.github.io/"},{"id":"http://arxiv.org/abs/2405.02652v2","updated":"2024-06-25T16:53:21Z","published":"2024-05-04T12:37:07Z","title":"Deep Pulse-Signal Magnification for remote Heart Rate Estimation in\n  Compressed Videos","summary":"  Recent advancements in data-driven approaches for remote photoplethysmography\n(rPPG) have significantly improved the accuracy of remote heart rate\nestimation. However, the performance of such approaches worsens considerably\nunder video compression, which is nevertheless necessary to store and transmit\nvideo data efficiently. In this paper, we present a novel approach to address\nthe impact of video compression on rPPG estimation, which leverages a\npulse-signal magnification transformation to adapt compressed videos to an\nuncompressed data domain in which the rPPG signal is magnified. We validate the\neffectiveness of our model by exhaustive evaluations on two publicly available\ndatasets, UCLA-rPPG and UBFC-rPPG, employing both intra- and cross-database\nperformance at several compression rates. Additionally, we assess the\nrobustness of our approach on two additional highly compressed and widely-used\ndatasets, MAHNOB-HCI and COHFACE, which reveal outstanding heart rate\nestimation results.\n","authors":["Joaquim Comas","Adria Ruiz","Federico Sukno"],"pdf_url":"https://arxiv.org/pdf/2405.02652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17709v1","updated":"2024-06-25T16:48:18Z","published":"2024-06-25T16:48:18Z","title":"Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and\n  Image Preprocessing","summary":"  In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse datasets from varied clinical settings and neonatal\nage groups. The metrics used for assessment included the DICE similarity\ncoefficient, recall, and accuracy for image segmentation; structural similarity\nfor image reconstruction; and root mean squared error for total brain volume\nestimation from 3D ultrasound images. Our results demonstrate that MGA-Net\nsignificantly outperforms traditional methods, offering superior performance in\nbrain extraction and segmentation while achieving high precision in image\nreconstruction and volumetric analysis. Thus, MGA-Net represents a robust and\neffective preprocessing tool for MRI and 3D ultrasound images, marking a\nsignificant advance in neuroimaging that enhances both research and clinical\ndiagnostics in the neonatal period and beyond.\n","authors":["Bahram Jafrasteh","Simon Pedro Lubian-Lopez","Emiliano Trimarco","Macarena Roman Ruiz","Carmen Rodriguez Barrios","Yolanda Marin Almagro","Isabel Benavente-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2406.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17707v1","updated":"2024-06-25T16:46:21Z","published":"2024-06-25T16:46:21Z","title":"SurgeMOD: Translating image-space tissue motions into vision-based\n  surgical forces","summary":"  We present a new approach for vision-based force estimation in Minimally\nInvasive Robotic Surgery based on frequency domain basis of motion of organs\nderived directly from video. Using internal movements generated by natural\nprocesses like breathing or the cardiac cycle, we infer the image-space basis\nof the motion on the frequency domain. As we are working with this\nrepresentation, we discretize the problem to a limited amount of\nlow-frequencies to build an image-space mechanical model of the environment. We\nuse this pre-built model to define our force estimation problem as a dynamic\nconstraint problem. We demonstrate that this method can estimate point contact\nforces reliably for silicone phantom and ex-vivo experiments, matching real\nreadings from a force sensor. In addition, we perform qualitative experiments\nin which we synthesize coherent force textures from surgical videos over a\ncertain region of interest selected by the user. Our method demonstrates good\nresults for both quantitative and qualitative analysis, providing a good\nstarting point for a purely vision-based method for surgical force estimation.\n","authors":["Mikel De Iturrate Reyzabal","Dionysios Malas","Shuai Wang","Sebastien Ourselin","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17697v1","updated":"2024-06-25T16:33:33Z","published":"2024-06-25T16:33:33Z","title":"HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target\n  Binding Affinity Prediction","summary":"  Drug target binding affinity (DTA) is a key criterion for drug screening.\nExisting experimental methods are time-consuming and rely on limited structural\nand domain information. While learning-based methods can model sequence and\nstructural information, they struggle to integrate contextual data and often\nlack comprehensive modeling of drug-target interactions. In this study, we\npropose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic\nprompts within a hybrid Graph-Transformer framework. Our method generates\ncontext-specific prompts for each drug-target pair, enhancing the model's\nability to capture unique interactions. The introduction of prompt tuning\nfurther optimizes the prediction process by filtering out irrelevant noise and\nemphasizing task-relevant information, dynamically adjusting the input features\nof the molecular graph. The proposed hybrid Graph-Transformer architecture\ncombines structural information from Graph Convolutional Networks (GCNs) with\nsequence information captured by Transformers, facilitating the interaction\nbetween global and local information. Additionally, we adopted the multi-view\nfeature fusion method to project molecular graph views and affinity subgraph\nviews into a common feature space, effectively combining structural and\ncontextual information. Experiments on two widely used public datasets, Davis\nand KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction\nmethods in both prediction performance and generalization ability.\n","authors":["Xi Xiao","Wentao Wang","Jiacheng Xie","Lijing Zhu","Gaofei Chen","Zhengji Li","Tianyang Wang","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13536v2","updated":"2024-06-25T16:28:48Z","published":"2024-06-19T13:19:08Z","title":"Image Distillation for Safe Data Sharing in Histopathology","summary":"  Histopathology can help clinicians make accurate diagnoses, determine disease\nprognosis, and plan appropriate treatment strategies. As deep learning\ntechniques prove successful in the medical domain, the primary challenges\nbecome limited data availability and concerns about data sharing and privacy.\nFederated learning has addressed this challenge by training models locally and\nupdating parameters on a server. However, issues, such as domain shift and\nbias, persist and impact overall performance. Dataset distillation presents an\nalternative approach to overcoming these challenges. It involves creating a\nsmall synthetic dataset that encapsulates essential information, which can be\nshared without constraints. At present, this paradigm is not practicable as\ncurrent distillation approaches only generate non human readable\nrepresentations and exhibit insufficient performance for downstream learning\ntasks. We train a latent diffusion model and construct a new distilled\nsynthetic dataset with a small number of human readable synthetic images.\nSelection of maximally informative synthetic images is done via graph community\nanalysis of the representation space. We compare downstream classification\nmodels trained on our synthetic distillation data to models trained on real\ndata and reach performances suitable for practical application.\n","authors":["Zhe Li","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2406.13536v2.pdf","comment":"accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17688v1","updated":"2024-06-25T16:24:34Z","published":"2024-06-25T16:24:34Z","title":"Unified Auto-Encoding with Masked Diffusion","summary":"  At the core of both successful generative and self-supervised representation\nlearning models there is a reconstruction objective that incorporates some form\nof image corruption. Diffusion models implement this approach through a\nscheduled Gaussian corruption process, while masked auto-encoder models do so\nby masking patches of the image. Despite their different approaches, the\nunderlying similarity in their methodologies suggests a promising avenue for an\nauto-encoder capable of both de-noising tasks. We propose a unified\nself-supervised objective, dubbed Unified Masked Diffusion (UMD), that combines\npatch-based and noise-based corruption techniques within a single auto-encoding\nframework. Specifically, UMD modifies the diffusion transformer (DiT) training\nprocess by introducing an additional noise-free, high masking representation\nstep in the diffusion noising schedule, and utilizes a mixed masked and noised\nimage for subsequent timesteps. By integrating features useful for diffusion\nmodeling and for predicting masked patch tokens, UMD achieves strong\nperformance in downstream generative and representation learning tasks,\nincluding linear probing and class-conditional generation. This is achieved\nwithout the need for heavy data augmentations, multiple views, or additional\nencoders. Furthermore, UMD improves over the computational efficiency of prior\ndiffusion based methods in total training time. We release our code at\nhttps://github.com/philippe-eecs/small-vision.\n","authors":["Philippe Hansen-Estruch","Sriram Vishwanath","Amy Zhang","Manan Tomar"],"pdf_url":"https://arxiv.org/pdf/2406.17688v1.pdf","comment":"19 Pages, 8 Figures, 3Tables"},{"id":"http://arxiv.org/abs/2403.07576v3","updated":"2024-06-25T16:15:54Z","published":"2024-03-12T12:05:43Z","title":"Fine-grained Prompt Tuning: A Parameter and Memory Efficient Transfer\n  Learning Method for High-resolution Medical Image Classification","summary":"  Parameter-efficient transfer learning (PETL) is proposed as a cost-effective\nway to transfer pre-trained models to downstream tasks, avoiding the high cost\nof updating entire large-scale pre-trained models (LPMs). In this work, we\npresent Fine-grained Prompt Tuning (FPT), a novel PETL method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPETL methods, especially in high-resolution input contexts. To achieve this, we\nfirst freeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2403.07576v3.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17680v1","updated":"2024-06-25T16:12:52Z","published":"2024-06-25T16:12:52Z","title":"End-to-End Autonomous Driving without Costly Modularization and 3D\n  Manual Annotation","summary":"  We propose UAD, a method for vision-based end-to-end autonomous driving\n(E2EAD), achieving the best open-loop evaluation performance in nuScenes,\nmeanwhile showing robust closed-loop driving quality in CARLA. Our motivation\nstems from the observation that current E2EAD models still mimic the modular\narchitecture in typical driving stacks, with carefully designed supervised\nperception and prediction subtasks to provide environment information for\noriented planning. Although achieving groundbreaking progress, such design has\ncertain drawbacks: 1) preceding subtasks require massive high-quality 3D\nannotations as supervision, posing a significant impediment to scaling the\ntraining data; 2) each submodule entails substantial computation overhead in\nboth training and inference. To this end, we propose UAD, an E2EAD framework\nwith an unsupervised proxy to address all these issues. Firstly, we design a\nnovel Angular Perception Pretext to eliminate the annotation requirement. The\npretext models the driving scene by predicting the angular-wise spatial\nobjectness and temporal dynamics, without manual annotation. Secondly, a\nself-supervised training strategy, which learns the consistency of the\npredicted trajectories under different augment views, is proposed to enhance\nthe planning robustness in steering scenarios. Our UAD achieves 38.7% relative\nimprovements over UniAD on the average collision rate in nuScenes and surpasses\nVAD for 41.32 points on the driving score in CARLA's Town05 Long benchmark.\nMoreover, the proposed method only consumes 44.3% training resources of UniAD\nand runs 3.4 times faster in inference. Our innovative design not only for the\nfirst time demonstrates unarguable performance advantages over supervised\ncounterparts, but also enjoys unprecedented efficiency in data, training, and\ninference. Code and models will be released at\nhttps://github.com/KargoBot_Research/UAD.\n","authors":["Mingzhe Guo","Zhipeng Zhang","Yuan He","Ke Wang","Liping Jing"],"pdf_url":"https://arxiv.org/pdf/2406.17680v1.pdf","comment":"17 pages, 10 figures and 15 tables"},{"id":"http://arxiv.org/abs/2406.17679v1","updated":"2024-06-25T16:12:20Z","published":"2024-06-25T16:12:20Z","title":"Local-to-Global Cross-Modal Attention-Aware Fusion for HSI-X Semantic\n  Segmentation","summary":"  Hyperspectral image (HSI) classification has recently reached its performance\nbottleneck. Multimodal data fusion is emerging as a promising approach to\novercome this bottleneck by providing rich complementary information from the\nsupplementary modality (X-modality). However, achieving comprehensive\ncross-modal interaction and fusion that can be generalized across different\nsensing modalities is challenging due to the disparity in imaging sensors,\nresolution, and content of different modalities. In this study, we propose a\nLocal-to-Global Cross-modal Attention-aware Fusion (LoGoCAF) framework for\nHSI-X classification that jointly considers efficiency, accuracy, and\ngeneralizability. LoGoCAF adopts a pixel-to-pixel two-branch semantic\nsegmentation architecture to learn information from HSI and X modalities. The\npipeline of LoGoCAF consists of a local-to-global encoder and a lightweight\nmultilayer perceptron (MLP) decoder. In the encoder, convolutions are used to\nencode local and high-resolution fine details in shallow layers, while\ntransformers are used to integrate global and low-resolution coarse features in\ndeeper layers. The MLP decoder aggregates information from the encoder for\nfeature fusion and prediction. In particular, two cross-modality modules, the\nfeature enhancement module (FEM) and the feature interaction and fusion module\n(FIFM), are introduced in each encoder stage. The FEM is used to enhance\ncomplementary information by combining the feature from the other modality\nacross direction-aware, position-sensitive, and channel-wise dimensions. With\nthe enhanced features, the FIFM is designed to promote cross-modality\ninformation interaction and fusion for the final semantic prediction. Extensive\nexperiments demonstrate that our LoGoCAF achieves superior performance and\ngeneralizes well. The code will be made publicly available.\n","authors":["Xuming Zhang","Naoto Yokoya","Xingfa Gu","Qingjiu Tian","Lorenzo Bruzzone"],"pdf_url":"https://arxiv.org/pdf/2406.17679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15889v2","updated":"2024-06-25T16:02:25Z","published":"2023-03-28T11:04:18Z","title":"Metrics for Dataset Demographic Bias: A Case Study on Facial Expression\n  Recognition","summary":"  Demographic biases in source datasets have been shown as one of the causes of\nunfairness and discrimination in the predictions of Machine Learning models.\nOne of the most prominent types of demographic bias are statistical imbalances\nin the representation of demographic groups in the datasets. In this paper, we\nstudy the measurement of these biases by reviewing the existing metrics,\nincluding those that can be borrowed from other disciplines. We develop a\ntaxonomy for the classification of these metrics, providing a practical guide\nfor the selection of appropriate metrics. To illustrate the utility of our\nframework, and to further understand the practical characteristics of the\nmetrics, we conduct a case study of 20 datasets used in Facial Emotion\nRecognition (FER), analyzing the biases present in them. Our experimental\nresults show that many metrics are redundant and that a reduced subset of\nmetrics may be sufficient to measure the amount of demographic bias. The paper\nprovides valuable insights for researchers in AI and related fields to mitigate\ndataset bias and improve the fairness and accuracy of AI models. The code is\navailable at https://github.com/irisdominguez/dataset_bias_metrics.\n","authors":["Iris Dominguez-Catena","Daniel Paternain","Mikel Galar"],"pdf_url":"https://arxiv.org/pdf/2303.15889v2.pdf","comment":"16 pages, 8 figures. Including appendix: 45 pages, 32 figures.\n  Updated from previous version with an additional appendix, addressing\n  concerns about the interest of studying bias at the dataset level"},{"id":"http://arxiv.org/abs/2406.17670v1","updated":"2024-06-25T15:58:56Z","published":"2024-06-25T15:58:56Z","title":"Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration","summary":"  Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.\n","authors":["Mohammad Ali Labbaf Khaniki","Alireza Golkarieh","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2406.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17652v1","updated":"2024-06-25T15:43:20Z","published":"2024-06-25T15:43:20Z","title":"Time-varying Extremum Graphs","summary":"  We introduce time-varying extremum graph (TVEG), a topological structure to\nsupport visualization and analysis of a time-varying scalar field. The extremum\ngraph is a substructure of the Morse-Smale complex. It captures the adjacency\nrelationship between cells in the Morse decomposition of a scalar field. We\ndefine the TVEG as a time-varying extension of the extremum graph and\ndemonstrate how it captures salient feature tracks within a dynamic scalar\nfield. We formulate the construction of the TVEG as an optimization problem and\ndescribe an algorithm for computing the graph. We also demonstrate the\ncapabilities of \\TVEG towards identification and exploration of topological\nevents such as deletion, generation, split, and merge within a dynamic scalar\nfield via comprehensive case studies including a viscous fingers and a 3D von\nK\\'arm\\'an vortex street dataset.\n","authors":["Somenath Das","Raghavendra Sridharamurthy","Vijay Natarajan"],"pdf_url":"https://arxiv.org/pdf/2406.17652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17640v1","updated":"2024-06-25T15:24:06Z","published":"2024-06-25T15:24:06Z","title":"BayTTA: Uncertainty-aware medical image classification with optimized\n  test-time augmentation using Bayesian model averaging","summary":"  Test-time augmentation (TTA) is a well-known technique employed during the\ntesting phase of computer vision tasks. It involves aggregating multiple\naugmented versions of input data. Combining predictions using a simple average\nformulation is a common and straightforward approach after performing TTA. This\npaper introduces a novel framework for optimizing TTA, called BayTTA\n(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,\nwe generate a model list associated with different variations of the input data\ncreated through TTA. Then, we use BMA to combine model predictions weighted by\ntheir respective posterior probabilities. Such an approach allows one to take\ninto account model uncertainty, and thus to enhance the predictive performance\nof the related machine learning or deep learning model. We evaluate the\nperformance of BayTTA on various public data, including three medical image\ndatasets comprising skin cancer, breast cancer, and chest X-ray images and two\nwell-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental\nresults indicate that BayTTA can be effectively integrated into\nstate-of-the-art deep learning models used in medical image analysis as well as\ninto some popular pre-trained CNN models such as VGG-16, MobileNetV2,\nDenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in\ntheir accuracy and robustness performance.\n","authors":["Zeinab Sherkatghanad","Moloud Abdar","Mohammadreza Bakhtyari","Vladimir Makarenkov"],"pdf_url":"https://arxiv.org/pdf/2406.17640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17639v1","updated":"2024-06-25T15:24:02Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17636v1","updated":"2024-06-25T15:21:50Z","published":"2024-06-25T15:21:50Z","title":"Aligning Diffusion Models with Noise-Conditioned Perception","summary":"  Recent advancements in human preference optimization, initially developed for\nLanguage Models (LMs), have shown promise for text-to-image Diffusion Models,\nenhancing prompt alignment, visual appeal, and user preference. Unlike LMs,\nDiffusion Models typically optimize in pixel or VAE space, which does not align\nwell with human perception, leading to slower and less efficient training\nduring the preference alignment stage. We propose using a perceptual objective\nin the U-Net embedding space of the diffusion model to address these issues.\nOur approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct\nPreference Optimization (DPO), Contrastive Preference Optimization (CPO), and\nsupervised fine-tuning (SFT) within this embedding space. This method\nsignificantly outperforms standard latent-space implementations across various\nmetrics, including quality and computational cost. For SDXL, our approach\nprovides 60.8\\% general preference, 62.2\\% visual appeal, and 52.1\\% prompt\nfollowing against original open-sourced SDXL-DPO on the PartiPrompts dataset,\nwhile significantly reducing compute. Our approach not only improves the\nefficiency and quality of human preference alignment for diffusion models but\nis also easily integrable with other optimization techniques. The training code\nand LoRA weights will be available here:\nhttps://huggingface.co/alexgambashidze/SDXL\\_NCP-DPO\\_v0.1\n","authors":["Alexander Gambashidze","Anton Kulikov","Yuriy Sosnin","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2406.17636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17628v1","updated":"2024-06-25T15:15:54Z","published":"2024-06-25T15:15:54Z","title":"Video Inpainting Localization with Contrastive Learning","summary":"  Deep video inpainting is typically used as malicious manipulation to remove\nimportant objects for creating fake videos. It is significant to identify the\ninpainted regions blindly. This letter proposes a simple yet effective forensic\nscheme for Video Inpainting LOcalization with ContrAstive Learning (ViLocal).\nSpecifically, a 3D Uniformer encoder is applied to the video noise residual for\nlearning effective spatiotemporal forensic features. To enhance the\ndiscriminative power, supervised contrastive learning is adopted to capture the\nlocal inconsistency of inpainted videos through attracting/repelling the\npositive/negative pristine and forged pixel pairs. A pixel-wise inpainting\nlocalization map is yielded by a lightweight convolution decoder with a\nspecialized two-stage training strategy. To prepare enough training samples, we\nbuild a video object segmentation dataset of 2500 videos with pixel-level\nannotations per frame. Extensive experimental results validate the superiority\nof ViLocal over state-of-the-arts. Code and dataset will be available at\nhttps://github.com/multimediaFor/ViLocal.\n","authors":["Zijie Lou","Gang Cao","Man Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17628v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.13576"},{"id":"http://arxiv.org/abs/2406.17617v1","updated":"2024-06-25T15:02:01Z","published":"2024-06-25T15:02:01Z","title":"Embedded event based object detection with spiking neural network","summary":"  The complexity of event-based object detection (OD) poses considerable\nchallenges. Spiking Neural Networks (SNNs) show promising results and pave the\nway for efficient event-based OD. Despite this success, the path to efficient\nSNNs on embedded devices remains a challenge. This is due to the size of the\nnetworks required to accomplish the task and the ability of devices to take\nadvantage of SNNs benefits. Even when \"edge\" devices are considered, they\ntypically use embedded GPUs that consume tens of watts. In response to these\nchallenges, our research introduces an embedded neuromorphic testbench that\nutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.\nUsing an extended version of the Qualia framework, we can train, evaluate,\nquantize, and deploy spiking neural networks on an FPGA implementation of\nSPLEAT. We used this testbench to load a state-of-the-art SNN solution,\nestimate the performance loss associated with deploying the network on\ndedicated hardware, and run real-world event-based OD on neuromorphic hardware\nspecifically designed for low-power spiking neural networks. Remarkably, our\nembedded spiking solution, which includes a model with 1.08 million parameters,\noperates efficiently with 490 mJ per prediction.\n","authors":["Jonathan Courtois","Pierre-Emmanuel Novac","Edgar Lemaire","Alain Pegatoquet","Benoit Miramond"],"pdf_url":"https://arxiv.org/pdf/2406.17617v1.pdf","comment":"Result link: https://youtu.be/TsolUDaMY7Y"},{"id":"http://arxiv.org/abs/2406.17614v1","updated":"2024-06-25T15:00:43Z","published":"2024-06-25T15:00:43Z","title":"MSRS: Training Multimodal Speech Recognition Models from Scratch with\n  Sparse Mask Optimization","summary":"  Pre-trained models have been a foundational approach in speech recognition,\nalbeit with associated additional costs. In this study, we propose a\nregularization technique that facilitates the training of visual and\naudio-visual speech recognition models (VSR and AVSR) from scratch. This\napproach, abbreviated as \\textbf{MSRS} (Multimodal Speech Recognition from\nScratch), introduces a sparse regularization that rapidly learns sparse\nstructures within the dense model at the very beginning of training, which\nreceives healthier gradient flow than the dense equivalent. Once the sparse\nmask stabilizes, our method allows transitioning to a dense model or keeping a\nsparse model by updating non-zero values. MSRS achieves competitive results in\nVSR and AVSR with 21.1% and 0.9% WER on the LRS3 benchmark, while reducing\ntraining time by at least 2x. We explore other sparse approaches and show that\nonly MSRS enables training from scratch by implicitly masking the weights\naffected by vanishing gradients.\n","authors":["Adriana Fernandez-Lopez","Honglie Chen","Pingchuan Ma","Lu Yin","Qiao Xiao","Stavros Petridis","Shiwei Liu","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2406.17614v1.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.17608v1","updated":"2024-06-25T14:53:01Z","published":"2024-06-25T14:53:01Z","title":"Test-Time Generative Augmentation for Medical Image Segmentation","summary":"  In this paper, we propose a novel approach to enhance medical image\nsegmentation during test time. Instead of employing hand-crafted transforms or\nfunctions on the input test image to create multiple views for test-time\naugmentation, we advocate for the utilization of an advanced domain-fine-tuned\ngenerative model (GM), e.g., stable diffusion (SD), for test-time augmentation.\nGiven that the GM has been trained to comprehend and encapsulate comprehensive\ndomain data knowledge, it is superior than segmentation models in terms of\nrepresenting the data characteristics and distribution. Hence, by integrating\nthe GM into test-time augmentation, we can effectively generate multiple views\nof a given test sample, aligning with the content and appearance\ncharacteristics of the sample and the related local data distribution. This\napproach renders the augmentation process more adaptable and resilient compared\nto conventional handcrafted transforms. Comprehensive experiments conducted\nacross three medical image segmentation tasks (nine datasets) demonstrate the\nefficacy and versatility of the proposed TTGA in enhancing segmentation\noutcomes. Moreover, TTGA significantly improves pixel-wise error estimation,\nthereby facilitating the deployment of a more reliable segmentation system.\nCode will be released at: https://github.com/maxiao0234/TTGA.\n","authors":["Xiao Ma","Yuhui Tao","Yuhan Zhang","Zexuan Ji","Yizhe Zhang","Qiang Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17608v1.pdf","comment":"12pages, 2figures"},{"id":"http://arxiv.org/abs/2404.17569v2","updated":"2024-06-25T14:44:52Z","published":"2024-04-26T17:54:38Z","title":"MaPa: Text-driven Photorealistic Material Painting for 3D Shapes","summary":"  This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page: https://zju3dv.github.io/MaPa\n","authors":["Shangzhan Zhang","Sida Peng","Tao Xu","Yuanbo Yang","Tianrun Chen","Nan Xue","Yujun Shen","Hujun Bao","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.17569v2.pdf","comment":"SIGGRAPH 2024. Project page: https://zju3dv.github.io/MaPa"},{"id":"http://arxiv.org/abs/2406.17601v1","updated":"2024-06-25T14:42:51Z","published":"2024-06-25T14:42:51Z","title":"Director3D: Real-world Camera Trajectory and 3D Scene Generation from\n  Text","summary":"  Recent advancements in 3D generation have leveraged synthetic datasets with\nground truth 3D assets and predefined cameras. However, the potential of\nadopting real-world datasets, which can produce significantly more realistic 3D\nscenes, remains largely unexplored. In this work, we delve into the key\nchallenge of the complex and scene-specific camera trajectories found in\nreal-world captures. We introduce Director3D, a robust open-world text-to-3D\ngeneration framework, designed to generate both real-world 3D scenes and\nadaptive camera trajectories. To achieve this, (1) we first utilize a\nTrajectory Diffusion Transformer, acting as the Cinematographer, to model the\ndistribution of camera trajectories based on textual descriptions. (2) Next, a\nGaussian-driven Multi-view Latent Diffusion Model serves as the Decorator,\nmodeling the image sequence distribution given the camera trajectories and\ntexts. This model, fine-tuned from a 2D diffusion model, directly generates\npixel-aligned 3D Gaussians as an immediate 3D scene representation for\nconsistent denoising. (3) Lastly, the 3D Gaussians are refined by a novel SDS++\nloss as the Detailer, which incorporates the prior of the 2D diffusion model.\nExtensive experiments demonstrate that Director3D outperforms existing methods,\noffering superior performance in real-world 3D generation.\n","authors":["Xinyang Li","Zhangyu Lai","Linning Xu","Yansong Qu","Liujuan Cao","Shengchuan Zhang","Bo Dai","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.17601v1.pdf","comment":"Code: https://github.com/imlixinyang/director3d"},{"id":"http://arxiv.org/abs/2406.17591v1","updated":"2024-06-25T14:32:31Z","published":"2024-06-25T14:32:31Z","title":"DocParseNet: Advanced Semantic Segmentation and OCR Embeddings for\n  Efficient Scanned Document Annotation","summary":"  Automating the annotation of scanned documents is challenging, requiring a\nbalance between computational efficiency and accuracy. DocParseNet addresses\nthis by combining deep learning and multi-modal learning to process both text\nand visual data. This model goes beyond traditional OCR and semantic\nsegmentation, capturing the interplay between text and images to preserve\ncontextual nuances in complex document structures. Our evaluations show that\nDocParseNet significantly outperforms conventional models, achieving mIoU\nscores of 49.12 on validation and 49.78 on the test set. This reflects a 58%\naccuracy improvement over state-of-the-art baseline models and an 18% gain\ncompared to the UNext baseline. Remarkably, DocParseNet achieves these results\nwith only 2.8 million parameters, reducing the model size by approximately 25\ntimes and speeding up training by 5 times compared to other models. These\nmetrics, coupled with a computational efficiency of 0.034 TFLOPs (BS=1),\nhighlight DocParseNet's high performance in document annotation. The model's\nadaptability and scalability make it well-suited for real-world corporate\ndocument processing applications. The code is available at\nhttps://github.com/ahmad-shirazi/DocParseNet\n","authors":["Ahmad Mohammadshirazi","Ali Nosrati Firoozsalari","Mengxi Zhou","Dheeraj Kulshrestha","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2406.17591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17577v1","updated":"2024-06-25T14:18:42Z","published":"2024-06-25T14:18:42Z","title":"Advancing Cell Detection in Anterior Segment Optical Coherence\n  Tomography Images","summary":"  Anterior uveitis, a common form of eye inflammation, can lead to permanent\nvision loss if not promptly diagnosed. Monitoring this condition involves\nquantifying inflammatory cells in the anterior chamber (AC) of the eye, which\ncan be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).\nHowever, manually identifying cells in AS-OCT images is time-consuming and\nsubjective. Moreover, existing automated approaches may have limitations in\nboth the effectiveness of detecting cells and the reliability of their\ndetection results. To address these challenges, we propose an automated\nframework to detect cells in the AS-OCT images. This framework consists of a\nzero-shot chamber segmentation module and a cell detection module. The first\nmodule segments the AC area in the image without requiring human-annotated\ntraining data. Subsequently, the second module identifies individual cells\nwithin the segmented AC region. Through experiments, our framework demonstrates\nsuperior performance compared to current state-of-the-art methods for both AC\nsegmentation and cell detection tasks. Notably, we find that previous cell\ndetection approaches could suffer from low recall, potentially overlooking a\nsignificant number of cells. In contrast, our framework offers an improved\nsolution, which could benefit the diagnosis and study of anterior uveitis. Our\ncode for cell detection is publicly available at:\nhttps://github.com/joeybyc/cell_detection.\n","authors":["Boyu Chen","Ameenat L. Solebo","Paul Taylor"],"pdf_url":"https://arxiv.org/pdf/2406.17577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17575v1","updated":"2024-06-25T14:15:48Z","published":"2024-06-25T14:15:48Z","title":"Toward Universal Medical Image Registration via Sharpness-Aware\n  Meta-Continual Learning","summary":"  Current deep learning approaches in medical image registration usually face\nthe challenges of distribution shift and data collection, hindering real-world\ndeployment. In contrast, universal medical image registration aims to perform\nregistration on a wide range of clinically relevant tasks simultaneously, thus\nhaving tremendous potential for clinical applications. In this paper, we\npresent the first attempt to achieve the goal of universal 3D medical image\nregistration in sequential learning scenarios by proposing a continual learning\nmethod. Specifically, we utilize meta-learning with experience replay to\nmitigating the problem of catastrophic forgetting. To promote the\ngeneralizability of meta-continual learning, we further propose sharpness-aware\nmeta-continual learning (SAMCL). We validate the effectiveness of our method on\nfour datasets in a continual learning setup, including brain MR, abdomen CT,\nlung CT, and abdomen MR-CT image pairs. Results have shown the potential of\nSAMCL in realizing universal image registration, which performs better than or\non par with vanilla sequential or centralized multi-task training\nstrategies.The source code will be available from\nhttps://github.com/xzluo97/Continual-Reg.\n","authors":["Bomin Wang","Xinzhe Luo","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.17575v1.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17559v1","updated":"2024-06-25T13:54:39Z","published":"2024-06-25T13:54:39Z","title":"Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation","summary":"  The rapid scaling of large vision pretrained models makes fine-tuning tasks\nmore and more difficult on edge devices with low computational resources. We\nexplore a new visual adaptation paradigm called edge tuning, which treats large\npretrained models as standalone feature extractors that run on powerful cloud\nservers. The fine-tuning carries out on edge devices with small networks which\nrequire low computational resources. Existing methods that are potentially\nsuitable for our edge tuning paradigm are discussed. But, three major drawbacks\nhinder their application in edge tuning: low adaptation capability, large\nadapter network, and high information transfer overhead. To address these\nissues, we propose Minimal Interaction Edge Tuning, or MIET, which reveals that\nthe sum of intermediate features from pretrained models not only has minimal\ninformation transfer but also has high adaptation capability. With a\nlightweight attention-based adaptor network, MIET achieves information transfer\nefficiency, parameter efficiency, computational and memory efficiency, and at\nthe same time demonstrates competitive results on various visual adaptation\nbenchmarks.\n","authors":["Ningyuan Tang","Minghao Fu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17559v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.09335v2","updated":"2024-06-25T13:47:06Z","published":"2024-06-13T17:17:31Z","title":"Instance-level quantitative saliency in multiple sclerosis lesion\n  segmentation","summary":"  In recent years, explainable methods for artificial intelligence (XAI) have\ntried to reveal and describe models' decision mechanisms in the case of\nclassification tasks. However, XAI for semantic segmentation and in particular\nfor single instances has been little studied to date. Understanding the process\nunderlying automatic segmentation of single instances is crucial to reveal what\ninformation was used to detect and segment a given object of interest. In this\nstudy, we proposed two instance-level explanation maps for semantic\nsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated\ntheir relevance for the detection and segmentation of white matter lesions\n(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).\n687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans\nwere collected at the University Hospital of Basel, Switzerland. Data were\nrandomly split into training, validation and test sets to train a 3D U-Net for\nMS lesion segmentation. We observed 3050 true positive (TP), 1818 false\npositive (FP), and 789 false negative (FN) cases. We generated instance-level\nexplanation maps for semantic segmentation, by developing two XAI methods based\non SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients\nin saliency maps with respect to both input MRI sequences; 2) the model's\nresponse in the case of synthetic lesions; 3) the amount of perilesional tissue\nneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) in\nFLAIR showed positive values inside a lesion and negative in its neighborhood.\nPeak values of saliency maps generated for these four groups of volumes\npresented distributions that differ significantly from one another, suggesting\na quantitative nature of the proposed saliency. Contextual information of 7mm\naround the lesion border was required for their segmentation.\n","authors":["Federico Spagnolo","Nataliia Molchanova","Roger Schaer","Meritxell Bach Cuadra","Mario Ocampo Pineda","Lester Melie-Garcia","Cristina Granziera","Vincent Andrearczyk","Adrien Depeursinge"],"pdf_url":"https://arxiv.org/pdf/2406.09335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17547v1","updated":"2024-06-25T13:34:50Z","published":"2024-06-25T13:34:50Z","title":"Detection of Synthetic Face Images: Accuracy, Robustness, Generalization","summary":"  An experimental study on detecting synthetic face images is presented. We\ncollected a dataset, called FF5, of five fake face image generators, including\nrecent diffusion models. We find that a simple model trained on a specific\nimage generator can achieve near-perfect accuracy in separating synthetic and\nreal images. The model handles common image distortions (reduced resolution,\ncompression) by using data augmentation. Moreover, partial manipulations, where\nsynthetic images are blended into real ones by inpainting, are identified and\nthe area of the manipulation is localized by a simple model of YOLO\narchitecture. However, the model turned out to be vulnerable to adversarial\nattacks and does not generalize to unseen generators. Failure to generalize to\ndetect images produced by a newer generator also occurs for recent\nstate-of-the-art methods, which we tested on Realistic Vision, a fine-tuned\nversion of StabilityAI's Stable Diffusion image generator.\n","authors":["Nela Petrzelkova","Jan Cech"],"pdf_url":"https://arxiv.org/pdf/2406.17547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17541v1","updated":"2024-06-25T13:28:53Z","published":"2024-06-25T13:28:53Z","title":"Principal Component Clustering for Semantic Segmentation in Synthetic\n  Data Generation","summary":"  This technical report outlines our method for generating a synthetic dataset\nfor semantic segmentation using a latent diffusion model. Our approach\neliminates the need for additional models specifically trained on segmentation\ndata and is part of our submission to the CVPR 2024 workshop challenge,\nentitled CVPR 2024 workshop challenge \"SyntaGen Harnessing Generative Models\nfor Synthetic Visual Datasets\". Our methodology uses self-attentions to\nfacilitate a novel head-wise semantic information condensation, thereby\nenabling the direct acquisition of class-agnostic image segmentation from the\nStable Diffusion latents. Furthermore, we employ non-prompt-influencing\ncross-attentions from text to pixel, thus facilitating the classification of\nthe previously generated masks. Finally, we propose a mask refinement step by\nusing only the output image by Stable Diffusion.\n","authors":["Felix Stillger","Frederik Hasecke","Tobias Meisen"],"pdf_url":"https://arxiv.org/pdf/2406.17541v1.pdf","comment":"This is a technical report for a submission to the CVPR \"SyntaGen -\n  Harnessing Generative Models for Synthetic Visual Datasets\" workshop\n  challenge. The report is already uploaded to the workshop's homepage\n  https://syntagen.github.io/"},{"id":"http://arxiv.org/abs/2112.09726v4","updated":"2024-06-25T13:28:04Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Cristóbal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v4.pdf","comment":"https://soundify.cc"},{"id":"http://arxiv.org/abs/2406.17538v1","updated":"2024-06-25T13:22:22Z","published":"2024-06-25T13:22:22Z","title":"SKD-TSTSAN: Three-Stream Temporal-Shift Attention Network Based on\n  Self-Knowledge Distillation for Micro-Expression Recognition","summary":"  Micro-expressions (MEs) are subtle facial movements that occur spontaneously\nwhen people try to conceal the real emotions. Micro-expression recognition\n(MER) is crucial in many fields, including criminal analysis and psychotherapy.\nHowever, MER is challenging since MEs have low intensity and ME datasets are\nsmall in size. To this end, a three-stream temporal-shift attention network\nbased on self-knowledge distillation (SKD-TSTSAN) is proposed in this paper.\nFirstly, to address the low intensity of ME muscle movements, we utilize\nlearning-based motion magnification modules to enhance the intensity of ME\nmuscle movements. Secondly, we employ efficient channel attention (ECA) modules\nin the local-spatial stream to make the network focus on facial regions that\nare highly relevant to MEs. In addition, temporal shift modules (TSMs) are used\nin the dynamic-temporal stream, which enables temporal modeling with no\nadditional parameters by mixing ME motion information from two different\ntemporal domains. Furthermore, we introduce self-knowledge distillation (SKD)\ninto the MER task by introducing auxiliary classifiers and using the deepest\nsection of the network for supervision, encouraging all blocks to fully explore\nthe features of the training set. Finally, extensive experiments are conducted\non four ME datasets: CASME II, SAMM, MMEW, and CAS(ME)3. The experimental\nresults demonstrate that our SKD-TSTSAN outperforms other existing methods and\nachieves new state-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/SKD-TSTSAN.\n","authors":["Guanghao Zhu","Lin Liu","Yuhao Hu","Haixin Sun","Fang Liu","Xiaohui Du","Ruqian Hao","Juanxiu Liu","Yong Liu","Hao Deng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12492v2","updated":"2024-06-25T13:20:45Z","published":"2022-11-22T18:58:22Z","title":"VideoMap: Supporting Video Editing Exploration, Brainstorming, and\n  Prototyping in the Latent Space","summary":"  Video editing is a creative and complex endeavor and we believe that there is\npotential for reimagining a new video editing interface to better support the\ncreative and exploratory nature of video editing. We take inspiration from\nlatent space exploration tools that help users find patterns and connections\nwithin complex datasets. We present VideoMap, a proof-of-concept video editing\ninterface that operates on video frames projected onto a latent space. We\nsupport intuitive navigation through map-inspired navigational elements and\nfacilitate transitioning between different latent spaces through swappable\nlenses. We built three VideoMap components to support editors in three common\nvideo tasks. In a user study with both professionals and non-professionals,\neditors found that VideoMap helps reduce grunt work, offers a user-friendly\nexperience, provides an inspirational way of editing, and effectively supports\nthe exploratory nature of video editing. We further demonstrate the versatility\nof VideoMap by implementing three extended applications. For interactive\nexamples, we invite you to visit our project page:\nhttps://humanvideointeraction.github.io/videomap.\n","authors":["David Chuan-En Lin","Fabian Caba Heilbron","Joon-Young Lee","Oliver Wang","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2211.12492v2.pdf","comment":"https://humanvideointeraction.github.io/videomap"},{"id":"http://arxiv.org/abs/2406.17536v1","updated":"2024-06-25T13:20:39Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api.\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12493v2","updated":"2024-06-25T13:20:06Z","published":"2022-11-22T18:59:04Z","title":"Videogenic: Identifying Highlight Moments in Videos with Professional\n  Photographs as a Prior","summary":"  This paper investigates the challenge of extracting highlight moments from\nvideos. To perform this task, we need to understand what constitutes a\nhighlight for arbitrary video domains while at the same time being able to\nscale across different domains. Our key insight is that photographs taken by\nphotographers tend to capture the most remarkable or photogenic moments of an\nactivity. Drawing on this insight, we present Videogenic, a technique capable\nof creating domain-specific highlight videos for a diverse range of domains. In\na human evaluation study (N=50), we show that a high-quality photograph\ncollection combined with CLIP-based retrieval (which uses a neural network with\nsemantic knowledge of images) can serve as an excellent prior for finding video\nhighlights. In a within-subjects expert study (N=12), we demonstrate the\nusefulness of Videogenic in helping video editors create highlight videos with\nlighter workload, shorter task completion time, and better usability.\n","authors":["David Chuan-En Lin","Fabian Caba Heilbron","Joon-Young Lee","Oliver Wang","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2211.12493v2.pdf","comment":"https://humanvideointeraction.github.io/videogenic"},{"id":"http://arxiv.org/abs/2406.17530v1","updated":"2024-06-25T13:14:26Z","published":"2024-06-25T13:14:26Z","title":"Point Tree Transformer for Point Cloud Registration","summary":"  Point cloud registration is a fundamental task in the fields of computer\nvision and robotics. Recent developments in transformer-based methods have\ndemonstrated enhanced performance in this domain. However, the standard\nattention mechanism utilized in these methods often integrates many\nlow-relevance points, thereby struggling to prioritize its attention weights on\nsparse yet meaningful points. This inefficiency leads to limited local\nstructure modeling capabilities and quadratic computational complexity. To\novercome these limitations, we propose the Point Tree Transformer (PTT), a\nnovel transformer-based approach for point cloud registration that efficiently\nextracts comprehensive local and global features while maintaining linear\ncomputational complexity. The PTT constructs hierarchical feature trees from\npoint clouds in a coarse-to-dense manner, and introduces a novel Point Tree\nAttention (PTA) mechanism, which follows the tree structure to facilitate the\nprogressive convergence of attended regions towards salient points.\nSpecifically, each tree layer selectively identifies a subset of key points\nwith the highest attention scores. Subsequent layers focus attention on areas\nof significant relevance, derived from the child points of the selected point\nset. The feature extraction process additionally incorporates coarse point\nfeatures that capture high-level semantic information, thus facilitating local\nstructure modeling and the progressive integration of multiscale information.\nConsequently, PTA empowers the model to concentrate on crucial local structures\nand derive detailed local information while maintaining linear computational\ncomplexity. Extensive experiments conducted on the 3DMatch, ModelNet40, and\nKITTI datasets demonstrate that our method achieves superior performance over\nthe state-of-the-art methods.\n","authors":["Meiling Wang","Guangyan Chen","Yi Yang","Li Yuan","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2406.17530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08625v2","updated":"2024-06-25T13:12:20Z","published":"2024-06-12T20:15:00Z","title":"FSBI: Deepfakes Detection with Frequency Enhanced Self-Blended Images","summary":"  Advances in deepfake research have led to the creation of almost perfect\nmanipulations undetectable by human eyes and some deepfakes detection tools.\nRecently, several techniques have been proposed to differentiate deepfakes from\nrealistic images and videos. This paper introduces a Frequency Enhanced\nSelf-Blended Images (FSBI) approach for deepfakes detection. This proposed\napproach utilizes Discrete Wavelet Transforms (DWT) to extract discriminative\nfeatures from the self-blended images (SBI) to be used for training a\nconvolutional network architecture model. The SBIs blend the image with itself\nby introducing several forgery artifacts in a copy of the image before blending\nit. This prevents the classifier from overfitting specific artifacts by\nlearning more generic representations. These blended images are then fed into\nthe frequency features extractor to detect artifacts that can not be detected\neasily in the time domain. The proposed approach has been evaluated on FF++ and\nCeleb-DF datasets and the obtained results outperformed the state-of-the-art\ntechniques with the cross-dataset evaluation protocol.\n","authors":["Ahmed Abul Hasanaath","Hamzah Luqman","Raed Katib","Saeed Anwar"],"pdf_url":"https://arxiv.org/pdf/2406.08625v2.pdf","comment":"The paper is under review"},{"id":"http://arxiv.org/abs/2404.10343v2","updated":"2024-06-25T13:05:30Z","published":"2024-04-16T07:26:20Z","title":"The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report","summary":"  This paper provides a comprehensive review of the NTIRE 2024 challenge,\nfocusing on efficient single-image super-resolution (ESR) solutions and their\noutcomes. The task of this challenge is to super-resolve an input image with a\nmagnification factor of x4 based on pairs of low and corresponding\nhigh-resolution images. The primary objective is to develop networks that\noptimize various aspects such as runtime, parameters, and FLOPs, while still\nmaintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on\nthe DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In\naddition, this challenge has 4 tracks including the main track (overall\nperformance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3\n(parameters). In the main track, all three metrics (ie runtime, FLOPs, and\nparameter count) were considered. The ranking of the main track is calculated\nbased on a weighted sum-up of the scores of all other sub-tracks. In sub-track\n1, the practical runtime performance of the submissions was evaluated, and the\ncorresponding score was used to determine the ranking. In sub-track 2, the\nnumber of FLOPs was considered. The score calculated based on the corresponding\nFLOPs was used to determine the ranking. In sub-track 3, the number of\nparameters was considered. The score calculated based on the corresponding\nparameters was used to determine the ranking. RLFN is set as the baseline for\nefficiency measurement. The challenge had 262 registered participants, and 34\nteams made valid submissions. They gauge the state-of-the-art in efficient\nsingle-image super-resolution. To facilitate the reproducibility of the\nchallenge and enable other researchers to build upon these findings, the code\nand the pre-trained model of validated solutions are made publicly available at\nhttps://github.com/Amazingren/NTIRE2024_ESR/.\n","authors":["Bin Ren","Yawei Li","Nancy Mehta","Radu Timofte","Hongyuan Yu","Cheng Wan","Yuxin Hong","Bingnan Han","Zhuoyuan Wu","Yajun Zou","Yuqing Liu","Jizhe Li","Keji He","Chao Fan","Heng Zhang","Xiaolin Zhang","Xuanwu Yin","Kunlong Zuo","Bohao Liao","Peizhe Xia","Long Peng","Zhibo Du","Xin Di","Wangkai Li","Yang Wang","Wei Zhai","Renjing Pei","Jiaming Guo","Songcen Xu","Yang Cao","Zhengjun Zha","Yan Wang","Yi Liu","Qing Wang","Gang Zhang","Liou Zhang","Shijie Zhao","Long Sun","Jinshan Pan","Jiangxin Dong","Jinhui Tang","Xin Liu","Min Yan","Qian Wang","Menghan Zhou","Yiqiang Yan","Yixuan Liu","Wensong Chan","Dehua Tang","Dong Zhou","Li Wang","Lu Tian","Barsoum Emad","Bohan Jia","Junbo Qiao","Yunshuai Zhou","Yun Zhang","Wei Li","Shaohui Lin","Shenglong Zhou","Binbin Chen","Jincheng Liao","Suiyi Zhao","Zhao Zhang","Bo Wang","Yan Luo","Yanyan Wei","Feng Li","Mingshen Wang","Yawei Li","Jinhan Guan","Dehua Hu","Jiawei Yu","Qisheng Xu","Tao Sun","Long Lan","Kele Xu","Xin Lin","Jingtong Yue","Lehan Yang","Shiyi Du","Lu Qi","Chao Ren","Zeyu Han","Yuhan Wang","Chaolin Chen","Haobo Li","Mingjun Zheng","Zhongbao Yang","Lianhong Song","Xingzhuo Yan","Minghan Fu","Jingyi Zhang","Baiang Li","Qi Zhu","Xiaogang Xu","Dan Guo","Chunle Guo","Jiadi Chen","Huanhuan Long","Chunjiang Duanmu","Xiaoyan Lei","Jie Liu","Weilin Jia","Weifeng Cao","Wenlong Zhang","Yanyu Mao","Ruilong Guo","Nihao Zhang","Qian Wang","Manoj Pandey","Maksym Chernozhukov","Giang Le","Shuli Cheng","Hongyuan Wang","Ziyan Wei","Qingting Tang","Liejun Wang","Yongming Li","Yanhui Guo","Hao Xu","Akram Khatami-Rizi","Ahmad Mahmoudi-Aznaveh","Chih-Chung Hsu","Chia-Ming Lee","Yi-Shiuan Chou","Amogh Joshi","Nikhil Akalwadi","Sampada Malagi","Palani Yashaswini","Chaitra Desai","Ramesh Ashok Tabib","Ujwala Patil","Uma Mudenagudi"],"pdf_url":"https://arxiv.org/pdf/2404.10343v2.pdf","comment":"The report paper of NTIRE2024 Efficient Super-resolution, accepted by\n  CVPRW2024"},{"id":"http://arxiv.org/abs/2406.17520v1","updated":"2024-06-25T12:59:46Z","published":"2024-06-25T12:59:46Z","title":"Tell Me Where You Are: Multimodal LLMs Meet Place Recognition","summary":"  Large language models (LLMs) exhibit a variety of promising capabilities in\nrobotics, including long-horizon planning and commonsense reasoning. However,\ntheir performance in place recognition is still underexplored. In this work, we\nintroduce multimodal LLMs (MLLMs) to visual place recognition (VPR), where a\nrobot must localize itself using visual observations. Our key design is to use\nvision-based retrieval to propose several candidates and then leverage\nlanguage-based reasoning to carefully inspect each candidate for a final\ndecision. Specifically, we leverage the robust visual features produced by\noff-the-shelf vision foundation models (VFMs) to obtain several candidate\nlocations. We then prompt an MLLM to describe the differences between the\ncurrent observation and each candidate in a pairwise manner, and reason about\nthe best candidate based on these descriptions. Our results on three datasets\ndemonstrate that integrating the general-purpose visual features from VFMs with\nthe reasoning capabilities of MLLMs already provides an effective place\nrecognition solution, without any VPR-specific supervised training. We believe\nour work can inspire new possibilities for applying and designing foundation\nmodels, i.e., VFMs, LLMs, and MLLMs, to enhance the localization and navigation\nof mobile robots.\n","authors":["Zonglin Lyu","Juexiao Zhang","Mingxuan Lu","Yiming Li","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2406.17520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00612v2","updated":"2024-06-25T12:49:54Z","published":"2024-03-01T15:35:48Z","title":"Advancing dermatological diagnosis: Development of a hyperspectral\n  dermatoscope for enhanced skin imaging","summary":"  Clinical dermatology necessitates precision and innovation for efficient\ndiagnosis and treatment of various skin conditions. This paper introduces the\ndevelopment of a cutting-edge hyperspectral dermatoscope (the Hyperscope)\ntailored for human skin analysis. We detail the requirements to such a device\nand the design considerations, from optical configurations to sensor selection,\nnecessary to capture a wide spectral range with high fidelity. Preliminary\nresults from 15 individuals and 160 recorded skin images demonstrate the\npotential of the Hyperscope in identifying and characterizing various skin\nconditions, offering a promising avenue for non-invasive skin evaluation and a\nplatform for future research in dermatology-related hyperspectral imaging.\n","authors":["Martin J. Hetz","Carina Nogueira Garcia","Sarah Haggenmüller","Titus J. Brinker"],"pdf_url":"https://arxiv.org/pdf/2403.00612v2.pdf","comment":"12 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2308.12143v4","updated":"2024-06-25T12:34:46Z","published":"2023-08-23T14:00:58Z","title":"A Probabilistic Fluctuation based Membership Inference Attack for\n  Diffusion Models","summary":"  Membership Inference Attack (MIA) identifies whether a record exists in a\nmachine learning model's training set by querying the model. MIAs on the\nclassic classification models have been well-studied, and recent works have\nstarted to explore how to transplant MIA onto generative models. Our\ninvestigation indicates that existing MIAs designed for generative models\nmainly depend on the overfitting in target models. However, overfitting can be\navoided by employing various regularization techniques, whereas existing MIAs\ndemonstrate poor performance in practice. Unlike overfitting, memorization is\nessential for deep learning models to attain optimal performance, making it a\nmore prevalent phenomenon. Memorization in generative models leads to an\nincreasing trend in the probability distribution of generating records around\nthe member record. Therefore, we propose a Probabilistic Fluctuation Assessing\nMembership Inference Attack (PFAMI), a black-box MIA that infers memberships by\ndetecting these trends via analyzing the overall probabilistic fluctuations\naround given records. We conduct extensive experiments across multiple\ngenerative models and datasets, which demonstrate PFAMI can improve the attack\nsuccess rate (ASR) by about 27.9% when compared with the best baseline.\n","authors":["Wenjie Fu","Huandong Wang","Chen Gao","Guanghua Liu","Yong Li","Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2308.12143v4.pdf","comment":"Repo: https://github.com/wjfu99/MIA-Gen"},{"id":"http://arxiv.org/abs/2402.03904v2","updated":"2024-06-25T12:13:58Z","published":"2024-02-06T11:16:18Z","title":"Deep Frequency-Aware Functional Maps for Robust Shape Matching","summary":"  Deep functional map frameworks are widely employed for 3D shape matching.\nHowever, most existing deep functional map methods cannot adaptively capture\nimportant frequency information for functional map estimation in specific\nmatching scenarios, i.e., lacking \\textit{frequency awareness}, resulting in\npoor performance when dealing with large deformable shape matching. To this\nend, we propose a novel unsupervised learning-based framework called Deep\nFrequency-Aware Functional Maps, which can gracefully cope with various shape\nmatching scenarios. We first introduce a general constraint called Spectral\nFilter Operator Preservation to compute desirable functional maps, where the\nspectral filter operator encodes informative frequency information and can\npromote frequency awareness for deep functional map frameworks by learning a\nset of filter functions. Then, we directly utilize the proposed constraint as a\nloss function to supervise functional maps, pointwise maps, and filter\nfunctions simultaneously, where the filter functions are derived from the\northonormal Jacobi basis, and the coefficients of the basis are learnable\nparameters. Finally, we develop an effective refinement strategy to improve the\nfinal pointwise map, which incorporates our constraint and learned filter\nfunctions, leading to more robust and accurate correspondences during the\ninference process. Extensive experimental results on various datasets\ndemonstrate that our approach outperforms the existing state-of-the-art\nmethods, especially in challenging settings like datasets with non-isometric\ndeformation and inconsistent topology.\n","authors":["Feifan Luo","Qinsong Li","Ling Hu","Haibo Wang","Xinru Liu","Shengjun Liu","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17483v1","updated":"2024-06-25T12:04:51Z","published":"2024-06-25T12:04:51Z","title":"TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient\n  Neuromorphic Processing on Event-based Vision","summary":"  Neuromorphic processors are well-suited for efficiently handling sparse\nevents from event-based cameras. However, they face significant challenges in\nthe growth of computing demand and hardware costs as the input resolution\nincreases. This paper proposes the Trainable Region-of-Interest Prediction\n(TRIP), the first hardware-efficient hard attention framework for event-based\nvision processing on a neuromorphic processor. Our TRIP framework actively\nproduces low-resolution Region-of-Interest (ROIs) for efficient and accurate\nclassification. The framework exploits sparse events' inherent low information\ndensity to reduce the overhead of ROI prediction. We introduced extensive\nhardware-aware optimizations for TRIP and implemented the hardware-optimized\nalgorithm on the SENECA neuromorphic processor. We utilized multiple\nevent-based classification datasets for evaluation. Our approach achieves\nstate-of-the-art accuracies in all datasets and produces reasonable ROIs with\nvarying locations and sizes. On the DvsGesture dataset, our solution requires\n46x less computation than the state-of-the-art while achieving higher accuracy.\nFurthermore, TRIP enables more than 2x latency and energy improvements on the\nSENECA neuromorphic processor compared to the conventional solution.\n","authors":["Cina Arjmand","Yingfu Xu","Kevin Shidqi","Alexandra F. Dobrita","Kanishkan Vadivel","Paul Detterer","Manolis Sifalakis","Amirreza Yousefzadeh","Guangzhi Tang"],"pdf_url":"https://arxiv.org/pdf/2406.17483v1.pdf","comment":"Accepted in ICONS 2024"},{"id":"http://arxiv.org/abs/2406.17473v1","updated":"2024-06-25T11:38:46Z","published":"2024-06-25T11:38:46Z","title":"TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image\n  Classification","summary":"  The usage of medical image data for the training of large-scale machine\nlearning approaches is particularly challenging due to its scarce availability\nand the costly generation of data annotations, typically requiring the\nengagement of medical professionals. The rapid development of generative models\nallows towards tackling this problem by leveraging large amounts of realistic\nsynthetically generated data for the training process. However, randomly\nchoosing synthetic samples, might not be an optimal strategy.\n  In this work, we investigate the targeted generation of synthetic training\ndata, in order to improve the accuracy and robustness of image classification.\nTherefore, our approach aims to guide the generative model to synthesize data\nwith high epistemic uncertainty, since large measures of epistemic uncertainty\nindicate underrepresented data points in the training set. During the image\ngeneration we feed images reconstructed by an auto encoder into the classifier\nand compute the mutual information over the class-probability distribution as a\nmeasure for uncertainty.We alter the feature space of the autoencoder through\nan optimization process with the objective of maximizing the classifier\nuncertainty on the decoded image. By training on such data we improve the\nperformance and robustness against test time data augmentations and adversarial\nattacks on several classifications tasks.\n","authors":["Joshua Niemeijer","Jan Ehrhardt","Hristina Uzunova","Heinz Handels"],"pdf_url":"https://arxiv.org/pdf/2406.17473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17472v1","updated":"2024-06-25T11:30:31Z","published":"2024-06-25T11:30:31Z","title":"UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo\n  Quality Assessment","summary":"  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073\nUHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to\nexisting No-Reference (NR) IQA datasets, ours focuses on highly aesthetic\nphotos of high technical quality, filling a gap in the literature. The images,\ncarefully curated to exclude synthetic content, are sufficiently diverse to\ntrain general NR-IQA models. The dataset is annotated with perceptual quality\nratings obtained through a crowdsourcing study. Ten expert raters, comprising\nphotographers and graphics artists, assessed each image at least twice in\nmultiple sessions spanning several days, resulting in highly reliable labels.\nAnnotators were rigorously selected based on several metrics, including\nself-consistency, to ensure their reliability. The dataset includes rich\nmetadata with user and machine-generated tags from over 5,000 categories and\npopularity indicators such as favorites, likes, downloads, and views. With its\nunique characteristics, such as its focus on high-quality images, reliable\ncrowdsourced annotations, and high annotation resolution, our dataset opens up\nnew opportunities for advancing perceptual image quality assessment research\nand developing practical NR-IQA models that apply to modern photos. Our dataset\nis available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Oliver Wiedemann","Daisuke Iso"],"pdf_url":"https://arxiv.org/pdf/2406.17472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17471v1","updated":"2024-06-25T11:15:56Z","published":"2024-06-25T11:15:56Z","title":"Medical Image Segmentation Using Directional Window Attention","summary":"  Accurate segmentation of medical images is crucial for diagnostic purposes,\nincluding cell segmentation, tumor identification, and organ localization.\nTraditional convolutional neural network (CNN)-based approaches struggled to\nachieve precise segmentation results due to their limited receptive fields,\nparticularly in cases involving multi-organ segmentation with varying shapes\nand sizes. The transformer-based approaches address this limitation by\nleveraging the global receptive field, but they often face challenges in\ncapturing local information required for pixel-precise segmentation. In this\nwork, we introduce DwinFormer, a hierarchical encoder-decoder architecture for\nmedical image segmentation comprising a directional window (Dwin) attention and\nglobal self-attention (GSA) for feature encoding. The focus of our design is\nthe introduction of Dwin block within DwinFormer that effectively captures\nlocal and global information along the horizontal, vertical, and depthwise\ndirections of the input feature map by separately performing attention in each\nof these directional volumes. To this end, our Dwin block introduces a nested\nDwin attention (NDA) that progressively increases the receptive field in\nhorizontal, vertical, and depthwise directions and a convolutional Dwin\nattention (CDA) that captures local contextual information for the attention\ncomputation. While the proposed Dwin block captures local and global\ndependencies at the first two high-resolution stages of DwinFormer, the GSA\nblock encodes global dependencies at the last two lower-resolution stages.\nExperiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS\ndataset demonstrate the benefits of our DwinFormer over the state-of-the-art\napproaches. Our source code will be publicly available at\n\\url{https://github.com/Daniyanaj/DWINFORMER}.\n","authors":["Daniya Najiha Abdul Kareem","Mustansar Fiaz","Noa Novershtern","Hisham Cholakkal"],"pdf_url":"https://arxiv.org/pdf/2406.17471v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2406.17469v1","updated":"2024-06-25T11:14:09Z","published":"2024-06-25T11:14:09Z","title":"Cross-Modal Spherical Aggregation for Weakly Supervised Remote Sensing\n  Shadow Removal","summary":"  Remote sensing shadow removal, which aims to recover contaminated surface\ninformation, is tricky since shadows typically display overwhelmingly low\nillumination intensities. In contrast, the infrared image is robust toward\nsignificant light changes, providing visual clues complementary to the visible\nimage. Nevertheless, the existing methods ignore the collaboration between\nheterogeneous modalities, leading to undesired quality degradation. To fill\nthis gap, we propose a weakly supervised shadow removal network with a\nspherical feature space, dubbed S2-ShadowNet, to explore the best of both\nworlds for visible and infrared modalities. Specifically, we employ a modal\ntranslation (visible-to-infrared) model to learn the cross-domain mapping, thus\ngenerating realistic infrared samples. Then, Swin Transformer is utilized to\nextract strong representational visible/infrared features. Simultaneously, the\nextracted features are mapped to the smooth spherical manifold, which\nalleviates the domain shift through regularization. Well-designed similarity\nloss and orthogonality loss are embedded into the spherical space, prompting\nthe separation of private visible/infrared features and the alignment of shared\nvisible/infrared features through constraints on both representation content\nand orientation. Such a manner encourages implicit reciprocity between\nmodalities, thus providing a novel insight into shadow removal. Notably, ground\ntruth is not available in practice, thus S2-ShadowNet is trained by cropping\nshadow and shadow-free patches from the shadow image itself, avoiding\nstereotypical and strict pair data acquisition. More importantly, we contribute\na large-scale weakly supervised shadow removal benchmark, including 4000 shadow\nimages with corresponding shadow masks.\n","authors":["Kaichen Chi","Wei Jing","Junjie Li","Qiang Li","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17469v1.pdf","comment":"9pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.17462v1","updated":"2024-06-25T11:05:26Z","published":"2024-06-25T11:05:26Z","title":"The Tree of Diffusion Life: Evolutionary Embeddings to Understand the\n  Generation Process of Diffusion Models","summary":"  Diffusion models generate high-quality samples by corrupting data with\nGaussian noise and iteratively reconstructing it with deep learning, slowly\ntransforming noisy images into refined outputs. Understanding this data\nevolution is important for interpretability but is complex due to its\nhigh-dimensional evolutionary nature. While traditional dimensionality\nreduction methods like t-distributed stochastic neighborhood embedding (t-SNE)\naid in understanding high-dimensional spaces, they neglect evolutionary\nstructure preservation. Hence, we propose Tree of Diffusion Life (TDL), a\nmethod to understand data evolution in the generative process of diffusion\nmodels. TDL samples a diffusion model's generative space via instances with\nvarying prompts and employs image encoders to extract semantic meaning from\nthese samples, projecting them to an intermediate space. It employs a novel\nevolutionary embedding algorithm that explicitly encodes the iterations while\npreserving the high-dimensional relations, facilitating the visualization of\ndata evolution. This embedding leverages three metrics: a standard t-SNE loss\nto group semantically similar elements, a displacement loss to group elements\nfrom the same iteration step, and an instance alignment loss to align elements\nof the same instance across iterations. We present rectilinear and radial\nlayouts to represent iterations, enabling comprehensive exploration. We assess\nvarious feature extractors and highlight TDL's potential with prominent\ndiffusion models like GLIDE and Stable Diffusion with different prompt sets.\nTDL simplifies understanding data evolution within diffusion models, offering\nvaluable insights into their functioning.\n","authors":["Vidya Prasad","Hans van Gorp","Christina Humer","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2406.17462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17460v1","updated":"2024-06-25T10:56:03Z","published":"2024-06-25T10:56:03Z","title":"Investigating Self-Supervised Methods for Label-Efficient Learning","summary":"  Vision transformers combined with self-supervised learning have enabled the\ndevelopment of models which scale across large datasets for several downstream\ntasks like classification, segmentation and detection. The low-shot learning\ncapability of these models, across several low-shot downstream tasks, has been\nlargely under explored. We perform a system level study of different self\nsupervised pretext tasks, namely contrastive learning, clustering, and masked\nimage modelling for their low-shot capabilities by comparing the pretrained\nmodels. In addition we also study the effects of collapse avoidance methods,\nnamely centring, ME-MAX, sinkhorn, on these downstream tasks. Based on our\ndetailed analysis, we introduce a framework involving both mask image modelling\nand clustering as pretext tasks, which performs better across all low-shot\ndownstream tasks, including multi-class classification, multi-label\nclassification and semantic segmentation. Furthermore, when testing the model\non full scale datasets, we show performance gains in multi-class\nclassification, multi-label classification and semantic segmentation.\n","authors":["Srinivasa Rao Nandam","Sara Atito","Zhenhua Feng","Josef Kittler","Muhammad Awais"],"pdf_url":"https://arxiv.org/pdf/2406.17460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17458v1","updated":"2024-06-25T10:53:57Z","published":"2024-06-25T10:53:57Z","title":"Continuous Urban Change Detection from Satellite Image Time Series with\n  Temporal Feature Refinement and Multi-Task Integration","summary":"  Urbanization advances at unprecedented rates, resulting in negative effects\non the environment and human well-being. Remote sensing has the potential to\nmitigate these effects by supporting sustainable development strategies with\naccurate information on urban growth. Deep learning-based methods have achieved\npromising urban change detection results from optical satellite image pairs\nusing convolutional neural networks (ConvNets), transformers, and a multi-task\nlearning setup. However, transformers have not been leveraged for urban change\ndetection with multi-temporal data, i.e., >2 images, and multi-task learning\nmethods lack integration approaches that combine change and segmentation\noutputs. To fill this research gap, we propose a continuous urban change\ndetection method that identifies changes in each consecutive image pair of a\nsatellite image time series. Specifically, we propose a temporal feature\nrefinement (TFR) module that utilizes self-attention to improve ConvNet-based\nmulti-temporal building representations. Furthermore, we propose a multi-task\nintegration (MTI) module that utilizes Markov networks to find an optimal\nbuilding map time series based on segmentation and dense change outputs. The\nproposed method effectively identifies urban changes based on high-resolution\nsatellite image time series acquired by the PlanetScope constellation (F1 score\n0.551) and Gaofen-2 (F1 score 0.440). Moreover, our experiments on two\nchallenging datasets demonstrate the effectiveness of the proposed method\ncompared to bi-temporal and multi-temporal urban change detection and\nsegmentation methods.\n","authors":["Sebastian Hafner","Heng Fang","Hossein Azizpour","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2406.17458v1.pdf","comment":"Submitted to IEEE Transactions on Geoscience and Remote Sensing, Code\n  will be available at https://github.com/SebastianHafner/ContUrbanCD.git"},{"id":"http://arxiv.org/abs/2406.17450v1","updated":"2024-06-25T10:41:45Z","published":"2024-06-25T10:41:45Z","title":"Pseudo Labelling for Enhanced Masked Autoencoders","summary":"  Masked Image Modeling (MIM)-based models, such as SdAE, CAE, GreenMIM, and\nMixAE, have explored different strategies to enhance the performance of Masked\nAutoencoders (MAE) by modifying prediction, loss functions, or incorporating\nadditional architectural components. In this paper, we propose an enhanced\napproach that boosts MAE performance by integrating pseudo labelling for both\nclass and data tokens, alongside replacing the traditional pixel-level\nreconstruction with token-level reconstruction. This strategy uses cluster\nassignments as pseudo labels to promote instance-level discrimination within\nthe network, while token reconstruction requires generation of discrete tokens\nencapturing local context. The targets for pseudo labelling and reconstruction\nneeds to be generated by a teacher network. To disentangle the generation of\ntarget pseudo labels and the reconstruction of the token features, we decouple\nthe teacher into two distinct models, where one serves as a labelling teacher\nand the other as a reconstruction teacher. This separation proves empirically\nsuperior to a single teacher, while having negligible impact on throughput and\nmemory consumption. Incorporating pseudo-labelling as an auxiliary task has\ndemonstrated notable improvements in ImageNet-1K and other downstream tasks,\nincluding classification, semantic segmentation, and detection.\n","authors":["Srinivasa Rao Nandam","Sara Atito","Zhenhua Feng","Josef Kittler","Muhammad Awais"],"pdf_url":"https://arxiv.org/pdf/2406.17450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.04691v4","updated":"2024-06-25T10:41:39Z","published":"2022-05-10T06:24:09Z","title":"Probabilistic Approach for Detection of High-Frequency Periodic Signals\n  using an Event Camera","summary":"  Being inspired by the biological eye, event camera is a novel asynchronous\ntechnology that pose a paradigm shift in acquisition of visual information.\nThis paradigm enables event cameras to capture pixel-size fast motions much\nmore naturally compared to classical cameras.\n  In this paper we present a new asynchronous event-driven algorithm for\ndetection of high-frequency pixel-size periodic signals using an event camera.\nDevelopment of such new algorithms, to efficiently process the asynchronous\ninformation of event cameras, is essential and being a main challenge in the\nresearch community, in order to utilize its special properties and potential.\n  It turns out that this algorithm, that was developed in order to satisfy the\nnew paradigm, is related to an untreated theoretical problem in probability:\nlet $0\\leq\\tau_{1}\\leq\\tau_{2}\\leq\\cdots\\leq\\tau_{m}\\leq1$, originated from an\nunknown distribution. Let also $\\epsilon,\\delta\\in\\mathbb{R}$, and\n$d\\in\\mathbb{N}$. What can be said about the probability $\\Phi(m,d)$ of having\nmore than $d$ adjacent $\\tau_{i}$-s pairs that the distance between them is\n$\\delta$, up to an error $\\epsilon$ ? This problem, that reminds the area of\norder statistic, shows how the new visualization paradigm is also an\nopportunity to develop new areas and problems in mathematics.\n","authors":["David El-Chai Ben-Ezra","Ron Arad","Ayelet Padowicz","Israel Tugendhaft"],"pdf_url":"https://arxiv.org/pdf/2205.04691v4.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2406.17443v1","updated":"2024-06-25T10:23:58Z","published":"2024-06-25T10:23:58Z","title":"Using joint angles based on the international biomechanical standards\n  for human action recognition and related tasks","summary":"  Keypoint data has received a considerable amount of attention in machine\nlearning for tasks like action detection and recognition. However, human\nexperts in movement such as doctors, physiotherapists, sports scientists and\ncoaches use a notion of joint angles standardised by the International Society\nof Biomechanics to precisely and efficiently communicate static body poses and\nmovements. In this paper, we introduce the basic biomechanical notions and show\nhow they can be used to convert common keypoint data into joint angles that\nuniquely describe the given pose and have various desirable mathematical\nproperties, such as independence of both the camera viewpoint and the person\nperforming the action. We experimentally demonstrate that the joint angle\nrepresentation of keypoint data is suitable for machine learning applications\nand can in some cases bring an immediate performance gain. The use of joint\nangles as a human meaningful representation of kinematic data is in particular\npromising for applications where interpretability and dialog with human experts\nis important, such as many sports and medical applications. To facilitate\nfurther research in this direction, we will release a python package to convert\nkeypoint data into joint angles as outlined in this paper.\n","authors":["Kevin Schlegel","Lei Jiang","Hao Ni"],"pdf_url":"https://arxiv.org/pdf/2406.17443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17442v1","updated":"2024-06-25T10:23:53Z","published":"2024-06-25T10:23:53Z","title":"Mamba24/8D: Enhancing Global Interaction in Point Clouds via State Space\n  Model","summary":"  Transformers have demonstrated impressive results for 3D point cloud semantic\nsegmentation. However, the quadratic complexity of transformer makes\ncomputation cost high, limiting the number of points that can be processed\nsimultaneously and impeding the modeling of long-range dependencies. Drawing\ninspiration from the great potential of recent state space models (SSM) for\nlong sequence modeling, we introduce Mamba, a SSM-based architecture, to the\npoint cloud domain and propose Mamba24/8D, which has strong global modeling\ncapability under linear complexity. Specifically, to make disorderness of point\nclouds fit in with the causal nature of Mamba, we propose a multi-path\nserialization strategy applicable to point clouds. Besides, we propose the\nConvMamba block to compensate for the shortcomings of Mamba in modeling local\ngeometries and in unidirectional modeling. Mamba24/8D obtains state of the art\nresults on several 3D point cloud segmentation tasks, including ScanNet v2,\nScanNet200 and nuScenes, while its effectiveness is validated by extensive\nexperiments.\n","authors":["Zhuoyuan Li","Yubo Ai","Jiahao Lu","ChuXin Wang","Jiacheng Deng","Hanzhi Chang","Yanzhe Liang","Wenfei Yang","Shifeng Zhang","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17438v1","updated":"2024-06-25T10:20:44Z","published":"2024-06-25T10:20:44Z","title":"Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D\n  Images and 3D Scenes","summary":"  Neural implicit functions have demonstrated significant importance in various\nareas such as computer vision, graphics. Their advantages include the ability\nto represent complex shapes and scenes with high fidelity, smooth interpolation\ncapabilities, and continuous representations. Despite these benefits, the\ndevelopment and analysis of implicit functions have been limited by the lack of\ncomprehensive datasets and the substantial computational resources required for\ntheir implementation and evaluation. To address these challenges, we introduce\n\"Implicit-Zoo\": a large-scale dataset requiring thousands of GPU training days\ndesigned to facilitate research and development in this field. Our dataset\nincludes diverse 2D and 3D scenes, such as CIFAR-10, ImageNet-1K, and\nCityscapes for 2D image tasks, and the OmniObject3D dataset for 3D vision\ntasks. We ensure high quality through strict checks, refining or filtering out\nlow-quality data. Using Implicit-Zoo, we showcase two immediate benefits as it\nenables to: (1) learn token locations for transformer models; (2) directly\nregress 3D cameras poses of 2D images with respect to NeRF models. This in turn\nleads to an improved performance in all three task of image classification,\nsemantic segmentation, and 3D pose regression, thereby unlocking new avenues\nfor research.\n","authors":["Qi Ma","Danda Pani Paudel","Ender Konukoglu","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2406.17438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17437v1","updated":"2024-06-25T10:18:50Z","published":"2024-06-25T10:18:50Z","title":"Advancing Question Answering on Handwritten Documents: A\n  State-of-the-Art Recognition-Based Model for HW-SQuAD","summary":"  Question-answering handwritten documents is a challenging task with numerous\nreal-world applications. This paper proposes a novel recognition-based approach\nthat improves upon the previous state-of-the-art on the HW-SQuAD and BenthamQA\ndatasets. Our model incorporates transformer-based document retrieval and\nensemble methods at the model level, achieving an Exact Match score of 82.02%\nand 92.55% in HW-SQuAD and BenthamQA datasets, respectively, surpassing the\nprevious best recognition-based approach by 10.89% and 26%. We also enhance the\ndocument retrieval component, boosting the top-5 retrieval accuracy from 90% to\n95.30%. Our results demonstrate the significance of our proposed approach in\nadvancing question answering on handwritten documents. The code and trained\nmodels will be publicly available to facilitate future research in this\ncritical area of natural language.\n","authors":["Aniket Pal","Ajoy Mondal","C. V. Jawahar"],"pdf_url":"https://arxiv.org/pdf/2406.17437v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2406.17423v1","updated":"2024-06-25T09:56:30Z","published":"2024-06-25T09:56:30Z","title":"Deep learning-based brain segmentation model performance validation with\n  clinical radiotherapy CT","summary":"  Manual segmentation of medical images is labor intensive and especially\nchallenging for images with poor contrast or resolution. The presence of\ndisease exacerbates this further, increasing the need for an automated\nsolution. To this extent, SynthSeg is a robust deep learning model designed for\nautomatic brain segmentation across various contrasts and resolutions. This\nstudy validates the SynthSeg robust brain segmentation model on computed\ntomography (CT), using a multi-center dataset. An open access dataset of 260\npaired CT and magnetic resonance imaging (MRI) from radiotherapy patients\ntreated in 5 centers was collected. Brain segmentations from CT and MRI were\nobtained with SynthSeg model, a component of the Freesurfer imaging suite.\nThese segmentations were compared and evaluated using Dice scores and Hausdorff\n95 distance (HD95), treating MRI-based segmentations as the ground truth. Brain\nregions that failed to meet performance criteria were excluded based on\nautomated quality control (QC) scores. Dice scores indicate a median overlap of\n0.76 (IQR: 0.65-0.83). The median HD95 is 2.95 mm (IQR: 1.73-5.39). QC score\nbased thresholding improves median dice by 0.1 and median HD95 by 0.05mm.\nMorphological differences related to sex and age, as detected by MRI, were also\nreplicated with CT, with an approximate 17% difference between the CT and MRI\nresults for sex and 10% difference between the results for age. SynthSeg can be\nutilized for CT-based automatic brain segmentation, but only in applications\nwhere precision is not essential. CT performance is lower than MRI based on the\nintegrated QC scores, but low-quality segmentations can be excluded with\nQC-based thresholding. Additionally, performing CT-based neuroanatomical\nstudies is encouraged, as the results show correlations in sex- and age-based\nanalyses similar to those found with MRI.\n","authors":["Selena Huisman","Matteo Maspero","Marielle Philippens","Joost Verhoeff","Szabolcs David"],"pdf_url":"https://arxiv.org/pdf/2406.17423v1.pdf","comment":"15 pages, 9 figures, 3 supplementary data csv's, 1 supplementary file\n  with 1 figure"},{"id":"http://arxiv.org/abs/2406.17420v1","updated":"2024-06-25T09:45:50Z","published":"2024-06-25T09:45:50Z","title":"Real-Time Remote Control via VR over Limited Wireless Connectivity","summary":"  This work introduces a solution to enhance human-robot interaction over\nlimited wireless connectivity. The goal is toenable remote control of a robot\nthrough a virtual reality (VR)interface, ensuring a smooth transition to\nautonomous mode in the event of connectivity loss. The VR interface provides\naccessto a dynamic 3D virtual map that undergoes continuous updatesusing\nreal-time sensor data collected and transmitted by therobot. Furthermore, the\nrobot monitors wireless connectivity and automatically switches to a autonomous\nmode in scenarios with limited connectivity. By integrating four key\nfunctionalities: real-time mapping, remote control through glasses VR,\ncontinuous monitoring of wireless connectivity, and autonomous navigation\nduring limited connectivity, we achieve seamless end-to-end operation.\n","authors":["H. P. Madushanka","Rafaela Scaciota","Sumudu Samarakoon","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2406.17420v1.pdf","comment":"Accepted in ISCC 2024 conference"},{"id":"http://arxiv.org/abs/2406.17414v1","updated":"2024-06-25T09:37:09Z","published":"2024-06-25T09:37:09Z","title":"Consensus Learning with Deep Sets for Essential Matrix Estimation","summary":"  Robust estimation of the essential matrix, which encodes the relative\nposition and orientation of two cameras, is a fundamental step in structure\nfrom motion pipelines. Recent deep-based methods achieved accurate estimation\nby using complex network architectures that involve graphs, attention layers,\nand hard pruning steps. Here, we propose a simpler network architecture based\non Deep Sets. Given a collection of point matches extracted from two images,\nour method identifies outlier point matches and models the displacement noise\nin inlier matches. A weighted DLT module uses these predictions to regress the\nessential matrix. Our network achieves accurate recovery that is superior to\nexisting networks with significantly more complex architectures.\n","authors":["Dror Moran","Yuval Margalit","Guy Trostianetsky","Fadi Khatib","Meirav Galun","Ronen Basri"],"pdf_url":"https://arxiv.org/pdf/2406.17414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17413v1","updated":"2024-06-25T09:36:50Z","published":"2024-06-25T09:36:50Z","title":"Depth-Guided Semi-Supervised Instance Segmentation","summary":"  Semi-Supervised Instance Segmentation (SSIS) aims to leverage an amount of\nunlabeled data during training. Previous frameworks primarily utilized the RGB\ninformation of unlabeled images to generate pseudo-labels. However, such a\nmechanism often introduces unstable noise, as a single instance can display\nmultiple RGB values. To overcome this limitation, we introduce a Depth-Guided\n(DG) SSIS framework. This framework uses depth maps extracted from input\nimages, which represent individual instances with closely associated distance\nvalues, offering precise contours for distinct instances. Unlike RGB data,\ndepth maps provide a unique perspective, making their integration into the SSIS\nprocess complex. To this end, we propose Depth Feature Fusion, which integrates\nfeatures extracted from depth estimation. This integration allows the model to\nunderstand depth information better and ensure its effective utilization.\nAdditionally, to manage the variability of depth images during training, we\nintroduce the Depth Controller. This component enables adaptive adjustments of\nthe depth map, enhancing convergence speed and dynamically balancing the loss\nweights between RGB and depth maps. Extensive experiments conducted on the COCO\nand Cityscapes datasets validate the efficacy of our proposed method. Our\napproach establishes a new benchmark for SSIS, outperforming previous methods.\nSpecifically, our DG achieves 22.29%, 31.47%, and 35.14% mAP for 1%, 5%, and\n10% labeled data on the COCO dataset, respectively.\n","authors":["Xin Chen","Jie Hu","Xiawu Zheng","Jianghang Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.17413v1.pdf","comment":"12 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.16658v2","updated":"2024-06-25T09:36:21Z","published":"2024-06-24T14:08:27Z","title":"Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods","summary":"  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n","authors":["Remi Laumont","Yiqiu Dong","Martin Skovgaard Andersen"],"pdf_url":"https://arxiv.org/pdf/2406.16658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17405v1","updated":"2024-06-25T09:26:49Z","published":"2024-06-25T09:26:49Z","title":"Less can be more: representational vs. stereotypical gender bias in\n  facial expression recognition","summary":"  Machine learning models can inherit biases from their training data, leading\nto discriminatory or inaccurate predictions. This is particularly concerning\nwith the increasing use of large, unsupervised datasets for training\nfoundational models. Traditionally, demographic biases within these datasets\nhave not been well-understood, limiting our ability to understand how they\npropagate to the models themselves. To address this issue, this paper\ninvestigates the propagation of demographic biases from datasets into machine\nlearning models. We focus on the gender demographic component, analyzing two\ntypes of bias: representational and stereotypical. For our analysis, we\nconsider the domain of facial expression recognition (FER), a field known to\nexhibit biases in most popular datasets. We use Affectnet, one of the largest\nFER datasets, as our baseline for carefully designing and generating subsets\nthat incorporate varying strengths of both representational and stereotypical\nbias. Subsequently, we train several models on these biased subsets, evaluating\ntheir performance on a common test set to assess the propagation of bias into\nthe models' predictions. Our results show that representational bias has a\nweaker impact than expected. Models exhibit a good generalization ability even\nin the absence of one gender in the training dataset. Conversely, stereotypical\nbias has a significantly stronger impact, primarily concentrated on the biased\nclass, although it can also influence predictions for unbiased classes. These\nresults highlight the need for a bias analysis that differentiates between\ntypes of bias, which is crucial for the development of effective bias\nmitigation strategies.\n","authors":["Iris Dominguez-Catena","Daniel Paternain","Aranzazu Jurio","Mikel Galar"],"pdf_url":"https://arxiv.org/pdf/2406.17405v1.pdf","comment":"21 pages including appendix, 11 figures"},{"id":"http://arxiv.org/abs/2406.17396v1","updated":"2024-06-25T09:17:35Z","published":"2024-06-25T09:17:35Z","title":"SyncNoise: Geometrically Consistent Noise Prediction for Text-based 3D\n  Scene Editing","summary":"  Text-based 2D diffusion models have demonstrated impressive capabilities in\nimage generation and editing. Meanwhile, the 2D diffusion models also exhibit\nsubstantial potentials for 3D editing tasks. However, how to achieve consistent\nedits across multiple viewpoints remains a challenge. While the iterative\ndataset update method is capable of achieving global consistency, it suffers\nfrom slow convergence and over-smoothed textures. We propose SyncNoise, a novel\ngeometry-guided multi-view consistent noise editing approach for high-fidelity\n3D scene editing. SyncNoise synchronously edits multiple views with 2D\ndiffusion models while enforcing multi-view noise predictions to be\ngeometrically consistent, which ensures global consistency in both semantic\nstructure and low-frequency appearance. To further enhance local consistency in\nhigh-frequency details, we set a group of anchor views and propagate them to\ntheir neighboring frames through cross-view reprojection. To improve the\nreliability of multi-view correspondences, we introduce depth supervision\nduring training to enhance the reconstruction of precise geometries. Our method\nachieves high-quality 3D editing results respecting the textual instructions,\nespecially in scenes with complex textures, by enhancing geometric consistency\nat the noise and pixel levels.\n","authors":["Ruihuang Li","Liyi Chen","Zhengqiang Zhang","Varun Jampani","Vishal M. Patel","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17396v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.04364v2","updated":"2024-06-25T09:02:42Z","published":"2024-01-09T05:32:22Z","title":"SoK: Facial Deepfake Detectors","summary":"  Deepfakes have rapidly emerged as a profound and serious threat to society,\nprimarily due to their ease of creation and dissemination. This situation has\ntriggered an accelerated development of deepfake detection technologies.\nHowever, many existing detectors rely heavily on lab-generated datasets for\nvalidation, which may not effectively prepare them for novel, emerging, and\nreal-world deepfake techniques. In this paper, we conduct an extensive and\ncomprehensive review and analysis of the latest state-of-the-art deepfake\ndetectors, evaluating them against several critical criteria. These criteria\nfacilitate the categorization of these detectors into 4 high-level groups and\n13 fine-grained sub-groups, all aligned with a unified standard conceptual\nframework. This classification and framework offer deep and practical insights\ninto the factors that affect detector efficacy. We assess the generalizability\nof 16 leading detectors across various standard attack scenarios, including\nblack-box, white-box, and gray-box settings. Our systematized analysis and\nexperimentation lay the groundwork for a deeper understanding of deepfake\ndetectors and their generalizability, paving the way for future research\nfocused on creating detectors adept at countering various attack scenarios.\nAdditionally, this work offers insights for developing more proactive defenses\nagainst deepfakes.\n","authors":["Binh M. Le","Jiwon Kim","Shahroz Tariq","Kristen Moore","Alsharif Abuadbba","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2401.04364v2.pdf","comment":"18 pages, 6 figures, 5 table, under peer-review"},{"id":"http://arxiv.org/abs/2406.17382v1","updated":"2024-06-25T08:58:53Z","published":"2024-06-25T08:58:53Z","title":"Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods","summary":"  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets featuring\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition. Surprisingly, all methods except DeepLabCut and MediaPipe have\ncompetitive performance without additional finetuning, with ViTPose performing\nbest. Next to standard performance metrics (object keypoint similarity, average\nprecision and recall), we introduce errors expressed in the neck-mid-hip ratio\nand additionally study missed and redundant detections and the reliability of\nthe internal confidence ratings of the different methods, which are relevant\nfor downstream tasks. Among the networks with competitive performance, only\nAlphaPose could run close to real time (27 fps) on our machine. We provide\ndocumented Docker containers or instructions for all the methods we used, our\nanalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu\nand https://osf.io/x465b/.\n","authors":["Filipe Gama","Matej Misar","Lukas Navara","Jason Khoury","Sergiu T. Popescu","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2406.17382v1.pdf","comment":"21 pages, 3 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.17381v1","updated":"2024-06-25T08:57:47Z","published":"2024-06-25T08:57:47Z","title":"Forget but Recall: Incremental Latent Rectification in Continual\n  Learning","summary":"  Intrinsic capability to continuously learn a changing data stream is a\ndesideratum of deep neural networks (DNNs). However, current DNNs suffer from\ncatastrophic forgetting, which hinders remembering past knowledge. To mitigate\nthis issue, existing Continual Learning (CL) approaches either retain exemplars\nfor replay, regularize learning, or allocate dedicated capacity for new tasks.\nThis paper investigates an unexplored CL direction for incremental learning\ncalled Incremental Latent Rectification or ILR. In a nutshell, ILR learns to\npropagate with correction (or rectify) the representation from the current\ntrained DNN backward to the representation space of the old task, where\nperforming predictive decisions is easier. This rectification process only\nemploys a chain of small representation mapping networks, called rectifier\nunits. Empirical experiments on several continual learning benchmarks,\nincluding CIFAR10, CIFAR100, and Tiny ImageNet, demonstrate the effectiveness\nand potential of this novel CL direction compared to existing representative CL\nmethods.\n","authors":["Nghia D. Nguyen","Hieu Trung Nguyen","Ang Li","Hoang Pham","Viet Anh Nguyen","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2406.17381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17349v1","updated":"2024-06-25T08:05:42Z","published":"2024-06-25T08:05:42Z","title":"Semantic Deep Hiding for Robust Unlearnable Examples","summary":"  Ensuring data privacy and protection has become paramount in the era of deep\nlearning. Unlearnable examples are proposed to mislead the deep learning models\nand prevent data from unauthorized exploration by adding small perturbations to\ndata. However, such perturbations (e.g., noise, texture, color change)\npredominantly impact low-level features, making them vulnerable to common\ncountermeasures. In contrast, semantic images with intricate shapes have a\nwealth of high-level features, making them more resilient to countermeasures\nand potential for producing robust unlearnable examples. In this paper, we\npropose a Deep Hiding (DH) scheme that adaptively hides semantic images\nenriched with high-level features. We employ an Invertible Neural Network (INN)\nto invisibly integrate predefined images, inherently hiding them with deceptive\nperturbations. To enhance data unlearnability, we introduce a Latent Feature\nConcentration module, designed to work with the INN, regularizing the\nintra-class variance of these perturbations. To further boost the robustness of\nunlearnable examples, we design a Semantic Images Generation module that\nproduces hidden semantic images. By utilizing similar semantic information,\nthis module generates similar semantic images for samples within the same\nclasses, thereby enlarging the inter-class distance and narrowing the\nintra-class distance. Extensive experiments on CIFAR-10, CIFAR-100, and an\nImageNet subset, against 18 countermeasures, reveal that our proposed method\nexhibits outstanding robustness for unlearnable examples, demonstrating its\nefficacy in preventing unauthorized data exploitation.\n","authors":["Ruohan Meng","Chenyu Yi","Yi Yu","Siyuan Yang","Bingquan Shen","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2406.17349v1.pdf","comment":"Accepted by TIFS 2024"},{"id":"http://arxiv.org/abs/2406.17345v1","updated":"2024-06-25T07:58:47Z","published":"2024-06-25T07:58:47Z","title":"NerfBaselines: Consistent and Reproducible Evaluation of Novel View\n  Synthesis Methods","summary":"  Novel view synthesis is an important problem with many applications,\nincluding AR/VR, gaming, and simulations for robotics. With the recent rapid\ndevelopment of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)\nmethods, it is becoming difficult to keep track of the current state of the art\n(SoTA) due to methods using different evaluation protocols, codebases being\ndifficult to install and use, and methods not generalizing well to novel 3D\nscenes. Our experiments support this claim by showing that tiny differences in\nevaluation protocols of various methods can lead to inconsistent reported\nmetrics. To address these issues, we propose a framework called NerfBaselines,\nwhich simplifies the installation of various methods, provides consistent\nbenchmarking tools, and ensures reproducibility. We validate our implementation\nexperimentally by reproducing numbers reported in the original papers. To\nfurther improve the accessibility, we release a web platform where commonly\nused methods are compared on standard benchmarks. Web:\nhttps://jkulhanek.com/nerfbaselines\n","authors":["Jonas Kulhanek","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2406.17345v1.pdf","comment":"Web: https://jkulhanek.com/nerfbaselines"},{"id":"http://arxiv.org/abs/2406.17343v1","updated":"2024-06-25T07:57:27Z","published":"2024-06-25T07:57:27Z","title":"Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers","summary":"  Recent advancements in diffusion models, particularly the trend of\narchitectural transformation from UNet-based Diffusion to Diffusion Transformer\n(DiT), have significantly improved the quality and scalability of image\nsynthesis. Despite the incredible generative quality, the large computational\nrequirements of these large-scale models significantly hinder the deployments\nin real-world scenarios. Post-training Quantization (PTQ) offers a promising\nsolution by compressing model sizes and speeding up inference for the\npretrained models while eliminating model retraining. However, we have observed\nthe existing PTQ frameworks exclusively designed for both ViT and conventional\nDiffusion models fall into biased quantization and result in remarkable\nperformance degradation. In this paper, we find that the DiTs typically exhibit\nconsiderable variance in terms of both weight and activation, which easily runs\nout of the limited numerical representations. To address this issue, we devise\nQ-DiT, which seamlessly integrates three techniques: fine-grained quantization\nto manage substantial variance across input channels of weights and\nactivations, an automatic search strategy to optimize the quantization\ngranularity and mitigate redundancies, and dynamic activation quantization to\ncapture the activation changes across timesteps. Extensive experiments on the\nImageNet dataset demonstrate the effectiveness of the proposed Q-DiT.\nSpecifically, when quantizing DiT-XL/2 to W8A8 on ImageNet 256x256, Q-DiT\nachieves a remarkable reduction in FID by 1.26 compared to the baseline. Under\na W4A8 setting, it maintains high fidelity in image generation, showcasing only\na marginal increase in FID and setting a new benchmark for efficient,\nhigh-quality quantization in diffusion transformers. Code is available at\n\\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.\n","authors":["Lei Chen","Yuan Meng","Chen Tang","Xinzhu Ma","Jingyan Jiang","Xin Wang","Zhi Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17342v1","updated":"2024-06-25T07:57:03Z","published":"2024-06-25T07:57:03Z","title":"Masked Generative Extractor for Synergistic Representation and 3D\n  Generation of Point Clouds","summary":"  In the field of 2D image generation modeling and representation learning,\nMasked Generative Encoder (MAGE) has demonstrated the synergistic potential\nbetween generative modeling and representation learning. Inspired by this, we\npropose Point-MAGE to extend this concept to point cloud data. Specifically,\nthis framework first utilizes a Vector Quantized Variational Autoencoder\n(VQVAE) to reconstruct a neural field representation of 3D shapes, thereby\nlearning discrete semantic features of point patches. Subsequently, by\ncombining the masking model with variable masking ratios, we achieve\nsynchronous training for both generation and representation learning.\nFurthermore, our framework seamlessly integrates with existing point cloud\nself-supervised learning (SSL) models, thereby enhancing their performance. We\nextensively evaluate the representation learning and generation capabilities of\nPoint-MAGE. In shape classification tasks, Point-MAGE achieved an accuracy of\n94.2% on the ModelNet40 dataset and 92.9% (+1.3%) on the ScanObjectNN dataset.\nAdditionally, it achieved new state-of-the-art performance in few-shot learning\nand part segmentation tasks. Experimental results also confirmed that\nPoint-MAGE can generate detailed and high-quality 3D shapes in both\nunconditional and conditional settings.\n","authors":["Hongliang Zeng","Ping Zhang","Fang Li","Jiahua Wang","Tingyu Ye","Pengteng Guo"],"pdf_url":"https://arxiv.org/pdf/2406.17342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17338v1","updated":"2024-06-25T07:50:09Z","published":"2024-06-25T07:50:09Z","title":"Robustly Optimized Deep Feature Decoupling Network for Fatty Liver\n  Diseases Detection","summary":"  Current medical image classification efforts mainly aim for higher average\nperformance, often neglecting the balance between different classes. This can\nlead to significant differences in recognition accuracy between classes and\nobvious recognition weaknesses. Without the support of massive data, deep\nlearning faces challenges in fine-grained classification of fatty liver. In\nthis paper, we propose an innovative deep learning framework that combines\nfeature decoupling and adaptive adversarial training. Firstly, we employ two\niteratively compressed decouplers to supervised decouple common features and\nspecific features related to fatty liver in abdominal ultrasound images.\nSubsequently, the decoupled features are concatenated with the original image\nafter transforming the color space and are fed into the classifier. During\nadversarial training, we adaptively adjust the perturbation and balance the\nadversarial strength by the accuracy of each class. The model will eliminate\nrecognition weaknesses by correctly classifying adversarial samples, thus\nimproving recognition robustness. Finally, the accuracy of our method improved\nby 4.16%, achieving 82.95%. As demonstrated by extensive experiments, our\nmethod is a generalized learning framework that can be directly used to\neliminate the recognition weaknesses of any classifier while improving its\naverage performance. Code is available at https://github.com/HP-ML/MICCAI2024.\n","authors":["Peng Huang","Shu Hu","Bo Peng","Jiashu Zhang","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17338v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15770v2","updated":"2024-06-25T07:45:53Z","published":"2024-03-23T08:57:46Z","title":"Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI\n  Reconstruction","summary":"  The inductive bias of the convolutional neural network (CNN) can be a strong\nprior for image restoration, which is known as the Deep Image Prior (DIP).\nRecently, DIP is utilized in unsupervised dynamic MRI reconstruction, which\nadopts a generative model from the latent space to the image space. However,\nexisting methods usually use a pyramid-shaped CNN generator shared by all\nframes, embedding the temporal modeling within the latent space, which may\nhamper the model expression capability. In this work, we propose a novel scheme\nfor dynamic MRI representation, named ``Graph Image Prior'' (GIP). GIP adopts a\ntwo-stage generative network in a new modeling methodology, which first employs\nindependent CNNs to recover the image structure for each frame, and then\nexploits the spatio-temporal correlations within the feature space\nparameterized by a graph model. A graph convolutional network is utilized for\nfeature fusion and dynamic image generation. In addition, we devise an ADMM\nalgorithm to alternately optimize the images and the network parameters to\nimprove the reconstruction performance. Experiments were conducted on cardiac\ncine MRI reconstruction, which demonstrate that GIP outperforms compressed\nsensing methods and other DIP-based unsupervised methods, significantly\nreducing the performance gap with state-of-the-art supervised algorithms.\nMoreover, GIP displays superior generalization ability when transferred to a\ndifferent reconstruction setting, without the need for any additional data.\n","authors":["Zhongsen Li","Wenxuan Chen","Shuai Wang","Chuyu Liu","Qing Zou","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2403.15770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15485v2","updated":"2024-06-25T07:37:22Z","published":"2024-06-17T11:00:04Z","title":"SegHist: A General Segmentation-based Framework for Chinese Historical\n  Document Text Line Detection","summary":"  Text line detection is a key task in historical document analysis facing many\nchallenges of arbitrary-shaped text lines, dense texts, and text lines with\nhigh aspect ratios, etc. In this paper, we propose a general framework for\nhistorical document text detection (SegHist), enabling existing\nsegmentation-based text detection methods to effectively address the\nchallenges, especially text lines with high aspect ratios. Integrating the\nSegHist framework with the commonly used method DB++, we develop DB-SegHist.\nThis approach achieves SOTA on the CHDAC, MTHv2, and competitive results on\nHDRC datasets, with a significant improvement of 1.19% on the most challenging\nCHDAC dataset which features more text lines with high aspect ratios. Moreover,\nour method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its\nrotational robustness. The code is available at\nhttps://github.com/LumionHXJ/SegHist.\n","authors":["Xingjian Hu","Baole Wei","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2406.15485v2.pdf","comment":"Accepted by ICDAR2024"},{"id":"http://arxiv.org/abs/2406.17323v1","updated":"2024-06-25T07:14:15Z","published":"2024-06-25T07:14:15Z","title":"XAMI -- A Benchmark Dataset for Artefact Detection in XMM-Newton Optical\n  Images","summary":"  Reflected or scattered light produce artefacts in astronomical observations\nthat can negatively impact the scientific study. Hence, automated detection of\nthese artefacts is highly beneficial, especially with the increasing amounts of\ndata gathered. Machine learning methods are well-suited to this problem, but\ncurrently there is a lack of annotated data to train such approaches to detect\nartefacts in astronomical observations. In this work, we present a dataset of\nimages from the XMM-Newton space telescope Optical Monitoring camera showing\ndifferent types of artefacts. We hand-annotated a sample of 1000 images with\nartefacts which we use to train automated ML methods. We further demonstrate\ntechniques tailored for accurate detection and masking of artefacts using\ninstance segmentation. We adopt a hybrid approach, combining knowledge from\nboth convolutional neural networks (CNNs) and transformer-based models and use\ntheir advantages in segmentation. The presented method and dataset will advance\nartefact detection in astronomical observations by providing a reproducible\nbaseline. All code and data are made available\n(https://github.com/ESA-Datalabs/XAMI-model and\nhttps://github.com/ESA-Datalabs/XAMI-dataset).\n","authors":["Elisabeta-Iulia Dima","Pablo Gómez","Sandor Kruk","Peter Kretschmar","Simon Rosen","Călin-Adrian Popa"],"pdf_url":"https://arxiv.org/pdf/2406.17323v1.pdf","comment":"submitted to SPAICE 2024"},{"id":"http://arxiv.org/abs/2406.17319v1","updated":"2024-06-25T07:08:19Z","published":"2024-06-25T07:08:19Z","title":"DMF-Net: Image-Guided Point Cloud Completion with Dual-Channel Modality\n  Fusion and Shape-Aware Upsampling Transformer","summary":"  In this paper we study the task of a single-view image-guided point cloud\ncompletion. Existing methods have got promising results by fusing the\ninformation of image into point cloud explicitly or implicitly. However, given\nthat the image has global shape information and the partial point cloud has\nrich local details, We believe that both modalities need to be given equal\nattention when performing modality fusion. To this end, we propose a novel\ndual-channel modality fusion network for image-guided point cloud\ncompletion(named DMF-Net), in a coarse-to-fine manner. In the first stage,\nDMF-Net takes a partial point cloud and corresponding image as input to recover\na coarse point cloud. In the second stage, the coarse point cloud will be\nupsampled twice with shape-aware upsampling transformer to get the dense and\ncomplete point cloud. Extensive quantitative and qualitative experimental\nresults show that DMF-Net outperforms the state-of-the-art unimodal and\nmultimodal point cloud completion works on ShapeNet-ViPC dataset.\n","authors":["Aihua Mao","Yuxuan Tang","Jiangtao Huang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2406.17319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16109v2","updated":"2024-06-25T06:47:07Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v2.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2406.17309v1","updated":"2024-06-25T06:42:26Z","published":"2024-06-25T06:42:26Z","title":"Zero-Shot Long-Form Video Understanding through Screenplay","summary":"  The Long-form Video Question-Answering task requires the comprehension and\nanalysis of extended video content to respond accurately to questions by\nutilizing both temporal and contextual information. In this paper, we present\nMM-Screenplayer, an advanced video understanding system with multi-modal\nperception capabilities that can convert any video into textual screenplay\nrepresentations. Unlike previous storytelling methods, we organize video\ncontent into scenes as the basic unit, rather than just visually continuous\nshots. Additionally, we developed a ``Look Back'' strategy to reassess and\nvalidate uncertain information, particularly targeting breakpoint mode.\nMM-Screenplayer achieved highest score in the CVPR'2024 LOng-form VidEo\nUnderstanding (LOVEU) Track 1 Challenge, with a global accuracy of 87.5% and a\nbreakpoint accuracy of 68.8%.\n","authors":["Yongliang Wu","Bozheng Li","Jiawang Cao","Wenbo Zhu","Yi Lu","Weiheng Chi","Chuyun Xie","Haolin Zheng","Ziyue Su","Jay Wu","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17309v1.pdf","comment":"Highest Score Award to the CVPR'2024 LOVEU Track 1 Challenge"},{"id":"http://arxiv.org/abs/2312.00690v4","updated":"2024-06-25T06:23:56Z","published":"2023-12-01T16:17:16Z","title":"Open-vocabulary object 6D pose estimation","summary":"  We introduce the new setting of open-vocabulary object 6D pose estimation, in\nwhich a textual prompt is used to specify the object of interest. In contrast\nto existing approaches, in our setting (i) the object of interest is specified\nsolely through the textual prompt, (ii) no object model (e.g., CAD or video\nsequence) is required at inference, and (iii) the object is imaged from two\nRGBD viewpoints of different scenes. To operate in this setting, we introduce a\nnovel approach that leverages a Vision-Language Model to segment the object of\ninterest from the scenes and to estimate its relative 6D pose. The key of our\napproach is a carefully devised strategy to fuse object-level information\nprovided by the prompt with local image features, resulting in a feature space\nthat can generalize to novel concepts. We validate our approach on a new\nbenchmark based on two popular datasets, REAL275 and Toyota-Light, which\ncollectively encompass 34 object instances appearing in four thousand image\npairs. The results demonstrate that our approach outperforms both a\nwell-established hand-crafted method and a recent deep learning-based baseline\nin estimating the relative 6D pose of objects in different scenes. Code and\ndataset are available at https://jcorsetti.github.io/oryon.\n","authors":["Jaime Corsetti","Davide Boscaini","Changjae Oh","Andrea Cavallaro","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2312.00690v4.pdf","comment":"Camera ready version (CVPR 2024, poster highlight). New Oryon\n  version: arXiv:2406.16384"},{"id":"http://arxiv.org/abs/2406.17297v1","updated":"2024-06-25T05:58:34Z","published":"2024-06-25T05:58:34Z","title":"Towards Open-set Camera 3D Object Detection","summary":"  Traditional camera 3D object detectors are typically trained to recognize a\npredefined set of known object classes. In real-world scenarios, these\ndetectors may encounter unknown objects outside the training categories and\nfail to identify them correctly. To address this gap, we present OS-Det3D\n(Open-set Camera 3D Object Detection), a two-stage training framework enhancing\nthe ability of camera 3D detectors to identify both known and unknown objects.\nThe framework involves our proposed 3D Object Discovery Network (ODN3D), which\nis specifically trained using geometric cues such as the location and scale of\n3D boxes to discover general 3D objects. ODN3D is trained in a class-agnostic\nmanner, and the provided 3D object region proposals inherently come with data\nnoise. To boost accuracy in identifying unknown objects, we introduce a Joint\nObjectness Selection (JOS) module. JOS selects the pseudo ground truth for\nunknown objects from the 3D object region proposals of ODN3D by combining the\nODN3D objectness and camera feature attention objectness. Experiments on the\nnuScenes and KITTI datasets demonstrate the effectiveness of our framework in\nenabling camera 3D detectors to successfully identify unknown objects while\nalso improving their performance on known objects.\n","authors":["Zhuolin He","Xinrun Li","Heng Gao","Jiachen Tang","Shoumeng Qiu","Wenfu Wang","Lvjian Lu","Xiuchong Qiu","Xiangyang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2406.17297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10882v3","updated":"2024-06-25T05:56:40Z","published":"2024-02-16T18:36:36Z","title":"Universal Prompt Optimizer for Safe Text-to-Image Generation","summary":"  Text-to-Image (T2I) models have shown great performance in generating images\nbased on textual prompts. However, these models are vulnerable to unsafe input\nto generate unsafe content like sexual, harassment and illegal-activity images.\nExisting studies based on image checker, model fine-tuning and embedding\nblocking are impractical in real-world applications. Hence, we propose the\nfirst universal prompt optimizer for safe T2I (POSI) generation in black-box\nscenario. We first construct a dataset consisting of toxic-clean prompt pairs\nby GPT-3.5 Turbo. To guide the optimizer to have the ability of converting\ntoxic prompt to clean prompt while preserving semantic information, we design a\nnovel reward function measuring toxicity and text alignment of generated images\nand train the optimizer through Proximal Policy Optimization. Experiments show\nthat our approach can effectively reduce the likelihood of various T2I models\nin generating inappropriate images, with no significant impact on text\nalignment. It is also flexible to be combined with methods to achieve better\nperformance. Our code is available at https://github.com/wzongyu/POSI.\n","authors":["Zongyu Wu","Hongcheng Gao","Yueze Wang","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.10882v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2401.03407v5","updated":"2024-06-25T05:54:38Z","published":"2024-01-07T07:56:47Z","title":"Bilateral Reference for High-Resolution Dichotomous Image Segmentation","summary":"  We introduce a novel bilateral reference framework (BiRefNet) for\nhigh-resolution dichotomous image segmentation (DIS). It comprises two\nessential components: the localization module (LM) and the reconstruction\nmodule (RM) with our proposed bilateral reference (BiRef). The LM aids in\nobject localization using global semantic information. Within the RM, we\nutilize BiRef for the reconstruction process, where hierarchical patches of\nimages provide the source reference and gradient maps serve as the target\nreference. These components collaborate to generate the final predicted maps.\nWe also introduce auxiliary gradient supervision to enhance focus on regions\nwith finer details. Furthermore, we outline practical training strategies\ntailored for DIS to improve map quality and training process. To validate the\ngeneral applicability of our approach, we conduct extensive experiments on four\ntasks to evince that BiRefNet exhibits remarkable performance, outperforming\ntask-specific cutting-edge methods across all benchmarks. Our codes are\navailable at https://github.com/ZhengPeng7/BiRefNet.\n","authors":["Peng Zheng","Dehong Gao","Deng-Ping Fan","Li Liu","Jorma Laaksonen","Wanli Ouyang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2401.03407v5.pdf","comment":"Version 5, with updated DIS performance, accuracy-efficiency\n  comparison, and 3rd-party applications"},{"id":"http://arxiv.org/abs/2403.05005v2","updated":"2024-06-25T04:31:12Z","published":"2024-03-08T03:03:41Z","title":"DITTO: Dual and Integrated Latent Topologies for Implicit 3D\n  Reconstruction","summary":"  We propose a novel concept of dual and integrated latent topologies (DITTO in\nshort) for implicit 3D reconstruction from noisy and sparse point clouds. Most\nexisting methods predominantly focus on single latent type, such as point or\ngrid latents. In contrast, the proposed DITTO leverages both point and grid\nlatents (i.e., dual latent) to enhance their strengths, the stability of grid\nlatents and the detail-rich capability of point latents. Concretely, DITTO\nconsists of dual latent encoder and integrated implicit decoder. In the dual\nlatent encoder, a dual latent layer, which is the key module block composing\nthe encoder, refines both latents in parallel, maintaining their distinct\nshapes and enabling recursive interaction. Notably, a newly proposed dynamic\nsparse point transformer within the dual latent layer effectively refines point\nlatents. Then, the integrated implicit decoder systematically combines these\nrefined latents, achieving high-fidelity 3D reconstruction and surpassing\nprevious state-of-the-art methods on object- and scene-level datasets,\nespecially in thin and detailed structures.\n","authors":["Jaehyeok Shim","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2403.05005v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2406.17265v1","updated":"2024-06-25T04:16:14Z","published":"2024-06-25T04:16:14Z","title":"Image-Guided Outdoor LiDAR Perception Quality Assessment for Autonomous\n  Driving","summary":"  LiDAR is one of the most crucial sensors for autonomous vehicle perception.\nHowever, current LiDAR-based point cloud perception algorithms lack\ncomprehensive and rigorous LiDAR quality assessment methods, leading to\nuncertainty in detection performance. Additionally, existing point cloud\nquality assessment algorithms are predominantly designed for indoor\nenvironments or single-object scenarios. In this paper, we introduce a novel\nimage-guided point cloud quality assessment algorithm for outdoor autonomous\ndriving environments, named the Image-Guided Outdoor Point Cloud Quality\nAssessment (IGO-PQA) algorithm. Our proposed algorithm comprises two main\ncomponents. The first component is the IGO-PQA generation algorithm, which\nleverages point cloud data, corresponding RGB surrounding view images, and\nagent objects' ground truth annotations to generate an overall quality score\nfor a single-frame LiDAR-based point cloud. The second component is a\ntransformer-based IGO-PQA regression algorithm for no-reference outdoor point\ncloud quality assessment. This regression algorithm allows for the direct\nprediction of IGO-PQA scores in an online manner, without requiring image data\nand object ground truth annotations. We evaluate our proposed algorithm using\nthe nuScenes and Waymo open datasets. The IGO-PQA generation algorithm provides\nconsistent and reasonable perception quality indices. Furthermore, our proposed\nIGO-PQA regression algorithm achieves a Pearson Linear Correlation Coefficient\n(PLCC) of 0.86 on the nuScenes dataset and 0.97 on the Waymo dataset.\n","authors":["Ce Zhang","Azim Eskandarian"],"pdf_url":"https://arxiv.org/pdf/2406.17265v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.09160v2","updated":"2024-06-25T04:15:03Z","published":"2024-01-17T12:08:30Z","title":"DK-SLAM: Monocular Visual SLAM with Deep Keypoint Learning, Tracking and\n  Loop-Closing","summary":"  The performance of visual SLAM in complex, real-world scenarios is often\ncompromised by unreliable feature extraction and matching when using\nhandcrafted features. Although deep learning-based local features excel at\ncapturing high-level information and perform well on matching benchmarks, they\nstruggle with generalization in continuous motion scenes, adversely affecting\nloop detection accuracy. Our system employs a Model-Agnostic Meta-Learning\n(MAML) strategy to optimize the training of keypoint extraction networks,\nenhancing their adaptability to diverse environments. Additionally, we\nintroduce a coarse-to-fine feature tracking mechanism for learned keypoints. It\nbegins with a direct method to approximate the relative pose between\nconsecutive frames, followed by a feature matching method for refined pose\nestimation. To mitigate cumulative positioning errors, DK-SLAM incorporates a\nnovel online learning module that utilizes binary features for loop closure\ndetection. This module dynamically identifies loop nodes within a sequence,\nensuring accurate and efficient localization. Experimental evaluations on\npublicly available datasets demonstrate that DK-SLAM outperforms leading\ntraditional and learning based SLAM systems, such as ORB-SLAM3 and LIFT-SLAM.\nThese results underscore the efficacy and robustness of our DK-SLAM in varied\nand challenging real-world environments.\n","authors":["Hao Qu","Lilian Zhang","Jun Mao","Junbo Tie","Xiaofeng He","Xiaoping Hu","Yifei Shi","Changhao Chen"],"pdf_url":"https://arxiv.org/pdf/2401.09160v2.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2104.00921v3","updated":"2024-06-25T04:08:21Z","published":"2021-04-02T08:00:25Z","title":"AAformer: Auto-Aligned Transformer for Person Re-Identification","summary":"  In person re-identification (re-ID), extracting part-level features from\nperson images has been verified to be crucial to offer fine-grained\ninformation. Most of the existing CNN-based methods only locate the human parts\ncoarsely, or rely on pretrained human parsing models and fail in locating the\nidentifiable nonhuman parts (e.g., knapsack). In this article, we introduce an\nalignment scheme in transformer architecture for the first time and propose the\nauto-aligned transformer (AAformer) to automatically locate both the human\nparts and nonhuman ones at patch level. We introduce the \"Part tokens\n([PART]s)\", which are learnable vectors, to extract part features in the\ntransformer. A [PART] only interacts with a local subset of patches in\nself-attention and learns to be the part representation. To adaptively group\nthe image patches into different subsets, we design the auto-alignment.\nAuto-alignment employs a fast variant of optimal transport (OT) algorithm to\nonline cluster the patch embeddings into several groups with the [PART]s as\ntheir prototypes. AAformer integrates the part alignment into the\nself-attention and the output [PART]s can be directly used as part features for\nretrieval. Extensive experiments validate the effectiveness of [PART]s and the\nsuperiority of AAformer over various state-of-the-art methods.\n","authors":["Kuan Zhu","Haiyun Guo","Shiliang Zhang","Yaowei Wang","Jing Liu","Jinqiao Wang","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2104.00921v3.pdf","comment":"Accepted by TNNLS. IEEE Transactions on Neural Networks and Learning\n  Systems (2023)"},{"id":"http://arxiv.org/abs/2406.17256v1","updated":"2024-06-25T03:50:20Z","published":"2024-06-25T03:50:20Z","title":"Disentangled Motion Modeling for Video Frame Interpolation","summary":"  Video frame interpolation (VFI) aims to synthesize intermediate frames in\nbetween existing frames to enhance visual smoothness and quality. Beyond the\nconventional methods based on the reconstruction loss, recent works employ the\nhigh quality generative models for perceptual quality. However, they require\ncomplex training and large computational cost for modeling on the pixel space.\nIn this paper, we introduce disentangled Motion Modeling (MoMo), a\ndiffusion-based approach for VFI that enhances visual quality by focusing on\nintermediate motion modeling. We propose disentangled two-stage training\nprocess, initially training a frame synthesis model to generate frames from\ninput pairs and their optical flows. Subsequently, we propose a motion\ndiffusion model, equipped with our novel diffusion U-Net architecture designed\nfor optical flow, to produce bi-directional flows between frames. This method,\nby leveraging the simpler low-frequency representation of motions, achieves\nsuperior perceptual quality with reduced computational demands compared to\ngenerative modeling methods on the pixel space. Our method surpasses\nstate-of-the-art methods in perceptual metrics across various benchmarks,\ndemonstrating its efficacy and efficiency in VFI. Our code is available at:\nhttps://github.com/JHLew/MoMo\n","authors":["Jaihyun Lew","Jooyoung Choi","Chaehun Shin","Dahuin Jung","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2406.17256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17254v1","updated":"2024-06-25T03:42:29Z","published":"2024-06-25T03:42:29Z","title":"Scalp Diagnostic System With Label-Free Segmentation and Training-Free\n  Image Translation","summary":"  Scalp diseases and alopecia affect millions of people around the world,\nunderscoring the urgent need for early diagnosis and management of the\ndisease.However, the development of a comprehensive AI-based diagnosis system\nencompassing these conditions remains an underexplored domain due to the\nchallenges associated with data imbalance and the costly nature of labeling. To\naddress these issues, we propose ``ScalpVision\", an AI-driven system for the\nholistic diagnosis of scalp diseases and alopecia.In ScalpVision, effective\nhair segmentation is achieved using pseudo image-label pairs and an innovative\nprompting method in the absence of traditional hair masking labels. This\napproach is crucial for extracting key features such as hair thickness and\ncount, which are then used to assess alopecia severity. Additionally,\nScalpVision introduces DiffuseIT-M, a generative model adept at dataset\naugmentation while maintaining hair information, facilitating improved\npredictions of scalp disease severity. Our experimental results affirm\nScalpVision's efficiency in diagnosing a variety of scalp conditions and\nalopecia, showcasing its potential as a valuable tool in dermatological care.\n","authors":["Youngmin Kim","Saejin Kim","Hoyeon Moon","Youngjae Yu","Junhyug Noh"],"pdf_url":"https://arxiv.org/pdf/2406.17254v1.pdf","comment":"IEEE Transactions on Medical Imaging (Under Review)"},{"id":"http://arxiv.org/abs/2403.07332v2","updated":"2024-06-25T03:37:26Z","published":"2024-03-12T05:34:51Z","title":"LKM-UNet: Large Kernel Vision Mamba UNet for Medical Image Segmentation","summary":"  In clinical practice, medical image segmentation provides useful information\non the contours and dimensions of target organs or tissues, facilitating\nimproved diagnosis, analysis, and treatment. In the past few years,\nconvolutional neural networks (CNNs) and Transformers have dominated this area,\nbut they still suffer from either limited receptive fields or costly long-range\nmodeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a\npromising paradigm for long-range dependency modeling with linear complexity.\nIn this paper, we introduce a Large Kernel Vision Mamba U-shape Network, or\nLKM-UNet, for medical image segmentation. A distinguishing feature of our\nLKM-UNet is its utilization of large Mamba kernels, excelling in locally\nspatial modeling compared to small kernel-based CNNs and Transformers, while\nmaintaining superior efficiency in global modeling compared to self-attention\nwith quadratic complexity. Additionally, we design a novel hierarchical and\nbidirectional Mamba block to further enhance Mamba's global and neighborhood\nspatial modeling capability for vision inputs. Comprehensive experiments\ndemonstrate the feasibility and the effectiveness of using large-size Mamba\nkernels to achieve large receptive fields. Codes are available at\nhttps://github.com/wjh892521292/LKM-UNet.\n","authors":["Jinhong Wang","Jintai Chen","Danny Chen","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2403.07332v2.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17250v1","updated":"2024-06-25T03:34:54Z","published":"2024-06-25T03:34:54Z","title":"A benchmark for 2D foetal brain ultrasound analysis","summary":"  Brain development involves a sequence of structural changes from early stages\nof the embryo until several months after birth. Currently, ultrasound is the\nestablished technique for screening due to its ability to acquire dynamic\nimages in real-time without radiation and to its cost-efficiency. However,\nidentifying abnormalities remains challenging due to the difficulty in\ninterpreting foetal brain images. In this work we present a set of 104 2D\nfoetal brain ultrasound images acquired during the 20th week of gestation that\nhave been co-registered to a common space from a rough skull segmentation. The\nimages are provided both on the original space and template space centred on\nthe ellipses of all the subjects. Furthermore, the images have been annotated\nto highlight landmark points from structures of interest to analyse brain\ndevelopment. Both the final atlas template with probabilistic maps and the\noriginal images can be used to develop new segmentation techniques, test\nregistration approaches for foetal brain ultrasound, extend our work to\nlongitudinal datasets and to detect anomalies in new images.\n","authors":["Mariano Cabezas","Yago Diez","Clara Martinez-Diago","Anna Maroto"],"pdf_url":"https://arxiv.org/pdf/2406.17250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15222v2","updated":"2024-06-25T03:17:22Z","published":"2024-06-14T02:15:09Z","title":"Rapid and Accurate Diagnosis of Acute Aortic Syndrome using Non-contrast\n  CT: A Large-scale, Retrospective, Multi-center and AI-based Study","summary":"  Chest pain symptoms are highly prevalent in emergency departments (EDs),\nwhere acute aortic syndrome (AAS) is a catastrophic cardiovascular emergency\nwith a high fatality rate, especially when timely and accurate treatment is not\nadministered. However, current triage practices in the ED can cause up to\napproximately half of patients with AAS to have an initially missed diagnosis\nor be misdiagnosed as having other acute chest pain conditions. Subsequently,\nthese AAS patients will undergo clinically inaccurate or suboptimal\ndifferential diagnosis. Fortunately, even under these suboptimal protocols,\nnearly all these patients underwent non-contrast CT covering the aorta anatomy\nat the early stage of differential diagnosis. In this study, we developed an\nartificial intelligence model (DeepAAS) using non-contrast CT, which is highly\naccurate for identifying AAS and provides interpretable results to assist in\nclinical decision-making. Performance was assessed in two major phases: a\nmulti-center retrospective study (n = 20,750) and an exploration in real-world\nemergency scenarios (n = 137,525). In the multi-center cohort, DeepAAS achieved\na mean area under the receiver operating characteristic curve of 0.958 (95% CI\n0.950-0.967). In the real-world cohort, DeepAAS detected 109 AAS patients with\nmisguided initial suspicion, achieving 92.6% (95% CI 76.2%-97.5%) in mean\nsensitivity and 99.2% (95% CI 99.1%-99.3%) in mean specificity. Our AI model\nperformed well on non-contrast CT at all applicable early stages of\ndifferential diagnosis workflows, effectively reduced the overall missed\ndiagnosis and misdiagnosis rate from 48.8% to 4.8% and shortened the diagnosis\ntime for patients with misguided initial suspicion from an average of 681.8\n(74-11,820) mins to 68.5 (23-195) mins. DeepAAS could effectively fill the gap\nin the current clinical workflow without requiring additional tests.\n","authors":["Yujian Hu","Yilang Xiang","Yan-Jie Zhou","Yangyan He","Shifeng Yang","Xiaolong Du","Chunlan Den","Youyao Xu","Gaofeng Wang","Zhengyao Ding","Jingyong Huang","Wenjun Zhao","Xuejun Wu","Donglin Li","Qianqian Zhu","Zhenjiang Li","Chenyang Qiu","Ziheng Wu","Yunjun He","Chen Tian","Yihui Qiu","Zuodong Lin","Xiaolong Zhang","Yuan He","Zhenpeng Yuan","Xiaoxiang Zhou","Rong Fan","Ruihan Chen","Wenchao Guo","Jianpeng Zhang","Tony C. W. Mok","Zi Li","Le Lu","Dehai Lang","Xiaoqiang Li","Guofu Wang","Wei Lu","Zhengxing Huang","Minfeng Xu","Hongkun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.15222v2.pdf","comment":"under peer review"},{"id":"http://arxiv.org/abs/2406.17238v1","updated":"2024-06-25T02:59:02Z","published":"2024-06-25T02:59:02Z","title":"Expansive Synthesis: Generating Large-Scale Datasets from Minimal\n  Samples","summary":"  The challenge of limited availability of data for training in machine\nlearning arises in many applications and the impact on performance and\ngeneralization is serious. Traditional data augmentation methods aim to enhance\ntraining with a moderately sufficient data set. Generative models like\nGenerative Adversarial Networks (GANs) often face problematic convergence when\ngenerating significant and diverse data samples. Diffusion models, though\neffective, still struggle with high computational cost and long training times.\nThis paper introduces an innovative Expansive Synthesis model that generates\nlarge-scale, high-fidelity datasets from minimal samples. The proposed approach\nexploits expander graph mappings and feature interpolation to synthesize\nexpanded datasets while preserving the intrinsic data distribution and feature\nstructural relationships. The rationale of the model is rooted in the\nnon-linear property of neural networks' latent space and in its capture by a\nKoopman operator to yield a linear space of features to facilitate the\nconstruction of larger and enriched consistent datasets starting with a much\nsmaller dataset. This process is optimized by an autoencoder architecture\nenhanced with self-attention layers and further refined for distributional\nconsistency by optimal transport. We validate our Expansive Synthesis by\ntraining classifiers on the generated datasets and comparing their performance\nto classifiers trained on larger, original datasets. Experimental results\ndemonstrate that classifiers trained on synthesized data achieve performance\nmetrics on par with those trained on full-scale datasets, showcasing the\nmodel's potential to effectively augment training data. This work represents a\nsignificant advancement in data generation, offering a robust solution to data\nscarcity and paving the way for enhanced data availability in machine learning\napplications.\n","authors":["Vahid Jebraeeli","Bo Jiang","Hamid Krim","Derya Cansever"],"pdf_url":"https://arxiv.org/pdf/2406.17238v1.pdf","comment":"14 pages. arXiv admin note: text overlap with arXiv:2405.13866"},{"id":"http://arxiv.org/abs/2406.17236v1","updated":"2024-06-25T02:56:16Z","published":"2024-06-25T02:56:16Z","title":"LIPE: Learning Personalized Identity Prior for Non-rigid Image Editing","summary":"  Although recent years have witnessed significant advancements in image\nediting thanks to the remarkable progress of text-to-image diffusion models,\nthe problem of non-rigid image editing still presents its complexities and\nchallenges. Existing methods often fail to achieve consistent results due to\nthe absence of unique identity characteristics. Thus, learning a personalized\nidentity prior might help with consistency in the edited results. In this\npaper, we explore a novel task: learning the personalized identity prior for\ntext-based non-rigid image editing. To address the problems in jointly learning\nprior and editing the image, we present LIPE, a two-stage framework designed to\ncustomize the generative model utilizing a limited set of images of the same\nsubject, and subsequently employ the model with learned prior for non-rigid\nimage editing. Experimental results demonstrate the advantages of our approach\nin various editing scenarios over past related leading methods in qualitative\nand quantitative ways.\n","authors":["Aoyang Liu","Qingnan Fan","Shuai Qin","Hong Gu","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2406.17236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17235v1","updated":"2024-06-25T02:53:37Z","published":"2024-06-25T02:53:37Z","title":"Task-Agnostic Federated Learning","summary":"  In the realm of medical imaging, leveraging large-scale datasets from various\ninstitutions is crucial for developing precise deep learning models, yet\nprivacy concerns frequently impede data sharing. federated learning (FL)\nemerges as a prominent solution for preserving privacy while facilitating\ncollaborative learning. However, its application in real-world scenarios faces\nseveral obstacles, such as task & data heterogeneity, label scarcity,\nnon-identically distributed (non-IID) data, computational vaiation, etc. In\nreal-world, medical institutions may not want to disclose their tasks to FL\nserver and generalization challenge of out-of-network institutions with un-seen\ntask want to join the on-going federated system. This study address\ntask-agnostic and generalization problem on un-seen tasks by adapting\nself-supervised FL framework. Utilizing Vision Transformer (ViT) as consensus\nfeature encoder for self-supervised pre-training, no initial labels required,\nthe framework enabling effective representation learning across diverse\ndatasets and tasks. Our extensive evaluations, using various real-world non-IID\nmedical imaging datasets, validate our approach's efficacy, retaining 90\\% of\nF1 accuracy with only 5\\% of the training data typically required for\ncentralized approaches and exhibiting superior adaptability to\nout-of-distribution task. The result indicate that federated learning\narchitecture can be a potential approach toward multi-task foundation modeling.\n","authors":["Zhengtao Yao","Hong Nguyen","Ajitesh Srivastava","Jose Luis Ambite"],"pdf_url":"https://arxiv.org/pdf/2406.17235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16620v2","updated":"2024-06-25T02:43:41Z","published":"2024-06-24T13:05:39Z","title":"OmAgent: A Multi-modal Agent Framework for Complex Video Understanding\n  with Task Divide-and-Conquer","summary":"  Recent advancements in Large Language Models (LLMs) have expanded their\ncapabilities to multimodal contexts, including comprehensive video\nunderstanding. However, processing extensive videos such as 24-hour CCTV\nfootage or full-length films presents significant challenges due to the vast\ndata and processing demands. Traditional methods, like extracting key frames or\nconverting frames to text, often result in substantial information loss. To\naddress these shortcomings, we develop OmAgent, efficiently stores and\nretrieves relevant video frames for specific queries, preserving the detailed\ncontent of videos. Additionally, it features an Divide-and-Conquer Loop capable\nof autonomous reasoning, dynamically invoking APIs and tools to enhance query\nprocessing and accuracy. This approach ensures robust video understanding,\nsignificantly reducing information loss. Experimental results affirm OmAgent's\nefficacy in handling various types of videos and complex tasks. Moreover, we\nhave endowed it with greater autonomy and a robust tool-calling system,\nenabling it to accomplish even more intricate tasks.\n","authors":["Lu Zhang","Tiancheng Zhao","Heting Ying","Yibo Ma","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2406.16620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14555v2","updated":"2024-06-25T02:43:06Z","published":"2024-01-25T22:50:39Z","title":"Revisiting Active Learning in the Era of Vision Foundation Models","summary":"  Foundation vision or vision-language models are trained on large unlabeled or\nnoisy data and learn robust representations that can achieve impressive zero-\nor few-shot performance on diverse tasks. Given these properties, they are a\nnatural fit for active learning (AL), which aims to maximize labeling\nefficiency. However, the full potential of foundation models has not been\nexplored in the context of AL, specifically in the low-budget regime. In this\nwork, we evaluate how foundation models influence three critical components of\neffective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse\nsampling, and 3) the trade-off between representative and uncertainty sampling.\nWe systematically study how the robust representations of foundation models\n(DINOv2, OpenCLIP) challenge existing findings in active learning. Our\nobservations inform the principled construction of a new simple and elegant AL\nstrategy that balances uncertainty estimated via dropout with sample diversity.\nWe extensively test our strategy on many challenging image classification\nbenchmarks, including natural images as well as out-of-domain biomedical images\nthat are relatively understudied in the AL literature. We also provide a highly\nperformant and efficient implementation of modern AL strategies (including our\nmethod) at https://github.com/sanketx/AL-foundation-models.\n","authors":["Sanket Rajan Gupte","Josiah Aklilu","Jeffrey J. Nirschl","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2401.14555v2.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2404.03876v3","updated":"2024-06-25T02:20:06Z","published":"2024-04-05T03:51:19Z","title":"Accurately Classifying Out-Of-Distribution Data in Facial Recognition","summary":"  Standard classification theory assumes that the distribution of images in the\ntest and training sets are identical. Unfortunately, real-life scenarios\ntypically feature unseen data (\"out-of-distribution data\") which is different\nfrom data in the training distribution(\"in-distribution\"). This issue is most\nprevalent in social justice problems where data from under-represented groups\nmay appear in the test data without representing an equal proportion of the\ntraining data. This may result in a model returning confidently wrong decisions\nand predictions. We are interested in the following question: Can the\nperformance of a neural network improve on facial images of out-of-distribution\ndata when it is trained simultaneously on multiple datasets of in-distribution\ndata? We approach this problem by incorporating the Outlier Exposure model and\ninvestigate how the model's performance changes when other datasets of facial\nimages were implemented. We observe that the accuracy and other metrics of the\nmodel can be increased by applying Outlier Exposure, incorporating a trainable\nweight parameter to increase the machine's emphasis on outlier images, and by\nre-weighting the importance of different class labels. We also experimented\nwith whether sorting the images and determining outliers via image features\nwould have more of an effect on the metrics than sorting by average pixel\nvalue. Our goal was to make models not only more accurate but also more fair by\nscanning a more expanded range of images. We also tested the datasets in\nreverse order to see whether a more fair dataset with balanced features has an\neffect on the model's accuracy.\n","authors":["Gianluca Barone","Aashrit Cunchala","Rudy Nunez"],"pdf_url":"https://arxiv.org/pdf/2404.03876v3.pdf","comment":"18 pages, 6 tables, 6 figures"},{"id":"http://arxiv.org/abs/2406.17225v1","updated":"2024-06-25T02:18:35Z","published":"2024-06-25T02:18:35Z","title":"Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide\n  Pathological Images","summary":"  Survival prediction, utilizing pathological images and genomic profiles, is\nincreasingly important in cancer analysis and prognosis. Despite significant\nprogress, precise survival analysis still faces two main challenges: (1) The\nmassive pixels contained in whole slide images (WSIs) complicate the process of\npathological images, making it difficult to generate an effective\nrepresentation of the tumor microenvironment (TME). (2) Existing multimodal\nmethods often rely on alignment strategies to integrate complementary\ninformation, which may lead to information loss due to the inherent\nheterogeneity between pathology and genes. In this paper, we propose a\nMultimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic\ncorrelations between subtype classification and survival analysis tasks.\nSpecifically, to capture TME-related features in WSIs, we leverage the subtype\nclassification task to mine tumor regions. Simultaneously, multi-head attention\nmechanisms are applied in genomic feature extraction, adaptively performing\ngenes grouping to obtain task-related genomic embedding. With the joint\nrepresentation of pathological images and genomic data, we further introduce a\nTransport-Guided Attention (TGA) module that uses optimal transport theory to\nmodel the correlation between subtype classification and survival analysis\ntasks, effectively transferring potential information. Extensive experiments\ndemonstrate the superiority of our approaches, with MCTI outperforming\nstate-of-the-art frameworks on three public benchmarks.\n\\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.\n","authors":["Songhan Jiang","Zhengyu Gan","Linghan Cai","Yifeng Wang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17224v1","updated":"2024-06-25T02:18:15Z","published":"2024-06-25T02:18:15Z","title":"Large Language Models are Interpretable Learners","summary":"  The trade-off between expressiveness and interpretability remains a core\nchallenge when building human-centric predictive models for classification and\ndecision-making. While symbolic rules offer interpretability, they often lack\nexpressiveness, whereas neural networks excel in performance but are known for\nbeing black boxes. In this paper, we show a combination of Large Language\nModels (LLMs) and symbolic programs can bridge this gap. In the proposed\nLLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language\nprompts provides a massive set of interpretable modules that can transform raw\ninput into natural language concepts. Symbolic programs then integrate these\nmodules into an interpretable decision rule. To train LSPs, we develop a\ndivide-and-conquer approach to incrementally build the program from scratch,\nwhere the learning process of each step is guided by LLMs. To evaluate the\neffectiveness of LSPs in extracting interpretable and accurate knowledge from\ndata, we introduce IL-Bench, a collection of diverse tasks, including both\nsynthetic and real-world scenarios across different modalities. Empirical\nresults demonstrate LSP's superior performance compared to traditional\nneurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,\nas the knowledge learned by LSP is a combination of natural language\ndescriptions and symbolic rules, it is easily transferable to humans\n(interpretable), and other LLMs, and generalizes well to out-of-distribution\nsamples.\n","authors":["Ruochen Wang","Si Si","Felix Yu","Dorothea Wiesmann","Cho-Jui Hsieh","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2406.17224v1.pdf","comment":"Preliminary Version, Code at [this\n  url](https://github.com/ruocwang/llm-symbolic-program)"},{"id":"http://arxiv.org/abs/2406.16439v2","updated":"2024-06-25T02:16:47Z","published":"2024-06-24T08:30:03Z","title":"Exploring Test-Time Adaptation for Object Detection in Continually\n  Changing Environments","summary":"  For real-world applications, neural network models are commonly deployed in\ndynamic environments, where the distribution of the target domain undergoes\ntemporal changes. Continual Test-Time Adaptation (CTTA) has recently emerged as\na promising technique to gradually adapt a source-trained model to test data\ndrawn from a continually changing target domain. Despite recent advancements in\naddressing CTTA, two critical issues remain: 1) The use of a fixed threshold\nfor pseudo-labeling in existing methodologies leads to the generation of\nlow-quality pseudo-labels, as model confidence varies across categories and\ndomains; 2) While current solutions utilize stochastic parameter restoration to\nmitigate catastrophic forgetting, their capacity to preserve critical\ninformation is undermined by its intrinsic randomness. To tackle these\nchallenges, we present CTAOD, aiming to enhance the performance of detection\nmodels in CTTA scenarios. Inspired by prior CTTA works for effective\nadaptation, CTAOD is founded on the mean-teacher framework, characterized by\nthree core components. Firstly, the object-level contrastive learning module\ntailored for object detection extracts object-level features using the\nteacher's region of interest features and optimizes them through contrastive\nlearning. Secondly, the dynamic threshold strategy updates the\ncategory-specific threshold based on predicted confidence scores to improve the\nquality of pseudo-labels. Lastly, we design a data-driven stochastic\nrestoration mechanism to selectively reset inactive parameters using the\ngradients as weights for a random mask matrix, thereby ensuring the retention\nof essential knowledge. We demonstrate the effectiveness of our approach on\nfour CTTA tasks for object detection, where CTAOD outperforms existing methods,\nespecially achieving a 3.0 mAP improvement on the Cityscapes-to-Cityscapes-C\nCTTA task.\n","authors":["Shilei Cao","Yan Liu","Juepeng Zheng","Weijia Li","Runmin Dong","Haohuan Fu"],"pdf_url":"https://arxiv.org/pdf/2406.16439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09676v3","updated":"2024-06-25T02:16:41Z","published":"2023-07-18T23:06:47Z","title":"Domain Adaptation based Object Detection for Autonomous Driving in Foggy\n  and Rainy Weather","summary":"  Typically, object detection methods for autonomous driving that rely on\nsupervised learning make the assumption of a consistent feature distribution\nbetween the training and testing data, this such assumption may fail in\ndifferent weather conditions. Due to the domain gap, a detection model trained\nunder clear weather may not perform well in foggy and rainy conditions.\nOvercoming detection bottlenecks in foggy and rainy weather is a real challenge\nfor autonomous vehicles deployed in the wild. To bridge the domain gap and\nimprove the performance of object detection in foggy and rainy weather, this\npaper presents a novel framework for domain-adaptive object detection. The\nadaptations at both the image-level and object-level are intended to minimize\nthe differences in image style and object appearance between domains.\nFurthermore, in order to improve the model's performance on challenging\nexamples, we introduce a novel adversarial gradient reversal layer that\nconducts adversarial mining on difficult instances in addition to domain\nadaptation. Additionally, we suggest generating an auxiliary domain through\ndata augmentation to enforce a new domain-level metric regularization.\nExperimental findings on public V2V benchmark exhibit a substantial enhancement\nin object detection specifically for foggy and rainy driving scenarios.\n","authors":["Jinlong Li","Runsheng Xu","Xinyu Liu","Jin Ma","Baolu Li","Qin Zou","Jiaqi Ma","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2307.09676v3.pdf","comment":"the final version"},{"id":"http://arxiv.org/abs/2406.17219v1","updated":"2024-06-25T02:07:55Z","published":"2024-06-25T02:07:55Z","title":"Facial Identity Anonymization via Intrinsic and Extrinsic Attention\n  Distraction","summary":"  The unprecedented capture and application of face images raise increasing\nconcerns on anonymization to fight against privacy disclosure. Most existing\nmethods may suffer from the problem of excessive change of the\nidentity-independent information or insufficient identity protection. In this\npaper, we present a new face anonymization approach by distracting the\nintrinsic and extrinsic identity attentions. On the one hand, we anonymize the\nidentity information in the feature space by distracting the intrinsic identity\nattention. On the other, we anonymize the visual clues (i.e. appearance and\ngeometry structure) by distracting the extrinsic identity attention. Our\napproach allows for flexible and intuitive manipulation of face appearance and\ngeometry structure to produce diverse results, and it can also be used to\ninstruct users to perform personalized anonymization. We conduct extensive\nexperiments on multiple datasets and demonstrate that our approach outperforms\nstate-of-the-art methods.\n","authors":["Zhenzhong Kuang","Xiaochen Yang","Yingjie Shen","Chao Hu","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2406.17219v1.pdf","comment":"IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2024: 12406-12415"},{"id":"http://arxiv.org/abs/2406.03293v2","updated":"2024-06-25T02:04:29Z","published":"2024-06-05T14:02:31Z","title":"Text-to-Image Rectified Flow as Plug-and-Play Priors","summary":"  Large-scale diffusion models have achieved remarkable performance in\ngenerative tasks. Beyond their initial training applications, these models have\nproven their ability to function as versatile plug-and-play priors. For\ninstance, 2D diffusion models can serve as loss functions to optimize 3D\nimplicit models. Rectified flow, a novel class of generative models, enforces a\nlinear progression from the source to the target distribution and has\ndemonstrated superior performance across various domains. Compared to\ndiffusion-based methods, rectified flow approaches surpass in terms of\ngeneration quality and efficiency, requiring fewer inference steps. In this\nwork, we present theoretical and experimental evidence demonstrating that\nrectified flow based methods offer similar functionalities to diffusion models\n- they can also serve as effective priors. Besides the generative capabilities\nof diffusion priors, motivated by the unique time-symmetry properties of\nrectified flow models, a variant of our method can additionally perform image\ninversion. Experimentally, our rectified flow-based priors outperform their\ndiffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our\nmethod also displays competitive performance in image inversion and editing.\n","authors":["Xiaofeng Yang","Cheng Chen","Xulei Yang","Fayao Liu","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2406.03293v2.pdf","comment":"Added results on Stable Diffusion 3. Code:\n  https://github.com/yangxiaofeng/rectified_flow_prior"},{"id":"http://arxiv.org/abs/2406.16360v2","updated":"2024-06-25T01:19:14Z","published":"2024-06-24T07:00:57Z","title":"MIRReS: Multi-bounce Inverse Rendering using Reservoir Sampling","summary":"  We present MIRReS, a novel two-stage inverse rendering framework that jointly\nreconstructs and optimizes the explicit geometry, material, and lighting from\nmulti-view images. Unlike previous methods that rely on implicit irradiance\nfields or simplified path tracing algorithms, our method extracts an explicit\ngeometry (triangular mesh) in stage one, and introduces a more realistic\nphysically-based inverse rendering model that utilizes multi-bounce path\ntracing and Monte Carlo integration. By leveraging multi-bounce path tracing,\nour method effectively estimates indirect illumination, including\nself-shadowing and internal reflections, which improves the intrinsic\ndecomposition of shape, material, and lighting. Moreover, we incorporate\nreservoir sampling into our framework to address the noise in Monte Carlo\nintegration, enhancing convergence and facilitating gradient-based optimization\nwith low sample counts. Through qualitative and quantitative evaluation of\nseveral scenarios, especially in challenging scenarios with complex shadows, we\ndemonstrate that our method achieves state-of-the-art performance on\ndecomposition results. Additionally, our optimized explicit geometry enables\napplications such as scene editing, relighting, and material editing with\nmodern graphics engines or CAD software. The source code is available at\nhttps://brabbitdousha.github.io/MIRReS/\n","authors":["Yuxin Dai","Qi Wang","Jingsen Zhu","Dianbing Xi","Yuchi Huo","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2406.16360v2.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.15093v2","updated":"2024-06-25T01:07:15Z","published":"2024-06-21T12:14:24Z","title":"ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse\n  Diffusion Purification","summary":"  Clean-label indiscriminate poisoning attacks add invisible perturbations to\ncorrectly labeled training images, thus dramatically reducing the\ngeneralization capability of the victim models. Recently, some defense\nmechanisms have been proposed such as adversarial training, image\ntransformation techniques, and image purification. However, these schemes are\neither susceptible to adaptive attacks, built on unrealistic assumptions, or\nonly effective against specific poison types, limiting their universal\napplicability. In this research, we propose a more universally effective,\npractical, and robust defense scheme called ECLIPSE. We first investigate the\nimpact of Gaussian noise on the poisons and theoretically prove that any kind\nof poison will be largely assimilated when imposing sufficient random noise. In\nlight of this, we assume the victim has access to an extremely limited number\nof clean images (a more practical scene) and subsequently enlarge this sparse\nset for training a denoising probabilistic model (a universal denoising tool).\nWe then begin by introducing Gaussian noise to absorb the poisons and then\napply the model for denoising, resulting in a roughly purified dataset.\nFinally, to address the trade-off of the inconsistency in the assimilation\nsensitivity of different poisons by Gaussian noise, we propose a lightweight\ncorruption compensation module to effectively eliminate residual poisons,\nproviding a more universal defense approach. Extensive experiments demonstrate\nthat our defense approach outperforms 10 state-of-the-art defenses. We also\npropose an adaptive attack against ECLIPSE and verify the robustness of our\ndefense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.\n","authors":["Xianlong Wang","Shengshan Hu","Yechao Zhang","Ziqi Zhou","Leo Yu Zhang","Peng Xu","Wei Wan","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2406.15093v2.pdf","comment":"Accepted by ESORICS 2024"},{"id":"http://arxiv.org/abs/2209.11359v7","updated":"2024-06-25T23:35:36Z","published":"2022-09-23T01:09:06Z","title":"CUTS: A Deep Learning and Topological Framework for Multigranular\n  Unsupervised Medical Image Segmentation","summary":"  Segmenting medical images is critical to facilitating both patient diagnoses\nand quantitative research. A major limiting factor is the lack of labeled data,\nas obtaining expert annotations for each new set of imaging data and task can\nbe labor intensive and inconsistent among annotators. We present CUTS, an\nunsupervised deep learning framework for medical image segmentation. CUTS\noperates in two stages. For each image, it produces an embedding map via\nintra-image contrastive learning and local patch reconstruction. Then, these\nembeddings are partitioned at dynamic granularity levels that correspond to the\ndata topology. CUTS yields a series of coarse-to-fine-grained segmentations\nthat highlight features at various granularities. We applied CUTS to retinal\nfundus images and two types of brain MRI images to delineate structures and\npatterns at different scales. When evaluated against predefined anatomical\nmasks, CUTS improved the dice coefficient and Hausdorff distance by at least\n10% compared to existing unsupervised methods. Finally, CUTS showed performance\non par with Segment Anything Models (SAM, MedSAM, SAM-Med2D) pre-trained on\ngigantic labeled datasets.\n","authors":["Chen Liu","Matthew Amodio","Liangbo L. Shen","Feng Gao","Arman Avesta","Sanjay Aneja","Jay C. Wang","Lucian V. Del Priore","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2209.11359v7.pdf","comment":"Accepted to the 27th International Conference on Medical Image\n  Computing and Computer-Assisted Intervention (MICCAI 2024)"},{"id":"http://arxiv.org/abs/2406.17974v1","updated":"2024-06-25T23:11:39Z","published":"2024-06-25T23:11:39Z","title":"Evaluating Fairness in Large Vision-Language Models Across Diverse\n  Demographic Attributes and Prompts","summary":"  Large vision-language models (LVLMs) have recently achieved significant\nprogress, demonstrating strong capabilities in open-world visual understanding.\nHowever, it is not yet clear how LVLMs address demographic biases in real life,\nespecially the disparities across attributes such as gender, skin tone, and\nage. In this paper, we empirically investigate \\emph{visual fairness} in\nseveral mainstream LVLMs and audit their performance disparities across\nsensitive demographic attributes, based on public fairness benchmark datasets\n(e.g., FACET). To disclose the visual bias in LVLMs, we design a fairness\nevaluation framework with direct questions and single-choice\nquestion-instructed prompts on visual question-answering/classification tasks.\nThe zero-shot prompting results indicate that, despite enhancements in visual\nunderstanding, both open-source and closed-source LVLMs exhibit prevalent\nfairness issues across different instruct prompts and demographic attributes.\n","authors":["Xuyang Wu","Yuan Wang","Hsin-Tai Wu","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2406.17974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17970v1","updated":"2024-06-25T23:03:48Z","published":"2024-06-25T23:03:48Z","title":"Highly Constrained Coded Aperture Imaging Systems Design Via a Knowledge\n  Distillation Approach","summary":"  Computational optical imaging (COI) systems have enabled the acquisition of\nhigh-dimensional signals through optical coding elements (OCEs). OCEs encode\nthe high-dimensional signal in one or more snapshots, which are subsequently\ndecoded using computational algorithms. Currently, COI systems are optimized\nthrough an end-to-end (E2E) approach, where the OCEs are modeled as a layer of\na neural network and the remaining layers perform a specific imaging task.\nHowever, the performance of COI systems optimized through E2E is limited by the\nphysical constraints imposed by these systems. This paper proposes a knowledge\ndistillation (KD) framework for the design of highly physically constrained COI\nsystems. This approach employs the KD methodology, which consists of a\nteacher-student relationship, where a high-performance, unconstrained COI\nsystem (the teacher), guides the optimization of a physically constrained\nsystem (the student) characterized by a limited number of snapshots. We\nvalidate the proposed approach, using a binary coded apertures single pixel\ncamera for monochromatic and multispectral image reconstruction. Simulation\nresults demonstrate the superiority of the KD scheme over traditional E2E\noptimization for the designing of highly physically constrained COI systems.\n","authors":["Leon Suarez-Rodriguez","Roman Jacome","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2406.17970v1.pdf","comment":"7 pages, 3 figures. Accepted at ICIP 2024"},{"id":"http://arxiv.org/abs/2405.10244v2","updated":"2024-06-25T23:02:12Z","published":"2024-05-16T16:47:46Z","title":"Towards Task-Compatible Compressible Representations","summary":"  We identify an issue in multi-task learnable compression, in which a\nrepresentation learned for one task does not positively contribute to the\nrate-distortion performance of a different task as much as expected, given the\nestimated amount of information available in it. We interpret this issue using\nthe predictive $\\mathcal{V}$-information framework. In learnable scalable\ncoding, previous work increased the utilization of side-information for input\nreconstruction by also rewarding input reconstruction when learning this shared\nrepresentation. We evaluate the impact of this idea in the context of input\nreconstruction more rigorously and extended it to other computer vision tasks.\nWe perform experiments using representations trained for object detection on\nCOCO 2017 and depth estimation on the Cityscapes dataset, and use them to\nassist in image reconstruction and semantic segmentation tasks. The results\nshow considerable improvements in the rate-distortion performance of the\nassisted tasks. Moreover, using the proposed representations, the performance\nof the base tasks are also improved. Results suggest that the proposed method\ninduces simpler representations that are more compatible with downstream\nprocesses.\n","authors":["Anderson de Andrade","Ivan Bajić"],"pdf_url":"https://arxiv.org/pdf/2405.10244v2.pdf","comment":"To be published in ICME Workshops 2024"},{"id":"http://arxiv.org/abs/2402.19455v2","updated":"2024-06-25T22:43:54Z","published":"2024-02-29T18:50:11Z","title":"Listening to the Noise: Blind Denoising with Gibbs Diffusion","summary":"  In recent years, denoising problems have become intertwined with the\ndevelopment of deep generative models. In particular, diffusion models are\ntrained like denoisers, and the distribution they model coincide with denoising\npriors in the Bayesian picture. However, denoising through diffusion-based\nposterior sampling requires the noise level and covariance to be known,\npreventing blind denoising. We overcome this limitation by introducing Gibbs\nDiffusion (GDiff), a general methodology addressing posterior sampling of both\nthe signal and the noise parameters. Assuming arbitrary parametric Gaussian\nnoise, we develop a Gibbs algorithm that alternates sampling steps from a\nconditional diffusion model trained to map the signal prior to the family of\nnoise distributions, and a Monte Carlo sampler to infer the noise parameters.\nOur theoretical analysis highlights potential pitfalls, guides diagnostic\nusage, and quantifies errors in the Gibbs stationary distribution caused by the\ndiffusion model. We showcase our method for 1) blind denoising of natural\nimages involving colored noises with unknown amplitude and spectral index, and\n2) a cosmology problem, namely the analysis of cosmic microwave background\ndata, where Bayesian inference of \"noise\" parameters means constraining models\nof the evolution of the Universe.\n","authors":["David Heurtel-Depeiges","Charles C. Margossian","Ruben Ohana","Bruno Régaldo-Saint Blancard"],"pdf_url":"https://arxiv.org/pdf/2402.19455v2.pdf","comment":"12+9 pages, 7+5 figures, 1+1 tables; accepted to 2024 International\n  Conference on Machine Learning; code:\n  https://github.com/rubenohana/Gibbs-Diffusion"},{"id":"http://arxiv.org/abs/2311.16711v2","updated":"2024-06-25T22:33:57Z","published":"2023-11-28T11:45:35Z","title":"LEDITS++: Limitless Image Editing using Text-to-Image Models","summary":"  Text-to-image diffusion models have recently received increasing interest for\ntheir astonishing ability to produce high-fidelity images from solely text\ninputs. Subsequent research efforts aim to exploit and apply their capabilities\nto real image editing. However, existing image-to-image methods are often\ninefficient, imprecise, and of limited versatility. They either require\ntime-consuming finetuning, deviate unnecessarily strongly from the input image,\nand/or lack support for multiple, simultaneous edits. To address these issues,\nwe introduce LEDITS++, an efficient yet versatile and precise textual image\nmanipulation technique. LEDITS++'s novel inversion approach requires no tuning\nnor optimization and produces high-fidelity results with a few diffusion steps.\nSecond, our methodology supports multiple simultaneous edits and is\narchitecture-agnostic. Third, we use a novel implicit masking technique that\nlimits changes to relevant image regions. We propose the novel TEdBench++\nbenchmark as part of our exhaustive evaluation. Our results demonstrate the\ncapabilities of LEDITS++ and its improvements over previous methods.\n","authors":["Manuel Brack","Felix Friedrich","Katharina Kornmeier","Linoy Tsaban","Patrick Schramowski","Kristian Kersting","Apolinário Passos"],"pdf_url":"https://arxiv.org/pdf/2311.16711v2.pdf","comment":"Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) The project page is available at\n  https://leditsplusplus-project.static.hf.space"},{"id":"http://arxiv.org/abs/2406.17960v1","updated":"2024-06-25T22:33:41Z","published":"2024-06-25T22:33:41Z","title":"MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for\n  Effective-and-Efficient Vision-and-Language Navigation","summary":"  Despite the remarkable developments of recent large models in Embodied\nArtificial Intelligence (E-AI), their integration into robotics is hampered by\ntheir excessive parameter sizes and computational demands. Towards the\nVision-and-Language Navigation (VLN) task, a core task in E-AI, this paper\nreveals the great potential of using knowledge distillation for obtaining\nlightweight student models by proposing a Meta-Ability Guided Interactive\nChain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge\nDistillation (MAKD) framework is proposed for decoupling and refining the\nnecessary meta-abilities of VLN agents. A Meta-Knowledge Randomization\nWeighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module\nare incorporated to dynamically adjust aggregation weights at the meta-ability\nand sample levels, respectively. Move beyond the traditional one-step\nunidirectional distillation, an Interactive Chain-of-Distillation (ICoD)\nlearning strategy is proposed to allow students to give feedback to teachers,\nforming a new multi-step teacher-student co-evolution pipeline. Remarkably, on\nthe R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only\n5% (11M) of the teacher's size, outperforms all previous methods under the same\ntraining data. Additionally, our largest model, MAGIC-L, surpasses the previous\nstate-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset\nwas collected and annotated from our living environments, where MAGIC-S\ndemonstrated superior performance and real-time efficiency. Our code is\npublicly available on https://github.com/CrystalSixone/VLN-MAGIC.\n","authors":["Liuyi Wang","Zongtao He","Mengjiao Shen","Jingwei Yang","Chengju Liu","Qijun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02416v3","updated":"2024-06-25T22:21:17Z","published":"2024-01-04T18:59:25Z","title":"ODIN: A Single Model for 2D and 3D Segmentation","summary":"  State-of-the-art models on contemporary 3D segmentation benchmarks like\nScanNet consume and label dataset-provided 3D point clouds, obtained through\npost processing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website (https://odin-seg.github.io).\n","authors":["Ayush Jain","Pushkal Katara","Nikolaos Gkanatsios","Adam W. Harley","Gabriel Sarch","Kriti Aggarwal","Vishrav Chaudhary","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2401.02416v3.pdf","comment":"Camera Ready (CVPR 2024, Highlight)"},{"id":"http://arxiv.org/abs/2406.17936v1","updated":"2024-06-25T20:56:41Z","published":"2024-06-25T20:56:41Z","title":"Hot-Distance: Combining One-Hot and Signed Distance Embeddings for\n  Segmentation","summary":"  Machine learning models are only as good as the data to which they are fit.\nAs such, it is always preferable to use as much data as possible in training\nmodels. What data can be used for fitting a model depends a lot on the\nformulation of the task. We introduce Hot-Distance, a novel segmentation target\nthat incorporates the strength of signed boundary distance prediction with the\nflexibility of one-hot encoding, to increase the amount of usable training data\nfor segmentation of subcellular structures in focused ion beam scanning\nelectron microscopy (FIB-SEM).\n","authors":["Marwan Zouinkhi","Jeff L. Rhoades","Aubrey V. Weigel"],"pdf_url":"https://arxiv.org/pdf/2406.17936v1.pdf","comment":"3 pages, 1 figure, in progress"},{"id":"http://arxiv.org/abs/2406.17915v1","updated":"2024-06-25T19:56:12Z","published":"2024-06-25T19:56:12Z","title":"Semi-supervised classification of dental conditions in panoramic\n  radiographs using large language model and instance segmentation: A\n  real-world dataset evaluation","summary":"  Dental panoramic radiographs offer vast diagnostic opportunities, but\ntraining supervised deep learning networks for automatic analysis of those\nradiology images is hampered by a shortage of labeled data. Here, a different\nperspective on this problem is introduced. A semi-supervised learning framework\nis proposed to classify thirteen dental conditions on panoramic radiographs,\nwith a particular emphasis on teeth. Large language models were explored to\nannotate the most common dental conditions based on dental reports.\nAdditionally, a masked autoencoder was employed to pre-train the classification\nneural network, and a Vision Transformer was used to leverage the unlabeled\ndata. The analyses were validated using two of the most extensive datasets in\nthe literature, comprising 8,795 panoramic radiographs and 8,029 paired reports\nand images. Encouragingly, the results consistently met or surpassed the\nbaseline metrics for the Matthews correlation coefficient. A comparison of the\nproposed solution with human practitioners, supported by statistical analysis,\nhighlighted its effectiveness and performance limitations; based on the degree\nof agreement among specialists, the solution demonstrated an accuracy level\ncomparable to that of a junior specialist.\n","authors":["Bernardo Silva","Jefferson Fontinele","Carolina Letícia Zilli Vieira","João Manuel R. S. Tavares","Patricia Ramos Cury","Luciano Oliveira"],"pdf_url":"https://arxiv.org/pdf/2406.17915v1.pdf","comment":"43 pages, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2406.17908v1","updated":"2024-06-25T19:43:49Z","published":"2024-06-25T19:43:49Z","title":"DeepSense-V2V: A Vehicle-to-Vehicle Multi-Modal Sensing, Localization,\n  and Communications Dataset","summary":"  High data rate and low-latency vehicle-to-vehicle (V2V) communication are\nessential for future intelligent transport systems to enable coordination,\nenhance safety, and support distributed computing and intelligence\nrequirements. Developing effective communication strategies, however, demands\nrealistic test scenarios and datasets. This is important at the high-frequency\nbands where more spectrum is available, yet harvesting this bandwidth is\nchallenged by the need for direction transmission and the sensitivity of signal\npropagation to blockages. This work presents the first large-scale multi-modal\ndataset for studying mmWave vehicle-to-vehicle communications. It presents a\ntwo-vehicle testbed that comprises data from a 360-degree camera, four radars,\nfour 60 GHz phased arrays, a 3D lidar, and two precise GPSs. The dataset\ncontains vehicles driving during the day and night for 120 km in intercity and\nrural settings, with speeds up to 100 km per hour. More than one million\nobjects were detected across all images, from trucks to bicycles. This work\nfurther includes detailed dataset statistics that prove the coverage of various\nsituations and highlights how this dataset can enable novel machine-learning\napplications.\n","authors":["Joao Morais","Gouranga Charan","Nikhil Srinivas","Ahmed Alkhateeb"],"pdf_url":"https://arxiv.org/pdf/2406.17908v1.pdf","comment":"14 pages, 15 figures, 2 tables. The dataset is available on the\n  DeepSense6G website: https://deepsense6g.net/"},{"id":"http://arxiv.org/abs/2405.16343v2","updated":"2024-06-25T19:35:10Z","published":"2024-05-25T20:00:27Z","title":"Learning Point Spread Function Invertibility Assessment for Image\n  Deconvolution","summary":"  Deep-learning (DL)-based image deconvolution (ID) has exhibited remarkable\nrecovery performance, surpassing traditional linear methods. However, unlike\ntraditional ID approaches that rely on analytical properties of the point\nspread function (PSF) to achieve high recovery performance - such as specific\nspectrum properties or small conditional numbers in the convolution matrix - DL\ntechniques lack quantifiable metrics for evaluating PSF suitability for\nDL-assisted recovery. Aiming to enhance deconvolution quality, we propose a\nmetric that employs a non-linear approach to learn the invertibility of an\narbitrary PSF using a neural network by mapping it to a unit impulse. A lower\ndiscrepancy between the mapped PSF and a unit impulse indicates a higher\nlikelihood of successful inversion by a DL network. Our findings reveal that\nthis metric correlates with high recovery performance in DL and traditional\nmethods, thereby serving as an effective regularizer in deconvolution tasks.\nThis approach reduces the computational complexity over conventional condition\nnumber assessments and is a differentiable process. These useful properties\nallow its application in designing diffractive optical elements through\nend-to-end (E2E) optimization, achieving invertible PSFs, and outperforming the\nE2E baseline framework.\n","authors":["Romario Gualdrón-Hurtado","Roman Jacome","Sergio Urrea","Henry Arguello","Luis Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2405.16343v2.pdf","comment":"Accepted at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.17902v1","updated":"2024-06-25T19:26:39Z","published":"2024-06-25T19:26:39Z","title":"Domain Adaptation of Echocardiography Segmentation Via Reinforcement\n  Learning","summary":"  Performance of deep learning segmentation models is significantly challenged\nin its transferability across different medical imaging domains, particularly\nwhen aiming to adapt these models to a target domain with insufficient\nannotated data for effective fine-tuning. While existing domain adaptation (DA)\nmethods propose strategies to alleviate this problem, these methods do not\nexplicitly incorporate human-verified segmentation priors, compromising the\npotential of a model to produce anatomically plausible segmentations. We\nintroduce RL4Seg, an innovative reinforcement learning framework that reduces\nthe need to otherwise incorporate large expertly annotated datasets in the\ntarget domain, and eliminates the need for lengthy manual human review. Using a\ntarget dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not\nonly outperforms existing state-of-the-art DA methods in accuracy but also\nachieves 99% anatomical validity on a subset of 220 expert-validated subjects\nfrom the target domain. Furthermore, our framework's reward network offers\nuncertainty estimates comparable with dedicated state-of-the-art uncertainty\nmethods, demonstrating the utility and effectiveness of RL4Seg in overcoming\ndomain adaptation challenges in medical image segmentation.\n","authors":["Arnaud Judge","Thierry Judge","Nicolas Duchateau","Roman A. Sandler","Joseph Z. Sokol","Olivier Bernard","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2406.17902v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.17899v1","updated":"2024-06-25T19:20:10Z","published":"2024-06-25T19:20:10Z","title":"Entity Augmentation for Efficient Classification of Vertically\n  Partitioned Data with Limited Overlap","summary":"  Vertical Federated Learning (VFL) is a machine learning paradigm for learning\nfrom vertically partitioned data (i.e. features for each input are distributed\nacross multiple \"guest\" clients and an aggregating \"host\" server owns labels)\nwithout communicating raw data. Traditionally, VFL involves an \"entity\nresolution\" phase where the host identifies and serializes the unique entities\nknown to all guests. This is followed by private set intersection to find\ncommon entities, and an \"entity alignment\" step to ensure all guests are always\nprocessing the same entity's data. However, using only data of entities from\nthe intersection means guests discard potentially useful data. Besides, the\neffect on privacy is dubious and these operations are computationally\nexpensive. We propose a novel approach that eliminates the need for set\nintersection and entity alignment in categorical tasks. Our Entity Augmentation\ntechnique generates meaningful labels for activations sent to the host,\nregardless of their originating entity, enabling efficient VFL without explicit\nentity alignment. With limited overlap between training data, this approach\nperforms substantially better (e.g. with 5% overlap, 48.1% vs 69.48% test\naccuracy on CIFAR-10). In fact, thanks to the regularizing effect, our model\nperforms marginally better even with 100% overlap.\n","authors":["Avi Amalanshu","Viswesh Nagaswamy","G. V. S. S. Prudhvi","Yash Sirvi","Debashish Chakravarty"],"pdf_url":"https://arxiv.org/pdf/2406.17899v1.pdf","comment":"GLOW @ IJCAI 2024 (12 pages + 2 page bibliography. 15 figures.)"},{"id":"http://arxiv.org/abs/2405.15636v2","updated":"2024-06-25T19:05:11Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18451v3","updated":"2024-06-25T19:04:56Z","published":"2024-02-28T16:24:08Z","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation","summary":"  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17659v2","updated":"2024-06-25T19:01:09Z","published":"2024-05-27T21:04:43Z","title":"Enhancing Global Sensitivity and Uncertainty Quantification in Medical\n  Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba","summary":"  Deep learning has been extensively applied in medical image reconstruction,\nwhere Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)\nrepresent the predominant paradigms, each possessing distinct advantages and\ninherent limitations: CNNs exhibit linear complexity with local sensitivity,\nwhereas ViTs demonstrate quadratic complexity with global sensitivity. The\nemerging Mamba has shown superiority in learning visual representation, which\ncombines the advantages of linear scalability and global sensitivity. In this\nstudy, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with\nwavelet decomposition for joint medical image reconstruction and uncertainty\nestimation. A novel Arbitrary Scan Masking (ASM) mechanism \"masks out\"\nredundant information to introduce randomness for further uncertainty\nestimation. Compared to the commonly used Monte Carlo (MC) dropout, our\nproposed MC-ASM provides an uncertainty map without the need for hyperparameter\ntuning and mitigates the performance drop typically observed when applying\ndropout to low-level tasks. For further texture preservation and better\nperceptual quality, we employ the wavelet transformation into MambaMIR and\nexplore its variant based on the Generative Adversarial Network, namely\nMambaMIR-GAN. Comprehensive experiments have been conducted for multiple\nrepresentative medical image reconstruction tasks, demonstrating that the\nproposed MambaMIR and MambaMIR-GAN outperform other baseline and\nstate-of-the-art methods in different reconstruction tasks, where MambaMIR\nachieves the best reconstruction fidelity and MambaMIR-GAN has the best\nperceptual quality. In addition, our MC-ASM provides uncertainty maps as an\nadditional tool for clinicians, while mitigating the typical performance drop\ncaused by the commonly used dropout.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Weiwen Wu","Chengyan Wang","Kuangyu Shi","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17880v1","updated":"2024-06-25T18:39:43Z","published":"2024-06-25T18:39:43Z","title":"MLLM as Video Narrator: Mitigating Modality Imbalance in Video Moment\n  Retrieval","summary":"  Video Moment Retrieval (VMR) aims to localize a specific temporal segment\nwithin an untrimmed long video given a natural language query. Existing methods\noften suffer from inadequate training annotations, i.e., the sentence typically\nmatches with a fraction of the prominent video content in the foreground with\nlimited wording diversity. This intrinsic modality imbalance leaves a\nconsiderable portion of visual information remaining unaligned with text. It\nconfines the cross-modal alignment knowledge within the scope of a limited text\ncorpus, thereby leading to sub-optimal visual-textual modeling and poor\ngeneralizability. By leveraging the visual-textual understanding capability of\nmulti-modal large language models (MLLM), in this work, we take an MLLM as a\nvideo narrator to generate plausible textual descriptions of the video, thereby\nmitigating the modality imbalance and boosting the temporal localization. To\neffectively maintain temporal sensibility for localization, we design to get\ntext narratives for each certain video timestamp and construct a structured\ntext paragraph with time information, which is temporally aligned with the\nvisual content. Then we perform cross-modal feature merging between the\ntemporal-aware narratives and corresponding video temporal features to produce\nsemantic-enhanced video representation sequences for query localization.\nSubsequently, we introduce a uni-modal narrative-query matching mechanism,\nwhich encourages the model to extract complementary information from contextual\ncohesive descriptions for improved retrieval. Extensive experiments on two\nbenchmarks show the effectiveness and generalizability of our proposed method.\n","authors":["Weitong Cai","Jiabo Huang","Shaogang Gong","Hailin Jin","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17880v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.17876v1","updated":"2024-06-25T18:35:13Z","published":"2024-06-25T18:35:13Z","title":"ET tu, CLIP? Addressing Common Object Errors for Unseen Environments","summary":"  We introduce a simple method that employs pre-trained CLIP encoders to\nenhance model generalization in the ALFRED task. In contrast to previous\nliterature where CLIP replaces the visual encoder, we suggest using CLIP as an\nadditional module through an auxiliary object detection objective. We validate\nour method on the recently proposed Episodic Transformer architecture and\ndemonstrate that incorporating CLIP improves task performance on the unseen\nvalidation set. Additionally, our analysis results support that CLIP especially\nhelps with leveraging object descriptions, detecting small objects, and\ninterpreting rare words.\n","authors":["Ye Won Byun","Cathy Jiao","Shahriar Noroozizadeh","Jimin Sun","Rosa Vitiello"],"pdf_url":"https://arxiv.org/pdf/2406.17876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15104v2","updated":"2024-06-25T18:21:17Z","published":"2024-06-21T12:45:07Z","title":"Deciphering the Definition of Adversarial Robustness for post-hoc OOD\n  Detectors","summary":"  Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in real-world scenarios. In recent years, many OOD\ndetectors have been developed, and even the benchmarking has been standardized,\ni.e. OpenOOD. The number of post-hoc detectors is growing fast and showing an\noption to protect a pre-trained classifier against natural distribution shifts,\nclaiming to be ready for real-world scenarios. However, its efficacy in\nhandling adversarial examples has been neglected in the majority of studies.\nThis paper investigates the adversarial robustness of the 16 post-hoc detectors\non several evasion attacks and discuss a roadmap towards adversarial defense in\nOOD detectors.\n","authors":["Peter Lorenz","Mario Fernandez","Jens Müller","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2406.15104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16549v3","updated":"2024-06-25T18:20:40Z","published":"2024-01-29T20:37:03Z","title":"Deep Learning for Multi-Label Learning: A Comprehensive Survey","summary":"  Multi-label learning is a rapidly growing research area that aims to predict\nmultiple labels from a single input data point. In the era of big data, tasks\ninvolving multi-label classification (MLC) or ranking present significant and\nintricate challenges, capturing considerable attention in diverse domains.\nInherent difficulties in MLC include dealing with high-dimensional data,\naddressing label correlations, and handling partial labels, for which\nconventional methods prove ineffective. Recent years have witnessed a notable\nincrease in adopting deep learning (DL) techniques to address these challenges\nmore effectively in MLC. Notably, there is a burgeoning effort to harness the\nrobust learning capabilities of DL for improved modelling of label dependencies\nand other challenges in MLC. However, it is noteworthy that comprehensive\nstudies specifically dedicated to DL for multi-label learning are limited.\nThus, this survey aims to thoroughly review recent progress in DL for\nmulti-label learning, along with a summary of open research problems in MLC.\nThe review consolidates existing research efforts in DL for MLC,including deep\nneural networks, transformers, autoencoders, and convolutional and recurrent\narchitectures. Finally, the study presents a comparative analysis of the\nexisting methods to provide insightful observations and stimulate future\nresearch directions in this domain.\n","authors":["Adane Nega Tarekegn","Mohib Ullah","Faouzi Alaya Cheikh"],"pdf_url":"https://arxiv.org/pdf/2401.16549v3.pdf","comment":"20 pages, 5 tables"},{"id":"http://arxiv.org/abs/2406.17869v1","updated":"2024-06-25T18:16:25Z","published":"2024-06-25T18:16:25Z","title":"Burst Image Super-Resolution with Base Frame Selection","summary":"  Burst image super-resolution has been a topic of active research in recent\nyears due to its ability to obtain a high-resolution image by using\ncomplementary information between multiple frames in the burst. In this work,\nwe explore using burst shots with non-uniform exposures to confront real-world\npractical scenarios by introducing a new benchmark dataset, dubbed\nNon-uniformly Exposed Burst Image (NEBI), that includes the burst frames at\nvarying exposure times to obtain a broader range of irradiance and motion\ncharacteristics within a scene. As burst shots with non-uniform exposures\nexhibit varying levels of degradation, fusing information of the burst shots\ninto the first frame as a base frame may not result in optimal image quality.\nTo address this limitation, we propose a Frame Selection Network (FSN) for\nnon-uniform scenarios. This network seamlessly integrates into existing\nsuper-resolution methods in a plug-and-play manner with low computational\ncosts. The comparative analysis reveals the effectiveness of the nonuniform\nsetting for the practical scenario and our FSN on synthetic-/real- NEBI\ndatasets.\n","authors":["Sanghyun Kim","Min Jung Lee","Woohyeok Kim","Deunsol Jung","Jaesung Rim","Sunghyun Cho","Minsu Cho"],"pdf_url":"https://arxiv.org/pdf/2406.17869v1.pdf","comment":"CVPR2024W NTIRE accepted"},{"id":"http://arxiv.org/abs/2406.01274v2","updated":"2024-06-25T18:10:15Z","published":"2024-06-03T12:40:30Z","title":"Expected Grad-CAM: Towards gradient faithfulness","summary":"  Although input-gradients techniques have evolved to mitigate and tackle the\nchallenges associated with gradients, modern gradient-weighted CAM approaches\nstill rely on vanilla gradients, which are inherently susceptible to the\nsaturation phenomena. Despite recent enhancements have incorporated\ncounterfactual gradient strategies as a mitigating measure, these local\nexplanation techniques still exhibit a lack of sensitivity to their baseline\nparameter. Our work proposes a gradient-weighted CAM augmentation that tackles\nboth the saturation and sensitivity problem by reshaping the gradient\ncomputation, incorporating two well-established and provably approaches:\nExpected Gradients and kernel smoothing. By revisiting the original formulation\nas the smoothed expectation of the perturbed integrated gradients, one can\nconcurrently construct more faithful, localized and robust explanations which\nminimize infidelity. Through fine modulation of the perturbation distribution\nit is possible to regulate the complexity characteristic of the explanation,\nselectively discriminating stable features. Our technique, Expected Grad-CAM,\ndifferently from recent works, exclusively optimizes the gradient computation,\npurposefully designed as an enhanced substitute of the foundational Grad-CAM\nalgorithm and any method built therefrom. Quantitative and qualitative\nevaluations have been conducted to assess the effectiveness of our method.\n","authors":["Vincenzo Buono","Peyman Sheikholharam Mashhadi","Mahmoud Rahat","Prayag Tiwari","Stefan Byttner"],"pdf_url":"https://arxiv.org/pdf/2406.01274v2.pdf","comment":"Updated appendix figures to vector format for improved clarity"},{"id":"http://arxiv.org/abs/2406.17858v1","updated":"2024-06-25T18:02:11Z","published":"2024-06-25T18:02:11Z","title":"Depth-Driven Geometric Prompt Learning for Laparoscopic Liver Landmark\n  Detection","summary":"  Laparoscopic liver surgery poses a complex intraoperative dynamic environment\nfor surgeons, where remains a significant challenge to distinguish critical or\neven hidden structures inside the liver. Liver anatomical landmarks, e.g.,\nridge and ligament, serve as important markers for 2D-3D alignment, which can\nsignificantly enhance the spatial perception of surgeons for precise surgery.\nTo facilitate the detection of laparoscopic liver landmarks, we collect a novel\ndataset called L3D, which comprises 1,152 frames with elaborated landmark\nannotations from surgical videos of 39 patients across two medical sites. For\nbenchmarking purposes, 12 mainstream detection methods are selected and\ncomprehensively evaluated on L3D. Further, we propose a depth-driven geometric\nprompt learning network, namely D2GPLand. Specifically, we design a Depth-aware\nPrompt Embedding (DPE) module that is guided by self-supervised prompts and\ngenerates semantically relevant geometric information with the benefit of\nglobal depth cues extracted from SAM-based features. Additionally, a\nSemantic-specific Geometric Augmentation (SGA) scheme is introduced to\nefficiently merge RGB-D spatial and geometric information through reverse\nanatomic perception. The experimental results indicate that D2GPLand obtains\nstate-of-the-art performance on L3D, with 63.52% DICE and 48.68% IoU scores.\nTogether with 2D-3D fusion technology, our method can directly provide the\nsurgeon with intuitive guidance information in laparoscopic scenarios.\n","authors":["Jialun Pei","Ruize Cui","Yaoqian Li","Weixin Si","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.17858v1.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17840v1","updated":"2024-06-25T17:46:28Z","published":"2024-06-25T17:46:28Z","title":"Human-Object Interaction from Human-Level Instructions","summary":"  Intelligent agents need to autonomously navigate and interact within\ncontextual environments to perform a wide range of daily tasks based on\nhuman-level instructions. These agents require a foundational understanding of\nthe world, incorporating common sense and knowledge, to interpret such\ninstructions. Moreover, they must possess precise low-level skills for movement\nand interaction to execute the detailed task plans derived from these\ninstructions. In this work, we address the task of synthesizing continuous\nhuman-object interactions for manipulating large objects within contextual\nenvironments, guided by human-level instructions. Our goal is to generate\nsynchronized object motion, full-body human motion, and detailed finger motion,\nall essential for realistic interactions. Our framework consists of a large\nlanguage model (LLM) planning module and a low-level motion generator. We use\nLLMs to deduce spatial object relationships and devise a method for accurately\ndetermining their positions and orientations in target scene layouts.\nAdditionally, the LLM planner outlines a detailed task plan specifying a\nsequence of sub-tasks. This task plan, along with the target object poses,\nserves as input for our low-level motion generator, which seamlessly alternates\nbetween navigation and interaction modules. We present the first complete\nsystem that can synthesize object motion, full-body motion, and finger motion\nsimultaneously from human-level instructions. Our experiments demonstrate the\neffectiveness of our high-level planner in generating plausible target layouts\nand our low-level motion generator in synthesizing realistic interactions for\ndiverse objects. Please refer to our project page for more results:\nhttps://hoifhli.github.io/.\n","authors":["Zhen Wu","Jiaman Li","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17840v1.pdf","comment":"10 pages"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2406.17650v1","updated":"2024-06-25T15:41:40Z","published":"2024-06-25T15:41:40Z","title":"ELIZA Reinterpreted: The world's first chatbot was not intended as a\n  chatbot at all","summary":"  ELIZA, often considered the world's first chatbot, was written by Joseph\nWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,\nbut rather to build a platform for research into human-machine conversation and\nthe important cognitive processes of interpretation and misinterpretation. His\npurpose was obscured by ELIZA's fame, resulting in large part from the\nfortuitous timing of it's creation, and it's escape into the wild. In this\npaper I provide a rich historical context for ELIZA's creation, demonstrating\nthat ELIZA arose from the intersection of some of the central threads in the\ntechnical history of AI. I also briefly discuss how ELIZA escaped into the\nworld, and how its accidental escape, along with several coincidental turns of\nthe programming language screws, led both to the misapprehension that ELIZA was\nintended as a chatbot, and to the loss of the original ELIZA to history for\nover 50 years.\n","authors":["Jeff Shrager"],"pdf_url":"https://arxiv.org/pdf/2406.17650v1.pdf","comment":"In review in IEEE Annals of the History of Computing (submitted Apr\n  2024)"},{"id":"http://arxiv.org/abs/2406.17641v1","updated":"2024-06-25T15:24:08Z","published":"2024-06-25T15:24:08Z","title":"The experience of humans' and robots' mutual (im)politeness in enacted\n  service scenarios: An empirical study","summary":"  The paper reports an empirical study of the effect of human treatment of a\nrobot on the social perception of the robot's behavior. The study employed an\nenacted interaction between an anthropomorphic \"waiter\" robot and two\ncustomers. The robot and one of the customers (acted out by a researcher) were\nfollowing four different interaction scripts, representing all combinations of\nmutual politeness and impoliteness of the robot and the customer. The\nparticipants (N=24, within-subject design) were assigned the role of an\n\"included observer\", that is, a fellow customer who was present in the\nsituation without being actively involved in the interactions. The participants\nassessed how they experienced the interaction scenarios by providing Likert\nscale scores and free-text responses. The results indicate that while impolite\nrobots' behavior was generally assessed negatively, it was commonly perceived\nas more justifiable and fairer if the robot was treated impolitely by the\nhuman. Politeness reciprocity expectations in the context of the social\nperception of robots are discussed.\n","authors":["Victor Kaptelinin","Suna Bensch","Thomas Hellström","Patrik Björnfot","Shikhar Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17641v1.pdf","comment":"19 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.15598v2","updated":"2024-06-25T14:22:30Z","published":"2024-06-21T18:57:37Z","title":"VR-NRP: A Virtual Reality Simulation for Training in the Neonatal\n  Resuscitation Program","summary":"  The use of Virtual Reality (VR) technologies has been extensively researched\nin surgical and anatomical education. VR provides a lifelike and interactive\nenvironment where healthcare providers can practice and refresh their skills in\na safe environment. VR has been shown to be as effective as traditional medical\neducation teaching methods, with the potential to provide more cost-effective\nand convenient means of curriculum delivery, especially in rural and remote\nareas or in environments with limited access to hands-on training. In this\nsense, VR offers the potential to be used to support resuscitation training for\nhealthcare providers such as the Neonatal Resuscitation Program (NRP). The NRP\nprogram is an evidence-based and standardized approach for training healthcare\nproviders on the resuscitation of the newborn. In this article, we describe a\nVR simulation environment that was designed and developed to refresh the skills\nof NRP providers. To validate this platform, we compared the VR-NRP simulation\nwith exposure to 360-degree immersive video. We found that both VR technologies\nwere positively viewed by healthcare professionals and performed very similarly\nto each other. However, the VR simulation provided a significantly increased\nfeeling of presence. Furthermore, participants found the VR simulation more\nuseful, leading to improved experiential learning outcomes. Also, participants\nusing VR simulation reported higher confidence in certain NRP skills, such as\nproper mask placement and newborn response evaluation. This research represents\na step forward in understanding how VR and related extended reality (XR)\ntechnologies can be applied for effective, immersive medical education, with\npotential benefits for remote and rural healthcare providers.\n","authors":["Mustafa Yalin Aydin","Vernon Curran","Susan White","Lourdes Pena-Castillo","Oscar Meruvia-Pastor"],"pdf_url":"https://arxiv.org/pdf/2406.15598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.09726v4","updated":"2024-06-25T13:28:04Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Cristóbal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v4.pdf","comment":"https://soundify.cc"},{"id":"http://arxiv.org/abs/2211.12492v2","updated":"2024-06-25T13:20:45Z","published":"2022-11-22T18:58:22Z","title":"VideoMap: Supporting Video Editing Exploration, Brainstorming, and\n  Prototyping in the Latent Space","summary":"  Video editing is a creative and complex endeavor and we believe that there is\npotential for reimagining a new video editing interface to better support the\ncreative and exploratory nature of video editing. We take inspiration from\nlatent space exploration tools that help users find patterns and connections\nwithin complex datasets. We present VideoMap, a proof-of-concept video editing\ninterface that operates on video frames projected onto a latent space. We\nsupport intuitive navigation through map-inspired navigational elements and\nfacilitate transitioning between different latent spaces through swappable\nlenses. We built three VideoMap components to support editors in three common\nvideo tasks. In a user study with both professionals and non-professionals,\neditors found that VideoMap helps reduce grunt work, offers a user-friendly\nexperience, provides an inspirational way of editing, and effectively supports\nthe exploratory nature of video editing. We further demonstrate the versatility\nof VideoMap by implementing three extended applications. For interactive\nexamples, we invite you to visit our project page:\nhttps://humanvideointeraction.github.io/videomap.\n","authors":["David Chuan-En Lin","Fabian Caba Heilbron","Joon-Young Lee","Oliver Wang","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2211.12492v2.pdf","comment":"https://humanvideointeraction.github.io/videomap"},{"id":"http://arxiv.org/abs/2211.12493v2","updated":"2024-06-25T13:20:06Z","published":"2022-11-22T18:59:04Z","title":"Videogenic: Identifying Highlight Moments in Videos with Professional\n  Photographs as a Prior","summary":"  This paper investigates the challenge of extracting highlight moments from\nvideos. To perform this task, we need to understand what constitutes a\nhighlight for arbitrary video domains while at the same time being able to\nscale across different domains. Our key insight is that photographs taken by\nphotographers tend to capture the most remarkable or photogenic moments of an\nactivity. Drawing on this insight, we present Videogenic, a technique capable\nof creating domain-specific highlight videos for a diverse range of domains. In\na human evaluation study (N=50), we show that a high-quality photograph\ncollection combined with CLIP-based retrieval (which uses a neural network with\nsemantic knowledge of images) can serve as an excellent prior for finding video\nhighlights. In a within-subjects expert study (N=12), we demonstrate the\nusefulness of Videogenic in helping video editors create highlight videos with\nlighter workload, shorter task completion time, and better usability.\n","authors":["David Chuan-En Lin","Fabian Caba Heilbron","Joon-Young Lee","Oliver Wang","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2211.12493v2.pdf","comment":"https://humanvideointeraction.github.io/videogenic"},{"id":"http://arxiv.org/abs/2406.17531v1","updated":"2024-06-25T13:15:36Z","published":"2024-06-25T13:15:36Z","title":"Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity\n  Awareness","summary":"  This paper presents a system for diversity-aware autonomous conversation\nleveraging the capabilities of large language models (LLMs). The system adapts\nto diverse populations and individuals, considering factors like background,\npersonality, age, gender, and culture. The conversation flow is guided by the\nstructure of the system's pre-established knowledge base, while LLMs are tasked\nwith various functions, including generating diversity-aware sentences.\nAchieving diversity-awareness involves providing carefully crafted prompts to\nthe models, incorporating comprehensive information about users, conversation\nhistory, contextual details, and specific guidelines. To assess the system's\nperformance, we conducted both controlled and real-world experiments, measuring\na wide range of performance indicators.\n","authors":["Lucrezia Grassi","Carmine Tommaso Recchiuto","Antonio Sgorbissa"],"pdf_url":"https://arxiv.org/pdf/2406.17531v1.pdf","comment":"8 pages, 6 figures, 7 tables. This paper has been accepted for\n  publication at IEEE ROMAN 2024"},{"id":"http://arxiv.org/abs/2310.08574v2","updated":"2024-06-25T12:50:34Z","published":"2023-10-12T17:57:57Z","title":"Jigsaw: Supporting Designers to Prototype Multimodal Applications by\n  Chaining AI Foundation Models","summary":"  Recent advancements in AI foundation models have made it possible for them to\nbe utilized off-the-shelf for creative tasks, including ideating design\nconcepts or generating visual prototypes. However, integrating these models\ninto the creative process can be challenging as they often exist as standalone\napplications tailored to specific tasks. To address this challenge, we\nintroduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to\nrepresent foundation models. Jigsaw allows designers to combine different\nfoundation model capabilities across various modalities by assembling\ncompatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten\ndesigners and distilled design goals. In a user study, we showed that Jigsaw\nenhanced designers' understanding of available foundation model capabilities,\nprovided guidance on combining capabilities across different modalities and\ntasks, and served as a canvas to support design exploration, prototyping, and\ndocumentation.\n","authors":["David Chuan-En Lin","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2310.08574v2.pdf","comment":"https://jigsaw.to"},{"id":"http://arxiv.org/abs/2406.17986v1","updated":"2024-06-25T23:49:18Z","published":"2024-06-25T23:49:18Z","title":"VisConductor: Affect-Varying Widgets for Animated Data Storytelling in\n  Gesture-Aware Augmented Video Presentation","summary":"  Augmented video presentation tools provide a natural way for presenters to\ninteract with their content, resulting in engaging experiences for remote\naudiences, such as when a presenter uses hand gestures to manipulate and direct\nattention to visual aids overlaid on their webcam feed. However, authoring and\ncustomizing these presentations can be challenging, particularly when\npresenting dynamic data visualization (i.e., animated charts). To this end, we\nintroduce VisConductor, an authoring and presentation tool that equips\npresenters with the ability to configure gestures that control affect-varying\nvisualization animation, foreshadow visualization transitions, direct attention\nto notable data points, and animate the disclosure of annotations. These\ngestures are integrated into configurable widgets, allowing presenters to\ntrigger content transformations by executing gestures within widget boundaries,\nwith feedback visible only to them. Altogether, our palette of widgets provides\na level of flexibility appropriate for improvisational presentations and ad-hoc\ncontent transformations, such as when responding to audience engagement. To\nevaluate VisConductor, we conducted two studies focusing on presenters (N = 11)\nand audience members (N = 11). Our findings indicate that our approach taken\nwith VisConductor can facilitate interactive and engaging remote presentations\nwith dynamic visual aids. Reflecting on our findings, we also offer insights to\ninform the future of augmented video presentation tools.\n","authors":["Temiloluwa Femi-Gege","Matthew Brehmer","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.17986v1.pdf","comment":"To appear in ACM ISS'24"},{"id":"http://arxiv.org/abs/2406.17963v1","updated":"2024-06-25T22:44:53Z","published":"2024-06-25T22:44:53Z","title":"Empowering Interdisciplinary Insights with Dynamic Graph Embedding\n  Trajectories","summary":"  We developed DyGETViz, a novel framework for effectively visualizing dynamic\ngraphs (DGs) that are ubiquitous across diverse real-world systems. This\nframework leverages recent advancements in discrete-time dynamic graph (DTDG)\nmodels to adeptly handle the temporal dynamics inherent in dynamic graphs.\nDyGETViz effectively captures both micro- and macro-level structural shifts\nwithin these graphs, offering a robust method for representing complex and\nmassive dynamic graphs. The application of DyGETViz extends to a diverse array\nof domains, including ethology, epidemiology, finance, genetics, linguistics,\ncommunication studies, social studies, and international relations. Through its\nimplementation, DyGETViz has revealed or confirmed various critical insights.\nThese include the diversity of content sharing patterns and the degree of\nspecialization within online communities, the chronological evolution of\nlexicons across decades, and the distinct trajectories exhibited by\naging-related and non-related genes. Importantly, DyGETViz enhances the\naccessibility of scientific findings to non-domain experts by simplifying the\ncomplexities of dynamic graphs. Our framework is released as an open-source\nPython package for use across diverse disciplines. Our work not only addresses\nthe ongoing challenges in visualizing and analyzing DTDG models but also\nestablishes a foundational framework for future investigations into dynamic\ngraph representation and analysis across various disciplines.\n","authors":["Yiqiao Jin","Andrew Zhao","Yeon-Chang Lee","Meng Ye","Ajay Divakaran","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17963v1.pdf","comment":"25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2311.16711v2","updated":"2024-06-25T22:33:57Z","published":"2023-11-28T11:45:35Z","title":"LEDITS++: Limitless Image Editing using Text-to-Image Models","summary":"  Text-to-image diffusion models have recently received increasing interest for\ntheir astonishing ability to produce high-fidelity images from solely text\ninputs. Subsequent research efforts aim to exploit and apply their capabilities\nto real image editing. However, existing image-to-image methods are often\ninefficient, imprecise, and of limited versatility. They either require\ntime-consuming finetuning, deviate unnecessarily strongly from the input image,\nand/or lack support for multiple, simultaneous edits. To address these issues,\nwe introduce LEDITS++, an efficient yet versatile and precise textual image\nmanipulation technique. LEDITS++'s novel inversion approach requires no tuning\nnor optimization and produces high-fidelity results with a few diffusion steps.\nSecond, our methodology supports multiple simultaneous edits and is\narchitecture-agnostic. Third, we use a novel implicit masking technique that\nlimits changes to relevant image regions. We propose the novel TEdBench++\nbenchmark as part of our exhaustive evaluation. Our results demonstrate the\ncapabilities of LEDITS++ and its improvements over previous methods.\n","authors":["Manuel Brack","Felix Friedrich","Katharina Kornmeier","Linoy Tsaban","Patrick Schramowski","Kristian Kersting","Apolinário Passos"],"pdf_url":"https://arxiv.org/pdf/2311.16711v2.pdf","comment":"Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) The project page is available at\n  https://leditsplusplus-project.static.hf.space"},{"id":"http://arxiv.org/abs/2210.09531v2","updated":"2024-06-25T19:35:40Z","published":"2022-10-18T01:48:32Z","title":"The Brain-Inspired Cooperative Shared Control for Brain-Machine\n  Interface","summary":"  In the practical application of brain-machine interface technology, the\nproblem often faced is the low information content and high noise of the neural\nsignals collected by the electrode and the difficulty of decoding by the\ndecoder, which makes it difficult for the robotic to obtain stable instructions\nto complete the task. The idea based on the principle of cooperative shared\ncontrol can be achieved by extracting general motor commands from brain\nactivity, while the fine details of the movement can be hosted to the robot for\ncompletion, or the brain can have complete control. This study proposes a\nbrain-machine interface shared control system based on spiking neural networks\nfor robotic arm movement control and wheeled robots wheel speed control and\nsteering, respectively. The former can reliably control the robotic arm to move\nto the destination position, while the latter controls the wheeled robots for\nobject tracking and map generation. The results show that the shared control\nbased on brain-inspired intelligence can perform some typical tasks in complex\nenvironments and positively improve the fluency and ease of use of\nbrain-machine interaction, and also demonstrate the potential of this control\nmethod in clinical applications of brain-machine interfaces.\n","authors":["Shengjie Zheng","Ling Liu","Junjie Yang","Lang Qian","Gang Gao","Xin Chen","Wenqi Jin","Chunshan Deng","Xiaojian Li"],"pdf_url":"https://arxiv.org/pdf/2210.09531v2.pdf","comment":"This article need to update the corrected figure and data"},{"id":"http://arxiv.org/abs/2406.17882v1","updated":"2024-06-25T18:42:52Z","published":"2024-06-25T18:42:52Z","title":"Cyber Security Operations Educational Gamification Application Listing","summary":"  This listing contains a total of 80 gamification applications (GA)s used in\ncyber security operations (CSO) undergraduate education, from 74 publications,\npublished between 2007 and June 2022. The listing outlines each GA identified\nand provides a short overview of each. This listing serves as both a\ncomprehensive repository of existing GAs in cybersecurity undergraduate\neducation, and as a starting point for adding new CSO GAs to the list. Contact\nthe first author to add a CSO GA to the next version of the list.\n","authors":["Sherri Weitl-Harms","Adam Spanier","John D. Hastings"],"pdf_url":"https://arxiv.org/pdf/2406.17882v1.pdf","comment":"15 pages, 0 figures"},{"id":"http://arxiv.org/abs/2406.17872v1","updated":"2024-06-25T18:19:44Z","published":"2024-06-25T18:19:44Z","title":"Analysis of the Causes of Car Accidents in the United States of America\n  in 2023: Gauge People Understanding of Data Visualisation","summary":"  This paper presents a comprehensive examination of interactive data\nvisualization tools and their efficacy in the context of United States car\naccident data for the year 2023. We developed interactive heatmaps, histograms,\nand pie charts to enhance the understanding of accident severity distribution\nover time and location. Our research included the creation and distribution of\nan online survey, consisting of nine questions designed to test participants\ncomprehension of the presented data. Fifteen respondents were recruited to\ncomplete the survey, with the intent of assessing the effectiveness of both\nstatic and interactive versions of each visualization tool. The results\nindicated that participants using interactive heatmaps showed a greater\nunderstanding of the data, as compared to those using histograms and pie\ncharts. In contrast, no notable difference in comprehension was observed\nbetween users of static and interactive histograms. Unexpectedly, static pie\ncharts were found to be slightly more effective than their interactive\ncounterparts. These findings suggest that while interactive visualizations can\nbe powerful, their utility may vary depending on the type and complexity of the\ndata presented. Future research is recommended to explore the influence of\nsocioeconomic factors on the understanding of car accident data, potentially\nleading to more tailored and effective visualization strategies. This could\nprovide deeper insights into the patterns and causes of car accidents,\nfacilitating better-informed decision-making for stakeholders. Visit our\nwebsite to explore our interactive plots and engage directly with the data for\na more comprehensive understanding of our findings.\n","authors":["Hamoud Alhazmi","Marcelo Morales","Jiachen Jiang","Jinxin Zhou","Jian Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17872v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.17838v1","updated":"2024-06-25T16:56:45Z","published":"2024-06-25T16:56:45Z","title":"InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge\n  Distillation","summary":"  The emergence of large-scale pre-trained models has heightened their\napplication in various downstream tasks, yet deployment is a challenge in\nenvironments with limited computational resources. Knowledge distillation has\nemerged as a solution in such scenarios, whereby knowledge from large teacher\nmodels is transferred into smaller student' models, but this is a non-trivial\nprocess that traditionally requires technical expertise in AI/ML. To address\nthese challenges, this paper presents InFiConD, a novel framework that\nleverages visual concepts to implement the knowledge distillation process and\nenable subsequent no-code fine-tuning of student models. We develop a novel\nknowledge distillation pipeline based on extracting text-aligned visual\nconcepts from a concept corpus using multimodal models, and construct highly\ninterpretable linear student models based on visual concepts that mimic a\nteacher model in a response-based manner. InFiConD's interface allows users to\ninteractively fine-tune the student model by manipulating concept influences\ndirectly in the user interface. We validate InFiConD via a robust usage\nscenario and user study. Our findings indicate that InFiConD's\nhuman-in-the-loop and visualization-driven approach enables users to\neffectively create and analyze student models, understand how knowledge is\ntransferred, and efficiently perform fine-tuning operations. We discuss how\nthis work highlights the potential of interactive and visual methods in making\nknowledge distillation and subsequent no-code fine-tuning more accessible and\nadaptable to a wider range of users with domain-specific demands.\n","authors":["Jinbin Huang","Wenbin He","Liang Gou","Liu Ren","Chris Bryan"],"pdf_url":"https://arxiv.org/pdf/2406.17838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03547v2","updated":"2024-06-25T16:37:48Z","published":"2023-11-06T21:30:59Z","title":"InterVLS: Interactive Model Understanding and Improvement with\n  Vision-Language Surrogates","summary":"  Deep learning models are widely used in critical applications, highlighting\nthe need for pre-deployment model understanding and improvement. Visual\nconcept-based methods, while increasingly used for this purpose, face\nchallenges: (1) most concepts lack interpretability, (2) existing methods\nrequire model knowledge, often unavailable at run time. Additionally, (3) there\nlacks a no-code method for post-understanding model improvement. Addressing\nthese, we present InterVLS. The system facilitates model understanding by\ndiscovering text-aligned concepts, measuring their influence with\nmodel-agnostic linear surrogates. Employing visual analytics, InterVLS offers\nconcept-based explanations and performance insights. It enables users to adjust\nconcept influences to update a model, facilitating no-code model improvement.\nWe evaluate InterVLS in a user study, illustrating its functionality with two\nscenarios. Results indicates that InterVLS is effective to help users identify\ninfluential concepts to a model, gain insights and adjust concept influence to\nimprove the model. We conclude with a discussion based on our study results.\n","authors":["Jinbin Huang","Wenbin He","Liang Gou","Liu Ren","Chris Bryan"],"pdf_url":"https://arxiv.org/pdf/2311.03547v2.pdf","comment":"The paper has gone through a major update. The newer version will be\n  titled \"InFiConD: Interactive No-code Finetuning with Concept-based Knowledge\n  Distillation\""}]},"2024-06-26T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.07544v2","updated":"2024-06-26T17:59:50Z","published":"2024-06-11T17:59:45Z","title":"Situational Awareness Matters in 3D Vision Language Reasoning","summary":"  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n","authors":["Yunze Man","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07544v2.pdf","comment":"CVPR 2024. Project Page: https://yunzeman.github.io/situation3d"},{"id":"http://arxiv.org/abs/2406.18533v1","updated":"2024-06-26T17:59:28Z","published":"2024-06-26T17:59:28Z","title":"On Scaling Up 3D Gaussian Splatting Training","summary":"  3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction\ndue to its superior visual quality and rendering speed. However, 3DGS training\ncurrently occurs on a single GPU, limiting its ability to handle\nhigh-resolution and large-scale 3D reconstruction tasks due to memory\nconstraints. We introduce Grendel, a distributed system designed to partition\n3DGS parameters and parallelize computation across multiple GPUs. As each\nGaussian affects a small, dynamic subset of rendered pixels, Grendel employs\nsparse all-to-all communication to transfer the necessary Gaussians to pixel\npartitions and performs dynamic load balancing. Unlike existing 3DGS systems\nthat train using one camera view image at a time, Grendel supports batched\ntraining with multiple views. We explore various optimization hyperparameter\nscaling strategies and find that a simple sqrt(batch size) scaling rule is\nhighly effective. Evaluations using large-scale, high-resolution scenes show\nthat Grendel enhances rendering quality by scaling up 3DGS parameters across\nmultiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by\ndistributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28\nusing 11.2 million Gaussians on a single GPU. Grendel is an open-source project\navailable at: https://github.com/nyu-systems/Grendel-GS\n","authors":["Hexu Zhao","Haoyang Weng","Daohan Lu","Ang Li","Jinyang Li","Aurojit Panda","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2406.18533v1.pdf","comment":"Code: https://github.com/nyu-systems/Grendel-GS ; Project page:\n  https://daohanlu.github.io/scaling-up-3dgs"},{"id":"http://arxiv.org/abs/2406.14539v2","updated":"2024-06-26T17:59:24Z","published":"2024-06-20T17:49:11Z","title":"Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps","summary":"  Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.\n","authors":["Nikita Starodubcev","Mikhail Khoroshikh","Artem Babenko","Dmitry Baranchuk"],"pdf_url":"https://arxiv.org/pdf/2406.14539v2.pdf","comment":"Project page: https://yandex-research.github.io/invertible-cd/"},{"id":"http://arxiv.org/abs/2406.18530v1","updated":"2024-06-26T17:57:25Z","published":"2024-06-26T17:57:25Z","title":"MatchTime: Towards Automatic Soccer Game Commentary Generation","summary":"  Soccer is a globally popular sport with a vast audience, in this paper, we\nconsider constructing an automatic soccer game commentary model to improve the\naudiences' viewing experience. In general, we make the following contributions:\nFirst, observing the prevalent video-text misalignment in existing datasets, we\nmanually annotate timestamps for 49 matches, establishing a more robust\nbenchmark for soccer game commentary generation, termed as\nSN-Caption-test-align; Second, we propose a multi-modal temporal alignment\npipeline to automatically correct and filter the existing dataset at scale,\ncreating a higher-quality soccer game commentary dataset for training, denoted\nas MatchTime; Third, based on our curated dataset, we train an automatic\ncommentary generation model, named MatchVoice. Extensive experiments and\nablation studies have demonstrated the effectiveness of our alignment pipeline,\nand training model on the curated datasets achieves state-of-the-art\nperformance for commentary generation, showcasing that better alignment can\nlead to significant performance improvements in downstream tasks.\n","authors":["Jiayuan Rao","Haoning Wu","Chang Liu","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.18530v1.pdf","comment":"Technical Report; Project Page:\n  https://haoningwu3639.github.io/MatchTime/"},{"id":"http://arxiv.org/abs/2406.18524v1","updated":"2024-06-26T17:53:51Z","published":"2024-06-26T17:53:51Z","title":"MultiDiff: Consistent Novel View Synthesis from a Single Image","summary":"  We introduce MultiDiff, a novel approach for consistent novel view synthesis\nof scenes from a single RGB image. The task of synthesizing novel views from a\nsingle reference image is highly ill-posed by nature, as there exist multiple,\nplausible explanations for unobserved areas. To address this issue, we\nincorporate strong priors in form of monocular depth predictors and\nvideo-diffusion models. Monocular depth enables us to condition our model on\nwarped reference images for the target views, increasing geometric stability.\nThe video-diffusion prior provides a strong proxy for 3D scenes, allowing the\nmodel to learn continuous and pixel-accurate correspondences across generated\nimages. In contrast to approaches relying on autoregressive image generation\nthat are prone to drifts and error accumulation, MultiDiff jointly synthesizes\na sequence of frames yielding high-quality and multi-view consistent results --\neven for long-term scene generation with large camera movements, while reducing\ninference time by an order of magnitude. For additional consistency and image\nquality improvements, we introduce a novel, structured noise distribution. Our\nexperimental results demonstrate that MultiDiff outperforms state-of-the-art\nmethods on the challenging, real-world datasets RealEstate10K and ScanNet.\nFinally, our model naturally supports multi-view consistent editing without the\nneed for further tuning.\n","authors":["Norman Müller","Katja Schwarz","Barbara Roessle","Lorenzo Porzi","Samuel Rota Bulò","Matthias Nießner","Peter Kontschieder"],"pdf_url":"https://arxiv.org/pdf/2406.18524v1.pdf","comment":"Project page: https://sirwyver.github.io/MultiDiff Video:\n  https://youtu.be/zBC4z4qXW_4 - CVPR 2024"},{"id":"http://arxiv.org/abs/2406.18522v1","updated":"2024-06-26T17:50:47Z","published":"2024-06-26T17:50:47Z","title":"ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation","summary":"  We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.\n","authors":["Shenghai Yuan","Jinfa Huang","Yongqi Xu","Yaoyang Liu","Shaofeng Zhang","Yujun Shi","Ruijie Zhu","Xinhua Cheng","Jiebo Luo","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18522v1.pdf","comment":"31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2406.18521v1","updated":"2024-06-26T17:50:11Z","published":"2024-06-26T17:50:11Z","title":"CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal\n  LLMs","summary":"  Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/\n","authors":["Zirui Wang","Mengzhou Xia","Luxi He","Howard Chen","Yitao Liu","Richard Zhu","Kaiqu Liang","Xindi Wu","Haotian Liu","Sadhika Malladi","Alexis Chevalier","Sanjeev Arora","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18521v1.pdf","comment":"121 pages, 90 figures"},{"id":"http://arxiv.org/abs/2406.18516v1","updated":"2024-06-26T17:40:30Z","published":"2024-06-26T17:40:30Z","title":"Denoising as Adaptation: Noise-Space Domain Adaptation for Image\n  Restoration","summary":"  Although deep learning-based image restoration methods have made significant\nprogress, they still struggle with limited generalization to real-world\nscenarios due to the substantial domain gap caused by training on synthetic\ndata. Existing methods address this issue by improving data synthesis\npipelines, estimating degradation kernels, employing deep internal learning,\nand performing domain adaptation and regularization. Previous domain adaptation\nmethods have sought to bridge the domain gap by learning domain-invariant\nknowledge in either feature or pixel space. However, these techniques often\nstruggle to extend to low-level vision tasks within a stable and compact\nframework. In this paper, we show that it is possible to perform domain\nadaptation via the noise-space using diffusion models. In particular, by\nleveraging the unique property of how the multi-step denoising process is\ninfluenced by auxiliary conditional inputs, we obtain meaningful gradients from\nnoise prediction to gradually align the restored results of both synthetic and\nreal-world data to a common clean distribution. We refer to this method as\ndenoising as adaptation. To prevent shortcuts during training, we present\nuseful techniques such as channel shuffling and residual-swapping contrastive\nlearning. Experimental results on three classical image restoration tasks,\nnamely denoising, deblurring, and deraining, demonstrate the effectiveness of\nthe proposed method. Code will be released at:\nhttps://github.com/KangLiao929/Noise-DA/.\n","authors":["Kang Liao","Zongsheng Yue","Zhouxia Wang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.18516v1.pdf","comment":"Github Repository: https://github.com/KangLiao929/Noise-DA/"},{"id":"http://arxiv.org/abs/2406.18481v1","updated":"2024-06-26T16:47:31Z","published":"2024-06-26T16:47:31Z","title":"Robust Surgical Phase Recognition From Annotation Efficient Supervision","summary":"  Surgical phase recognition is a key task in computer-assisted surgery, aiming\nto automatically identify and categorize the different phases within a surgical\nprocedure. Despite substantial advancements, most current approaches rely on\nfully supervised training, requiring expensive and time-consuming frame-level\nannotations. Timestamp supervision has recently emerged as a promising\nalternative, significantly reducing annotation costs while maintaining\ncompetitive performance. However, models trained on timestamp annotations can\nbe negatively impacted by missing phase annotations, leading to a potential\ndrawback in real-world scenarios. In this work, we address this issue by\nproposing a robust method for surgical phase recognition that can handle\nmissing phase annotations effectively. Furthermore, we introduce the SkipTag@K\nannotation approach to the surgical domain, enabling a flexible balance between\nannotation effort and model performance. Our method achieves competitive\nresults on two challenging datasets, demonstrating its efficacy in handling\nmissing phase annotations and its potential for reducing annotation costs.\nSpecifically, we achieve an accuracy of 85.1\\% on the MultiBypass140 dataset\nusing only 3 annotated frames per video, showcasing the effectiveness of our\nmethod and the potential of the SkipTag@K setup. We perform extensive\nexperiments to validate the robustness of our method and provide valuable\ninsights to guide future research in surgical phase recognition. Our work\ncontributes to the advancement of surgical workflow recognition and paves the\nway for more efficient and reliable surgical phase recognition systems.\n","authors":["Or Rubin","Shlomi Laufer"],"pdf_url":"https://arxiv.org/pdf/2406.18481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09858v2","updated":"2024-06-26T16:26:08Z","published":"2023-09-18T15:20:13Z","title":"Unsupervised Open-Vocabulary Object Localization in Videos","summary":"  In this paper, we show that recent advances in video representation learning\nand pre-trained vision-language models allow for substantial improvements in\nself-supervised video object localization. We propose a method that first\nlocalizes objects in videos via an object-centric approach with slot attention\nand then assigns text to the obtained slots. The latter is achieved by an\nunsupervised way to read localized semantic information from the pre-trained\nCLIP model. The resulting video object localization is entirely unsupervised\napart from the implicit annotation contained in CLIP, and it is effectively the\nfirst unsupervised approach that yields good results on regular video\nbenchmarks.\n","authors":["Ke Fan","Zechen Bai","Tianjun Xiao","Dominik Zietlow","Max Horn","Zixu Zhao","Carl-Johann Simon-Gabriel","Mike Zheng Shou","Francesco Locatello","Bernt Schiele","Thomas Brox","Zheng Zhang","Yanwei Fu","Tong He"],"pdf_url":"https://arxiv.org/pdf/2309.09858v2.pdf","comment":"Accepted by ICCV 2023; Presented on CVPR 2024 Workshop CORR; Project\n  Page:https://github.com/amazon-science/object-centric-vol"},{"id":"http://arxiv.org/abs/2406.18462v1","updated":"2024-06-26T16:12:09Z","published":"2024-06-26T16:12:09Z","title":"GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly\n  Enhanced Quality","summary":"  Recently, 3D Gaussian splatting (3D-GS) has achieved great success in\nreconstructing and rendering real-world scenes. To transfer the high rendering\nquality to generation tasks, a series of research works attempt to generate\n3D-Gaussian assets from text. However, the generated assets have not achieved\nthe same quality as those in reconstruction tasks. We observe that Gaussians\ntend to grow without control as the generation process may cause indeterminacy.\nAiming at highly enhancing the generation quality, we propose a novel framework\nnamed GaussianDreamerPro. The main idea is to bind Gaussians to reasonable\ngeometry, which evolves over the whole generation process. Along different\nstages of our framework, both the geometry and appearance can be enriched\nprogressively. The final output asset is constructed with 3D Gaussians bound to\nmesh, which shows significantly enhanced details and quality compared with\nprevious methods. Notably, the generated asset can also be seamlessly\nintegrated into downstream manipulation pipelines, e.g. animation, composition,\nand simulation etc., greatly promoting its potential in wide applications.\nDemos are available at https://taoranyi.com/gaussiandreamerpro/.\n","authors":["Taoran Yi","Jiemin Fang","Zanwei Zhou","Junjie Wang","Guanjun Wu","Lingxi Xie","Xiaopeng Zhang","Wenyu Liu","Xinggang Wang","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2406.18462v1.pdf","comment":"Project page: https://taoranyi.com/gaussiandreamerpro/"},{"id":"http://arxiv.org/abs/2406.18459v1","updated":"2024-06-26T16:10:31Z","published":"2024-06-26T16:10:31Z","title":"DiffuseHigh: Training-free Progressive High-Resolution Image Synthesis\n  through Structure Guidance","summary":"  Recent surge in large-scale generative models has spurred the development of\nvast fields in computer vision. In particular, text-to-image diffusion models\nhave garnered widespread adoption across diverse domain due to their potential\nfor high-fidelity image generation. Nonetheless, existing large-scale diffusion\nmodels are confined to generate images of up to 1K resolution, which is far\nfrom meeting the demands of contemporary commercial applications. Directly\nsampling higher-resolution images often yields results marred by artifacts such\nas object repetition and distorted shapes. Addressing the aforementioned issues\ntypically necessitates training or fine-tuning models on higher resolution\ndatasets. However, this undertaking poses a formidable challenge due to the\ndifficulty in collecting large-scale high-resolution contents and substantial\ncomputational resources. While several preceding works have proposed\nalternatives, they often fail to produce convincing results. In this work, we\nprobe the generative ability of diffusion models at higher resolution beyond\nits original capability and propose a novel progressive approach that fully\nutilizes generated low-resolution image to guide the generation of higher\nresolution image. Our method obviates the need for additional training or\nfine-tuning which significantly lowers the burden of computational costs.\nExtensive experiments and results validate the efficiency and efficacy of our\nmethod.\n","authors":["Younghyun Kim","Geunmin Hwang","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2406.18459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18453v1","updated":"2024-06-26T16:01:10Z","published":"2024-06-26T16:01:10Z","title":"Towards Human-Level 3D Relative Pose Estimation: Generalizable,\n  Training-Free, with Single Reference","summary":"  Humans can easily deduce the relative pose of an unseen object, without\nlabel/training, given only a single query-reference image pair. This is\narguably achieved by incorporating (i) 3D/2.5D shape perception from a single\nimage, (ii) render-and-compare simulation, and (iii) rich semantic cue\nawareness to furnish (coarse) reference-query correspondence. Existing methods\nimplement (i) by a 3D CAD model or well-calibrated multiple images and (ii) by\ntraining a network on specific objects, which necessitate laborious\nground-truth labeling and tedious training, potentially leading to challenges\nin generalization. Moreover, (iii) was less exploited in the paradigm of (ii),\ndespite that the coarse correspondence from (iii) enhances the compare process\nby filtering out non-overlapped parts under substantial pose\ndifferences/occlusions. Motivated by this, we propose a novel 3D generalizable\nrelative pose estimation method by elaborating (i) with a 2.5D shape from an\nRGB-D reference, (ii) with an off-the-shelf differentiable renderer, and (iii)\nwith semantic cues from a pretrained model like DINOv2. Specifically, our\ndifferentiable renderer takes the 2.5D rotatable mesh textured by the RGB and\nthe semantic maps (obtained by DINOv2 from the RGB input), then renders new RGB\nand semantic maps (with back-surface culling) under a novel rotated view. The\nrefinement loss comes from comparing the rendered RGB and semantic maps with\nthe query ones, back-propagating the gradients through the differentiable\nrenderer to refine the 3D relative pose. As a result, our method can be readily\napplied to unseen objects, given only a single RGB-D reference, without\nlabel/training. Extensive experiments on LineMOD, LM-O, and YCB-V show that our\ntraining-free method significantly outperforms the SOTA supervised methods,\nespecially under the rigorous Acc@5/10/15{\\deg} metrics and the challenging\ncross-dataset settings.\n","authors":["Yuan Gao","Yajing Luo","Junhong Wang","Kui Jia","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2406.18453v1.pdf","comment":"The codes are available at\n  https://github.com/ethanygao/training-free_generalizable_relative_pose"},{"id":"http://arxiv.org/abs/2406.18451v1","updated":"2024-06-26T16:00:35Z","published":"2024-06-26T16:00:35Z","title":"Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers","summary":"  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.\n","authors":["Jonas Ngnawé","Sabyasachi Sahoo","Yann Pequignot","Frédéric Precioso","Christian Gagné"],"pdf_url":"https://arxiv.org/pdf/2406.18451v1.pdf","comment":"11 pages, 7 figures, 2 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2406.18443v1","updated":"2024-06-26T15:48:24Z","published":"2024-06-26T15:48:24Z","title":"Unveiling the Unknown: Conditional Evidence Decoupling for Unknown\n  Rejection","summary":"  In this paper, we focus on training an open-set object detector under the\ncondition of scarce training samples, which should distinguish the known and\nunknown categories. Under this challenging scenario, the decision boundaries of\nunknowns are difficult to learn and often ambiguous. To mitigate this issue, we\ndevelop a novel open-set object detection framework, which delves into\nconditional evidence decoupling for the unknown rejection. Specifically, we\nselect pseudo-unknown samples by leveraging the discrepancy in attribution\ngradients between known and unknown classes, alleviating the inadequate unknown\ndistribution coverage of training data. Subsequently, we propose a Conditional\nEvidence Decoupling Loss (CEDL) based on Evidential Deep Learning (EDL) theory,\nwhich decouples known and unknown properties in pseudo-unknown samples to learn\ndistinct knowledge, enhancing separability between knowns and unknowns.\nAdditionally, we propose an Abnormality Calibration Loss (ACL), which serves as\na regularization term to adjust the output probability distribution,\nestablishing robust decision boundaries for the unknown rejection. Our method\nhas achieved the superiority performance over previous state-of-the-art\napproaches, improving the mean recall of unknown class by 7.24% across all\nshots in VOC10-5-5 dataset settings and 1.38% in VOC-COCO dataset settings. The\ncode is available via https://github.com/zjzwzw/CED-FOOD.\n","authors":["Zhaowei Wu","Binyi Su","Hua Zhang","Zhong Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15613v3","updated":"2024-06-26T15:47:02Z","published":"2024-01-28T10:00:45Z","title":"Towards Arbitrary-Scale Histopathology Image Super-resolution: An\n  Efficient Dual-branch Framework via Implicit Self-texture Enhancement","summary":"  High-quality whole-slide scanners are expensive, complex, and time-consuming,\nthus limiting the acquisition and utilization of high-resolution pathology\nwhole-slide images in daily clinical work. Deep learning-based single-image\nsuper-resolution techniques are an effective way to solve this problem by\nsynthesizing high-resolution images from low-resolution ones. However, the\nexisting super-resolution models applied in pathology images can only work in\nfixed integer magnifications, significantly decreasing their applicability.\nThough methods based on implicit neural representation have shown promising\nresults in arbitrary-scale super-resolution of natural images, applying them\ndirectly to pathology images is inadequate because they have unique\nfine-grained image textures different from natural images. Thus, we propose an\nImplicit Self-Texture Enhancement-based dual-branch framework (ISTE) for\narbitrary-scale super-resolution of pathology images to address this challenge.\nISTE contains a pixel learning branch and a texture learning branch, which\nfirst learn pixel features and texture features, respectively. Then, we design\na two-stage texture enhancement strategy to fuse the features from the two\nbranches to obtain the super-resolution results, where the first stage is\nfeature-based texture enhancement, and the second stage is spatial-domain-based\ntexture enhancement. Extensive experiments on three public datasets show that\nISTE outperforms existing fixed-scale and arbitrary-scale algorithms at\nmultiple magnifications and helps to improve downstream task performance. To\nthe best of our knowledge, this is the first work to achieve arbitrary-scale\nsuper-resolution in pathology images. Codes will be available.\n","authors":["Minghong Duan","Linhao Qu","Zhiwei Yang","Manning Wang","Chenxi Zhang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2401.15613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04063v3","updated":"2024-06-26T15:41:37Z","published":"2023-12-07T06:03:07Z","title":"An unsupervised approach towards promptable defect segmentation in\n  laser-based additive manufacturing by Segment Anything","summary":"  Foundation models are currently driving a paradigm shift in computer vision\ntasks for various fields including biology, astronomy, and robotics among\nothers, leveraging user-generated prompts to enhance their performance. In the\nLaser Additive Manufacturing (LAM) domain, accurate image-based defect\nsegmentation is imperative to ensure product quality and facilitate real-time\nprocess control. However, such tasks are often characterized by multiple\nchallenges including the absence of labels and the requirement for low latency\ninference among others. Porosity is a very common defect in LAM due to lack of\nfusion, entrapped gas, and keyholes, directly affecting mechanical properties\nlike tensile strength, stiffness, and hardness, thereby compromising the\nquality of the final product. To address these issues, we construct a framework\nfor image segmentation using a state-of-the-art Vision Transformer (ViT) based\nFoundation model (Segment Anything Model) with a novel multi-point prompt\ngeneration scheme using unsupervised clustering. Utilizing our framework we\nperform porosity segmentation in a case study of laser-based powder bed fusion\n(L-PBF) and obtain high accuracy without using any labeled data to guide the\nprompt tuning process. By capitalizing on lightweight foundation model\ninference combined with unsupervised prompt generation, we envision\nconstructing a real-time anomaly detection pipeline that could revolutionize\ncurrent laser additive manufacturing processes, thereby facilitating the shift\ntowards Industry 4.0 and promoting defect-free production along with\noperational efficiency.\n","authors":["Israt Zarin Era","Imtiaz Ahmed","Zhichao Liu","Srinjoy Das"],"pdf_url":"https://arxiv.org/pdf/2312.04063v3.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2211.05622v2","updated":"2024-06-26T15:34:47Z","published":"2022-11-10T14:54:31Z","title":"InstantGroup: Instant Template Generation for Scalable Group of Brain\n  MRI Registration","summary":"  Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAEs backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.\n","authors":["Ziyi He","Albert C. S. Chung"],"pdf_url":"https://arxiv.org/pdf/2211.05622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18430v1","updated":"2024-06-26T15:27:26Z","published":"2024-06-26T15:27:26Z","title":"Facial Image Feature Analysis and its Specialization for Fréchet\n  Distance and Neighborhoods","summary":"  Assessing distances between images and image datasets is a fundamental task\nin vision-based research. It is a challenging open problem in the literature\nand despite the criticism it receives, the most ubiquitous method remains the\nFr\\'echet Inception Distance. The Inception network is trained on a specific\nlabeled dataset, ImageNet, which has caused the core of its criticism in the\nmost recent research. Improvements were shown by moving to self-supervision\nlearning over ImageNet, leaving the training data domain as an open question.\nWe make that last leap and provide the first analysis on domain-specific\nfeature training and its effects on feature distance, on the widely-researched\nfacial image domain. We provide our findings and insights on this domain\nspecialization for Fr\\'echet distance and image neighborhoods, supported by\nextensive experiments and in-depth user studies.\n","authors":["Doruk Cetin","Benedikt Schesch","Petar Stamenkovic","Niko Benjamin Huber","Fabio Zünd","Majed El Helou"],"pdf_url":"https://arxiv.org/pdf/2406.18430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18422v1","updated":"2024-06-26T15:18:20Z","published":"2024-06-26T15:18:20Z","title":"Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D\n  Generative Modeling","summary":"  This paper investigates a 2D to 3D image translation method with a\nstraightforward technique, enabling correlated 2D X-ray to 3D CT-like\nreconstruction. We observe that existing approaches, which integrate\ninformation across multiple 2D views in the latent space, lose valuable signal\ninformation during latent encoding. Instead, we simply repeat and concatenate\nthe 2D views into higher-channel 3D volumes and approach the 3D reconstruction\nchallenge as a straightforward 3D to 3D generative modeling problem,\nsidestepping several complex modeling issues. This method enables the\nreconstructed 3D volume to retain valuable information from the 2D inputs,\nwhich are passed between channel states in a Swin UNETR backbone. Our approach\napplies neural optimal transport, which is fast and stable to train,\neffectively integrating signal information across multiple views without the\nrequirement for precise alignment; it produces non-collapsed reconstructions\nthat are highly faithful to the 2D views, even after limited training. We\ndemonstrate correlated results, both qualitatively and quantitatively, having\ntrained our model on a single dataset and evaluated its generalization ability\nacross six datasets, including out-of-distribution samples.\n","authors":["Abril Corona-Figueroa","Hubert P. H. Shum","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2406.18422v1.pdf","comment":"CVPRW 2024 - DCA in MI; Best Paper Award"},{"id":"http://arxiv.org/abs/2406.18414v1","updated":"2024-06-26T15:09:54Z","published":"2024-06-26T15:09:54Z","title":"BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using\n  Camera-LiDAR Data","summary":"  Compared with real-time multi-object tracking (MOT), offline multi-object\ntracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous\nlink correction, and full track optimization but has to deal with the\nchallenges from bounding box misalignment and track evaluation, editing, and\nrefinement. This paper proposes \"BiTrack\", a 3D OMOT framework that includes\nmodules of 2D-3D detection fusion, initial trajectory generation, and\nbidirectional trajectory re-optimization to achieve optimal tracking results\nfrom camera-LiDAR data. The novelty of this paper includes threefold: (1)\ndevelopment of a point-level object registration technique that employs a\ndensity-based similarity metric to achieve accurate fusion of 2D-3D detection\nresults; (2) development of a set of data association and track management\nskills that utilizes a vertex-based similarity metric as well as false alarm\nrejection and track recovery mechanisms to generate reliable bidirectional\nobject trajectories; (3) development of a trajectory re-optimization scheme\nthat re-organizes track fragments of different fidelities in a greedy fashion,\nas well as refines each trajectory with completion and smoothing techniques.\nThe experiment results on the KITTI dataset demonstrate that BiTrack achieves\nthe state-of-the-art performance for 3D OMOT tasks in terms of accuracy and\nefficiency.\n","authors":["Kemiao Huang","Meiying Zhang","Qi Hao"],"pdf_url":"https://arxiv.org/pdf/2406.18414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17775v2","updated":"2024-06-26T14:34:13Z","published":"2024-02-20T11:36:23Z","title":"WhaleNet: a Novel Deep Learning Architecture for Marine Mammals\n  Vocalizations on Watkins Marine Mammal Sound Database","summary":"  Marine mammal communication is a complex field, hindered by the diversity of\nvocalizations and environmental factors. The Watkins Marine Mammal Sound\nDatabase (WMMD) constitutes a comprehensive labeled dataset employed in machine\nlearning applications. Nevertheless, the methodologies for data preparation,\npreprocessing, and classification documented in the literature exhibit\nconsiderable variability and are typically not applied to the dataset in its\nentirety. This study initially undertakes a concise review of the\nstate-of-the-art benchmarks pertaining to the dataset, with a particular focus\non clarifying data preparation and preprocessing techniques. Subsequently, we\nexplore the utilization of the Wavelet Scattering Transform (WST) and Mel\nspectrogram as preprocessing mechanisms for feature extraction. In this paper,\nwe introduce \\textbf{WhaleNet} (Wavelet Highly Adaptive Learning Ensemble\nNetwork), a sophisticated deep ensemble architecture for the classification of\nmarine mammal vocalizations, leveraging both WST and Mel spectrogram for\nenhanced feature discrimination. By integrating the insights derived from WST\nand Mel representations, we achieved an improvement in classification accuracy\nby $8-10\\%$ over existing architectures, corresponding to a classification\naccuracy of $97.61\\%$.\n","authors":["Alessandro Licciardi","Davide Carbone"],"pdf_url":"https://arxiv.org/pdf/2402.17775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18387v1","updated":"2024-06-26T14:29:05Z","published":"2024-06-26T14:29:05Z","title":"DoubleTake: Geometry Guided Depth Estimation","summary":"  Estimating depth from a sequence of posed RGB images is a fundamental\ncomputer vision task, with applications in augmented reality, path planning\netc. Prior work typically makes use of previous frames in a multi view stereo\nframework, relying on matching textures in a local neighborhood. In contrast,\nour model leverages historical predictions by giving the latest 3D geometry\ndata as an extra input to our network. This self-generated geometric hint can\nencode information from areas of the scene not covered by the keyframes and it\nis more regularized when compared to individual predicted depth maps for\nprevious frames. We introduce a Hint MLP which combines cost volume features\nwith a hint of the prior geometry, rendered as a depth map from the current\ncamera location, together with a measure of the confidence in the prior\ngeometry. We demonstrate that our method, which can run at interactive speeds,\nachieves state-of-the-art estimates of depth and 3D scene reconstruction in\nboth offline and incremental evaluation scenarios.\n","authors":["Mohamed Sayed","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Guillermo Garcia-Hernando","Gabriel Brostow","Sara Vicente","Michael Firman"],"pdf_url":"https://arxiv.org/pdf/2406.18387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18375v1","updated":"2024-06-26T14:19:31Z","published":"2024-06-26T14:19:31Z","title":"From Majority to Minority: A Diffusion-based Augmentation for\n  Underrepresented Groups in Skin Lesion Analysis","summary":"  AI-based diagnoses have demonstrated dermatologist-level performance in\nclassifying skin cancer. However, such systems are prone to under-performing\nwhen tested on data from minority groups that lack sufficient representation in\nthe training sets. Although data collection and annotation offer the best means\nfor promoting minority groups, these processes are costly and time-consuming.\nPrior works have suggested that data from majority groups may serve as a\nvaluable information source to supplement the training of diagnosis tools for\nminority groups. In this work, we propose an effective diffusion-based\naugmentation framework that maximizes the use of rich information from majority\ngroups to benefit minority groups. Using groups with different skin types as a\ncase study, our results show that the proposed framework can generate synthetic\nimages that improve diagnostic results for the minority groups, even when there\nis little or no reference data from these target groups. The practical value of\nour work is evident in medical imaging analysis, where under-diagnosis persists\nas a problem for certain groups due to insufficient representation.\n","authors":["Janet Wang","Yunsung Chung","Zhengming Ding","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.18375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18361v1","updated":"2024-06-26T14:01:07Z","published":"2024-06-26T14:01:07Z","title":"Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process","summary":"  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n","authors":["Tianyu Lin","Zhiguang Chen","Zhonghao Yan","Fudan Zheng","Weijiang Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18361v1.pdf","comment":"Accepted at MICCAI 2024. Code and citation info see\n  https://github.com/lin-tianyu/Stable-Diffusion-Seg"},{"id":"http://arxiv.org/abs/2406.18360v1","updated":"2024-06-26T14:00:21Z","published":"2024-06-26T14:00:21Z","title":"XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis","summary":"  Thoroughly testing autonomy systems is crucial in the pursuit of safe\nautonomous driving vehicles. It necessitates creating safety-critical scenarios\nthat go beyond what can be safely collected from real-world data, as many of\nthese scenarios occur infrequently on public roads. However, the evaluation of\nmost existing NVS methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground truth images using\nmetrics. Unfortunately, this evaluation protocol falls short of meeting the\nactual requirements in closed-loop simulations. Specifically, the true\napplication demands the capability to render novel views that extend beyond the\noriginal trajectory (such as cross-lane views), which are challenging to\ncapture in the real world. To address this, this paper presents a novel driving\nview synthesis dataset and benchmark specifically designed for autonomous\ndriving simulations. This dataset is unique as it includes testing images\ncaptured by deviating from the training trajectory by 1-4 meters. It comprises\nsix sequences encompassing various time and weather conditions. Each sequence\ncontains 450 training images, 150 testing images, and their corresponding\ncamera poses and intrinsic parameters. Leveraging this novel dataset, we\nestablish the first realistic benchmark for evaluating existing NVS approaches\nunder front-only and multi-camera settings. The experimental findings\nunderscore the significant gap that exists in current approaches, revealing\ntheir inadequate ability to fulfill the demanding prerequisites of cross-lane\nor closed-loop simulation. Our dataset is released publicly at the project\npage: https://3d-aigc.github.io/XLD/.\n","authors":["Hao Li","Ming Yuan","Yan Zhang","Chenming Wu","Chen Zhao","Chunyu Song","Haocheng Feng","Errui Ding","Dingwen Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18360v1.pdf","comment":"project page: https://3d-aigc.github.io/XLD/"},{"id":"http://arxiv.org/abs/2212.13459v2","updated":"2024-06-26T13:59:56Z","published":"2022-12-27T12:03:38Z","title":"Scaling Painting Style Transfer","summary":"  Neural style transfer (NST) is a deep learning technique that produces an\nunprecedentedly rich style transfer from a style image to a content image. It\nis particularly impressive when it comes to transferring style from a painting\nto an image. NST was originally achieved by solving an optimization problem to\nmatch the global statistics of the style image while preserving the local\ngeometric features of the content image. The two main drawbacks of this\noriginal approach is that it is computationally expensive and that the\nresolution of the output images is limited by high GPU memory requirements.\nMany solutions have been proposed to both accelerate NST and produce images\nwith larger size. However, our investigation shows that these accelerated\nmethods all compromise the quality of the produced images in the context of\npainting style transfer. Indeed, transferring the style of a painting is a\ncomplex task involving features at different scales, from the color palette and\ncompositional style to the fine brushstrokes and texture of the canvas. This\npaper provides a solution to solve the original global optimization for\nultra-high resolution (UHR) images, enabling multiscale NST at unprecedented\nimage sizes. This is achieved by spatially localizing the computation of each\nforward and backward passes through the VGG network. Extensive qualitative and\nquantitative comparisons, as well as a \\textcolor{coverletter}{perceptual\nstudy}, show that our method produces style transfer of unmatched quality for\nsuch high-resolution painting styles. By a careful comparison, we show that\nstate-of-the-art fast methods are still prone to artifacts, thus suggesting\nthat fast painting style transfer remains an open problem. Source code is\navailable at https://github.com/bgalerne/scaling_painting_style_transfer.\n","authors":["Bruno Galerne","Lara Raad","José Lezama","Jean-Michel Morel"],"pdf_url":"https://arxiv.org/pdf/2212.13459v2.pdf","comment":"14 pages, 9 figures, 4 tables, accepted at EGSR 2024"},{"id":"http://arxiv.org/abs/2406.18350v1","updated":"2024-06-26T13:51:57Z","published":"2024-06-26T13:51:57Z","title":"On Reducing Activity with Distillation and Regularization for Energy\n  Efficient Spiking Neural Networks","summary":"  Interest in spiking neural networks (SNNs) has been growing steadily,\npromising an energy-efficient alternative to formal neural networks (FNNs),\ncommonly known as artificial neural networks (ANNs). Despite increasing\ninterest, especially for Edge applications, these event-driven neural networks\nsuffered from their difficulty to be trained compared to FNNs. To alleviate\nthis problem, a number of innovative methods have been developed to provide\nperformance more or less equivalent to that of FNNs. However, the spiking\nactivity of a network during inference is usually not considered. While SNNs\nmay usually have performance comparable to that of FNNs, it is often at the\ncost of an increase of the network's activity, thus limiting the benefit of\nusing them as a more energy-efficient solution.\n  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs\ntraining with surrogate gradient descent in order to optimize the trade-off\nbetween performance and spiking activity. Then, after understanding why KD led\nto an increase in sparsity, we also explored Activations regularization and\nproposed a novel method with Logits Regularization. These approaches, validated\non several datasets, clearly show a reduction in network spiking activity\n(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.\n","authors":["Thomas Louis","Benoit Miramond","Alain Pegatoquet","Adrien Girard"],"pdf_url":"https://arxiv.org/pdf/2406.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18344v1","updated":"2024-06-26T13:38:16Z","published":"2024-06-26T13:38:16Z","title":"AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature\n  Space","summary":"  We study the intriguing connection between visual data, deep networks, and\nthe brain. Our method creates a universal channel alignment by using brain\nvoxel fMRI response prediction as the training objective. We discover that deep\nnetworks, trained with different objectives, share common feature channels\nacross various models. These channels can be clustered into recurring sets,\ncorresponding to distinct brain regions, indicating the formation of visual\nconcepts. Tracing the clusters of channel responses onto the images, we see\nsemantically meaningful object segments emerge, even without any supervised\ndecoder. Furthermore, the universal feature alignment and the clustering of\nchannels produce a picture and quantification of how visual information is\nprocessed through the different network layers, which produces precise\ncomparisons between the networks.\n","authors":["Huzheng Yang","James Gee","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2406.18344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03251v4","updated":"2024-06-26T13:37:13Z","published":"2023-04-06T17:36:23Z","title":"SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation","summary":"  Learning models on one labeled dataset that generalize well on another domain\nis a difficult task, as several shifts might happen between the data domains.\nThis is notably the case for lidar data, for which models can exhibit large\nperformance discrepancies due for instance to different lidar patterns or\nchanges in acquisition conditions. This paper addresses the corresponding\nUnsupervised Domain Adaptation (UDA) task for semantic segmentation. To\nmitigate this problem, we introduce an unsupervised auxiliary task of learning\nan implicit underlying surface representation simultaneously on source and\ntarget data. As both domains share the same latent representation, the model is\nforced to accommodate discrepancies between the two sources of data. This novel\nstrategy differs from classical minimization of statistical divergences or\nlidar-specific domain adaptation techniques. Our experiments demonstrate that\nour method achieves a better performance than the current state of the art,\nboth in real-to-real and synthetic-to-real scenarios.\n","authors":["Björn Michele","Alexandre Boulch","Gilles Puy","Tuan-Hung Vu","Renaud Marlet","Nicolas Courty"],"pdf_url":"https://arxiv.org/pdf/2304.03251v4.pdf","comment":"Accepted as spotlight to 3DV 2024. Project repository:\n  github.com/valeoai/SALUDA"},{"id":"http://arxiv.org/abs/2406.18333v1","updated":"2024-06-26T13:21:08Z","published":"2024-06-26T13:21:08Z","title":"Continuous Sign Language Recognition Using Intra-inter Gloss Attention","summary":"  Many continuous sign language recognition (CSLR) studies adopt\ntransformer-based architectures for sequence modeling due to their powerful\ncapacity for capturing global contexts. Nevertheless, vanilla self-attention,\nwhich serves as the core module of the transformer, calculates a weighted\naverage over all time steps; therefore, the local temporal semantics of sign\nvideos may not be fully exploited. In this study, we introduce a novel module\nin sign language recognition studies, called intra-inter gloss attention\nmodule, to leverage the relationships among frames within glosses and the\nsemantic and grammatical dependencies between glosses in the video. In the\nintra-gloss attention module, the video is divided into equally sized chunks\nand a self-attention mechanism is applied within each chunk. This localized\nself-attention significantly reduces complexity and eliminates noise introduced\nby considering non-relative frames. In the inter-gloss attention module, we\nfirst aggregate the chunk-level features within each gloss chunk by average\npooling along the temporal dimension. Subsequently, multi-head self-attention\nis applied to all chunk-level features. Given the non-significance of the\nsigner-environment interaction, we utilize segmentation to remove the\nbackground of the videos. This enables the proposed model to direct its focus\ntoward the signer. Experimental results on the PHOENIX-2014 benchmark dataset\ndemonstrate that our method can effectively extract sign language features in\nan end-to-end manner without any prior knowledge, improve the accuracy of CSLR,\nand achieve the word error rate (WER) of 20.4 on the test set which is a\ncompetitive result compare to the state-of-the-art which uses additional\nsupervisions.\n","authors":["Hossein Ranjbar","Alireza Taheri"],"pdf_url":"https://arxiv.org/pdf/2406.18333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18327v1","updated":"2024-06-26T13:14:24Z","published":"2024-06-26T13:14:24Z","title":"Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor\n  Segmentation","summary":"  Accurate segmentation of tumors in PET/CT images is important in\ncomputer-aided diagnosis and treatment of cancer. The key issue of such a\nsegmentation problem lies in the effective integration of complementary\ninformation from PET and CT images. However, the quality of PET and CT images\nvaries widely in clinical settings, which leads to uncertainty in the modality\ninformation extracted by networks. To take the uncertainty into account in\nmulti-modal information fusion, this paper proposes a novel Multi-modal\nEvidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning\n(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module\nreduces the domain gap upon modality conversion and highlights common tumor\nfeatures, thereby alleviating the needs of the segmentation module to handle\nmodality specificity. The MTF module utilizes mutual attention mechanisms and\nan uncertainty calibrator to fuse modality features based on modality\nuncertainty and then fuse the segmentation results under the guidance of\nDempster-Shafer Theory. Besides, a new uncertainty perceptual loss is\nintroduced to force the model focusing on uncertain features and hence improve\nits ability to extract trusted modality information. Extensive comparative\nexperiments are conducted on two publicly available PET/CT datasets to evaluate\nthe performance of our proposed method whose results demonstrate that our MEFN\nsignificantly outperforms state-of-the-art methods with improvements of 2.15%\nand 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,\nrespectively. More importantly, our model can provide radiologists with\ncredible uncertainty of the segmentation results for their decision in\naccepting or rejecting the automatic segmentation results, which is\nparticularly important for clinical applications. Our code will be available at\nhttps://github.com/QPaws/MEFN.\n","authors":["Yuxuan Qi","Li Lin","Jiajun Wang","Jingya Zhang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06658v2","updated":"2024-06-26T13:01:55Z","published":"2024-03-11T12:27:20Z","title":"Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework","summary":"  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n","authors":["Henrique Jesus","Hugo Proença"],"pdf_url":"https://arxiv.org/pdf/2403.06658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04861v2","updated":"2024-06-26T12:59:02Z","published":"2024-01-10T00:40:05Z","title":"CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from\n  Monocular Video","summary":"  The goal of our work is to generate high-quality novel views from monocular\nvideos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have\nshown impressive performance by leveraging time-varying dynamic radiation\nfields. However, these methods have limitations when it comes to accurately\nmodeling the motion of complex objects, which can lead to inaccurate and blurry\nrenderings of details. To address this limitation, we propose a novel approach\nthat builds upon a recent generalization NeRF, which aggregates nearby views\nonto new viewpoints. However, such methods are typically only effective for\nstatic scenes. To overcome this challenge, we introduce a module that operates\nin both the time and frequency domains to aggregate the features of object\nmotion. This allows us to learn the relationship between frames and generate\nhigher-quality images. Our experiments demonstrate significant improvements\nover state-of-the-art methods on dynamic scene datasets. Specifically, our\napproach outperforms existing methods in terms of both the accuracy and visual\nquality of the synthesized views. Our code is available on\nhttps://github.com/xingy038/CTNeRF.\n","authors":["Xingyu Miao","Yang Bai","Haoran Duan","Yawen Huang","Fan Wan","Yang Long","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2401.04861v2.pdf","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2406.18310v1","updated":"2024-06-26T12:50:10Z","published":"2024-06-26T12:50:10Z","title":"Spatial-temporal Hierarchical Reinforcement Learning for Interpretable\n  Pathology Image Super-Resolution","summary":"  Pathology image are essential for accurately interpreting lesion cells in\ncytopathology screening, but acquiring high-resolution digital slides requires\nspecialized equipment and long scanning times. Though super-resolution (SR)\ntechniques can alleviate this problem, existing deep learning models recover\npathology image in a black-box manner, which can lead to untruthful biological\ndetails and misdiagnosis. Additionally, current methods allocate the same\ncomputational resources to recover each pixel of pathology image, leading to\nthe sub-optimal recovery issue due to the large variation of pathology image.\nIn this paper, we propose the first hierarchical reinforcement learning\nframework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),\nmainly for addressing the aforementioned issues in pathology image\nsuper-resolution problem. We reformulate the SR problem as a Markov decision\nprocess of interpretable operations and adopt the hierarchical recovery\nmechanism in patch level, to avoid sub-optimal recovery. Specifically, the\nhigher-level spatial manager is proposed to pick out the most corrupted patch\nfor the lower-level patch worker. Moreover, the higher-level temporal manager\nis advanced to evaluate the selected patch and determine whether the\noptimization should be stopped earlier, thereby avoiding the over-processed\nproblem. Under the guidance of spatial-temporal managers, the lower-level patch\nworker processes the selected patch with pixel-wise interpretable actions at\neach time step. Experimental results on medical images degraded by different\nkernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the\npromotion in tumor diagnosis with a large margin and shows generalizability\nunder various degradations. The source code is available at\nhttps://github.com/CUHK-AIM-Group/STAR-RL.\n","authors":["Wenting Chen","Jie Liu","Tommy W. S. Chow","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18310v1.pdf","comment":"Accepted to IEEE TRANSACTIONS ON MEDICAL IMAGING (TMI)"},{"id":"http://arxiv.org/abs/2405.20204v2","updated":"2024-06-26T12:31:48Z","published":"2024-05-30T16:07:54Z","title":"Jina CLIP: Your CLIP Model Is Also Your Text Retriever","summary":"  Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.\n","authors":["Andreas Koukounas","Georgios Mastrapas","Michael Günther","Bo Wang","Scott Martens","Isabelle Mohr","Saba Sturua","Mohammad Kalim Akram","Joan Fontanals Martínez","Saahil Ognawala","Susana Guzman","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2405.20204v2.pdf","comment":"4 pages, MFM-EAI@ICML2024"},{"id":"http://arxiv.org/abs/2306.06210v5","updated":"2024-06-26T12:31:04Z","published":"2023-05-26T13:06:38Z","title":"Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion","summary":"  Recent breakthroughs in generative modeling have sparked interest in\npractical single-model attribution. Such methods predict whether a sample was\ngenerated by a specific generator or not, for instance, to prove intellectual\nproperty theft. However, previous works are either limited to the closed-world\nsetting or require undesirable changes to the generative model. We address\nthese shortcomings by, first, viewing single-model attribution through the lens\nof anomaly detection. Arising from this change of perspective, we propose\nFLIPAD, a new approach for single-model attribution in the open-world setting\nbased on final-layer inversion and anomaly detection. We show that the utilized\nfinal-layer inversion can be reduced to a convex lasso optimization problem,\nmaking our approach theoretically sound and computationally efficient. The\ntheoretical findings are accompanied by an experimental study demonstrating the\neffectiveness of our approach and its flexibility to various domains.\n","authors":["Mike Laszkiewicz","Jonas Ricker","Johannes Lederer","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2306.06210v5.pdf","comment":"Accepted at the Forty-first International Conference on Machine\n  Learning [ICML2024]"},{"id":"http://arxiv.org/abs/2406.18295v1","updated":"2024-06-26T12:27:06Z","published":"2024-06-26T12:27:06Z","title":"Evaluating and Benchmarking Foundation Models for Earth Observation and\n  Geospatial AI","summary":"  When we are primarily interested in solving several problems jointly with a\ngiven prescribed high performance accuracy for each target application, then\nFoundation Models should for most cases be used rather than problem-specific\nmodels. We focus on the specific Computer Vision application of Foundation\nModels for Earth Observation (EO) and geospatial AI. These models can solve\nimportant problems we are tackling, including for example land cover\nclassification, crop type mapping, flood segmentation, building density\nestimation, and road regression segmentation. In this paper, we show that for a\nlimited number of labelled data, Foundation Models achieve improved performance\ncompared to problem-specific models. In this work, we also present our proposed\nevaluation benchmark for Foundation Models for EO. Benchmarking the\ngeneralization performance of Foundation Models is important as it has become\ndifficult to standardize a fair comparison across the many different models\nthat have been proposed recently. We present the results using our evaluation\nbenchmark for EO Foundation Models and show that Foundation Models are label\nefficient in the downstream tasks and help us solve problems we are tackling in\nEO and remote sensing.\n","authors":["Nikolaos Dionelis","Casper Fibaek","Luke Camilleri","Andreas Luyts","Jente Bosmans","Bertrand Le Saux"],"pdf_url":"https://arxiv.org/pdf/2406.18295v1.pdf","comment":"5 pages, 2 figures, Submitted"},{"id":"http://arxiv.org/abs/2406.18284v1","updated":"2024-06-26T12:09:59Z","published":"2024-06-26T12:09:59Z","title":"RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network","summary":"  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n","authors":["Xiaozhong Ji","Chuming Lin","Zhonggan Ding","Ying Tai","Jian Yang","Junwei Zhu","Xiaobin Hu","Jiangning Zhang","Donghao Luo","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18279v1","updated":"2024-06-26T12:05:49Z","published":"2024-06-26T12:05:49Z","title":"CAS: Confidence Assessments of classification algorithms for Semantic\n  segmentation of EO data","summary":"  Confidence assessments of semantic segmentation algorithms in remote sensing\nare important. It is a desirable property of models to a priori know if they\nproduce an incorrect output. Evaluations of the confidence assigned to the\nestimates of models for the task of classification in Earth Observation (EO)\nare crucial as they can be used to achieve improved semantic segmentation\nperformance and prevent high error rates during inference and deployment. The\nmodel we develop, the Confidence Assessments of classification algorithms for\nSemantic segmentation (CAS) model, performs confidence evaluations at both the\nsegment and pixel levels, and outputs both labels and confidence. The outcome\nof this work has important applications. The main application is the evaluation\nof EO Foundation Models on semantic segmentation downstream tasks, in\nparticular land cover classification using satellite Copernicus Sentinel-2\ndata. The evaluation shows that the proposed model is effective and outperforms\nother alternative baseline models.\n","authors":["Nikolaos Dionelis","Nicolas Longepe"],"pdf_url":"https://arxiv.org/pdf/2406.18279v1.pdf","comment":"5 pages, 7 figures, 4 tables, Submitted"},{"id":"http://arxiv.org/abs/2406.18278v1","updated":"2024-06-26T12:04:09Z","published":"2024-06-26T12:04:09Z","title":"Generalized Deepfake Attribution","summary":"  The landscape of fake media creation changed with the introduction of\nGenerative Adversarial Networks (GAN s). Fake media creation has been on the\nrise with the rapid advances in generation technology, leading to new\nchallenges in Detecting fake media. A fundamental characteristic of GAN s is\ntheir sensitivity to parameter initialization, known as seeds. Each distinct\nseed utilized during training leads to the creation of unique model instances,\nresulting in divergent image outputs despite employing the same architecture.\nThis means that even if we have one GAN architecture, it can produce countless\nvariations of GAN models depending on the seed used. Existing methods for\nattributing deepfakes work well only if they have seen the specific GAN model\nduring training. If the GAN architectures are retrained with a different seed,\nthese methods struggle to attribute the fakes. This seed dependency issue made\nit difficult to attribute deepfakes with existing methods. We proposed a\ngeneralized deepfake attribution network (GDA-N et) to attribute fake images to\ntheir respective GAN architectures, even if they are generated from a retrained\nversion of the GAN architecture with a different seed (cross-seed) or from the\nfine-tuned version of the existing GAN model. Extensive experiments on\ncross-seed and fine-tuned data of GAN models show that our method is highly\neffective compared to existing methods. We have provided the source code to\nvalidate our results.\n","authors":["Sowdagar Mahammad Shahid","Sudev Kumar Padhi","Umesh Kashyap","Sk. Subidh Ali"],"pdf_url":"https://arxiv.org/pdf/2406.18278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02311v2","updated":"2024-06-26T11:14:21Z","published":"2024-03-04T18:47:56Z","title":"Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications\n  to Cardiac MRI Segmentation","summary":"  Deep learning (DL)-based methods have achieved state-of-the-art performance\nfor a wide range of medical image segmentation tasks. Nevertheless, recent\nstudies show that deep neural networks (DNNs) can be miscalibrated and\noverconfident, leading to \"silent failures\" that are risky} for clinical\napplications. Bayesian statistics provide an intuitive approach to DL failure\ndetection, based on posterior probability estimation. However, Bayesian DL, and\nin particular the posterior estimation, is intractable for large medical image\nsegmentation DNNs. To tackle this challenge, we propose a Bayesian learning\nframework by Hamiltonian Monte Carlo (HMC), tempered by cold posterior (CP) to\naccommodate medical data augmentation, named HMC-CP. For HMC computation, we\nfurther propose a cyclical annealing strategy, which captures both local and\nglobal geometries of the posterior distribution, enabling highly efficient\nBayesian DNN training with the same computational budget requirements as\ntraining a single DNN. The resulting Bayesian DNN outputs an ensemble\nsegmentation along with the segmentation uncertainty. We evaluate the proposed\nHMC-CP extensively on cardiac magnetic resonance image (MRI) segmentation,\nusing in-domain steady-state free precession (SSFP) cine images as well as\nout-of-domain datasets of quantitative $T_1$ and $T_2$ mapping.\n","authors":["Yidong Zhao","Joao Tourais","Iain Pierce","Christian Nitsche","Thomas A. Treibel","Sebastian Weingärtner","Artur M. Schweidtmann","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2403.02311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17639v2","updated":"2024-06-26T10:58:48Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18253v1","updated":"2024-06-26T10:57:52Z","published":"2024-06-26T10:57:52Z","title":"On the Role of Visual Grounding in VQA","summary":"  Visual Grounding (VG) in VQA refers to a model's proclivity to infer answers\nbased on question-relevant image regions. Conceptually, VG identifies as an\naxiomatic requirement of the VQA task. In practice, however, DNN-based VQA\nmodels are notorious for bypassing VG by way of shortcut (SC) learning without\nsuffering obvious performance losses in standard benchmarks. To uncover the\nimpact of SC learning, Out-of-Distribution (OOD) tests have been proposed that\nexpose a lack of VG with low accuracy. These tests have since been at the\ncenter of VG research and served as basis for various investigations into VG's\nimpact on accuracy. However, the role of VG in VQA still remains not fully\nunderstood and has not yet been properly formalized.\n  In this work, we seek to clarify VG's role in VQA by formalizing it on a\nconceptual level. We propose a novel theoretical framework called \"Visually\nGrounded Reasoning\" (VGR) that uses the concepts of VG and Reasoning to\ndescribe VQA inference in ideal OOD testing. By consolidating fundamental\ninsights into VG's role in VQA, VGR helps to reveal rampant VG-related SC\nexploitation in OOD testing, which explains why the relationship between VG and\nOOD accuracy has been difficult to define. Finally, we propose an approach to\ncreate OOD tests that properly emphasize a requirement for VG, and show how to\nimprove performance on them.\n","authors":["Daniel Reich","Tanja Schultz"],"pdf_url":"https://arxiv.org/pdf/2406.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16493v3","updated":"2024-06-26T10:51:51Z","published":"2024-04-25T10:38:33Z","title":"Commonsense Prototype for Outdoor Unsupervised 3D Object Detection","summary":"  The prevalent approaches of unsupervised 3D object detection follow\ncluster-based pseudo-label generation and iterative self-training processes.\nHowever, the challenge arises due to the sparsity of LiDAR scans, which leads\nto pseudo-labels with erroneous size and position, resulting in subpar\ndetection performance. To tackle this problem, this paper introduces a\nCommonsense Prototype-based Detector, termed CPD, for unsupervised 3D object\ndetection. CPD first constructs Commonsense Prototype (CProto) characterized by\nhigh-quality bounding box and dense points, based on commonsense intuition.\nSubsequently, CPD refines the low-quality pseudo-labels by leveraging the size\nprior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely\nscanned objects by the geometric knowledge from CProto. CPD outperforms\nstate-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD),\nPandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD\nand testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on\neasy and moderate car classes, respectively. These achievements position CPD in\nclose proximity to fully supervised detectors, highlighting the significance of\nour method. The code will be available at https://github.com/hailanyi/CPD.\n","authors":["Hai Wu","Shijia Zhao","Xun Huang","Chenglu Wen","Xin Li","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.16493v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2406.18249v1","updated":"2024-06-26T10:51:44Z","published":"2024-06-26T10:51:44Z","title":"Foundational Models for Pathology and Endoscopy Images: Application for\n  Gastric Inflammation","summary":"  The integration of artificial intelligence (AI) in medical diagnostics\nrepresents a significant advancement in managing upper gastrointestinal (GI)\ncancer, a major cause of global cancer mortality. Specifically for gastric\ncancer (GC), chronic inflammation causes changes in the mucosa such as atrophy,\nintestinal metaplasia (IM), dysplasia and ultimately cancer. Early detection\nthrough endoscopic regular surveillance is essential for better outcomes.\nFoundation models (FM), which are machine or deep learning models trained on\ndiverse data and applicable to broad use cases, offer a promising solution to\nenhance the accuracy of endoscopy and its subsequent pathology image analysis.\nThis review explores the recent advancements, applications, and challenges\nassociated with FM in endoscopy and pathology imaging. We started by\nelucidating the core principles and architectures underlying these models,\nincluding their training methodologies and the pivotal role of large-scale data\nin developing their predictive capabilities. Moreover, this work discusses\nemerging trends and future research directions, emphasizing the integration of\nmultimodal data, the development of more robust and equitable models, and the\npotential for real-time diagnostic support. This review aims to provide a\nroadmap for researchers and practitioners in navigating the complexities of\nincorporating FM into clinical practice for prevention/management of GC cases,\nthereby improving patient outcomes.\n","authors":["Hamideh Kerdegari","Kyle Higgins","Dennis Veselkov","Ivan Laponogov","Inese Polaka","Miguel Coimbra","Junior Andrea Pescino","Marcis Leja","Mario Dinis-Ribeiro","Tania Fleitas Kanonnikoff","Kirill Veselkov"],"pdf_url":"https://arxiv.org/pdf/2406.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18247v1","updated":"2024-06-26T10:49:26Z","published":"2024-06-26T10:49:26Z","title":"Generative artificial intelligence in ophthalmology: multimodal retinal\n  images for the diagnosis of Alzheimer's disease with convolutional neural\n  networks","summary":"  Background/Aim. This study aims to predict Amyloid Positron Emission\nTomography (AmyloidPET) status with multimodal retinal imaging and\nconvolutional neural networks (CNNs) and to improve the performance through\npretraining with synthetic data. Methods. Fundus autofluorescence, optical\ncoherence tomography (OCT), and OCT angiography images from 328 eyes of 59\nAmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for\nclassification. Denoising Diffusion Probabilistic Models (DDPMs) were trained\nto generate synthetic images and unimodal CNNs were pretrained on synthetic\ndata and finetuned on real data or trained solely on real data. Multimodal\nclassifiers were developed to combine predictions of the four unimodal CNNs\nwith patient metadata. Class activation maps of the unimodal classifiers\nprovided insight into the network's attention to inputs. Results. DDPMs\ngenerated diverse, realistic images without memorization. Pretraining unimodal\nCNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration\nof metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the\nbest overall best classifier. Class activation maps highlighted relevant\nretinal regions which correlated with AD. Conclusion. Our method for generating\nand leveraging synthetic data has the potential to improve AmyloidPET\nprediction from multimodal retinal imaging. A DDPM can generate realistic and\nunique multimodal synthetic retinal images. Our best performing unimodal and\nmultimodal classifiers were not pretrained on synthetic data, however\npretraining with synthetic data slightly improved classification performance\nfor two out of the four modalities.\n","authors":["I. R. Slootweg","M. Thach","K. R. Curro-Tafili","F. D. Verbraak","F. H. Bouwman","Y. A. L. Pijnenburg","J. F. Boer","J. H. P. de Kwisthout","L. Bagheriye","P. J. González"],"pdf_url":"https://arxiv.org/pdf/2406.18247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18242v1","updated":"2024-06-26T10:46:44Z","published":"2024-06-26T10:46:44Z","title":"ConStyle v2: A Strong Prompter for All-in-One Image Restoration","summary":"  This paper introduces ConStyle v2, a strong plug-and-play prompter designed\nto output clean visual prompts and assist U-Net Image Restoration models in\nhandling multiple degradations. The joint training process of IRConStyle, an\nImage Restoration framework consisting of ConStyle and a general restoration\nnetwork, is divided into two stages: first, pre-training ConStyle alone, and\nthen freezing its weights to guide the training of the general restoration\nnetwork. Three improvements are proposed in the pre-training stage to train\nConStyle: unsupervised pre-training, adding a pretext task (i.e.\nclassification), and adopting knowledge distillation. Without bells and\nwhistles, we can get ConStyle v2, a strong prompter for all-in-one Image\nRestoration, in less than two GPU days and doesn't require any fine-tuning.\nExtensive experiments on Restormer (transformer-based), NAFNet (CNN-based),\nMAXIM-1S (MLP-based), and a vanilla CNN network demonstrate that ConStyle v2\ncan enhance any U-Net style Image Restoration models to all-in-one Image\nRestoration models. Furthermore, models guided by the well-trained ConStyle v2\nexhibit superior performance in some specific degradation compared to ConStyle.\n","authors":["Dongqi Fan","Junhao Zhang","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2406.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05746v3","updated":"2024-06-26T10:44:58Z","published":"2024-02-08T15:26:28Z","title":"Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents","summary":"  Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.\n","authors":["Yuxi Wei","Zi Wang","Yifan Lu","Chenxin Xu","Changxing Liu","Hao Zhao","Siheng Chen","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05746v3.pdf","comment":"CVPR 2024(Highlight)"},{"id":"http://arxiv.org/abs/2406.18240v1","updated":"2024-06-26T10:44:48Z","published":"2024-06-26T10:44:48Z","title":"Concordance in basal cell carcinoma diagnosis. Building a proper ground\n  truth to train Artificial Intelligence tools","summary":"  Background: The existence of different basal cell carcinoma (BCC) clinical\ncriteria cannot be objectively validated. An adequate ground-truth is needed to\ntrain an artificial intelligence (AI) tool that explains the BCC diagnosis by\nproviding its dermoscopic features. Objectives: To determine the consensus\namong dermatologists on dermoscopic criteria of 204 BCC. To analyze the\nperformance of an AI tool when the ground-truth is inferred. Methods: A single\ncenter, diagnostic and prospective study was conducted to analyze the agreement\nin dermoscopic criteria by four dermatologists and then derive a reference\nstandard. 1434 dermoscopic images have been used, that were taken by a primary\nhealth physician, sent via teledermatology, and diagnosed by a dermatologist.\nThey were randomly selected from the teledermatology platform (2019-2021). 204\nof them were tested with an AI tool; the remainder trained it. The performance\nof the AI tool trained using the ground-truth of one dermatologist versus the\nground-truth statistically inferred from the consensus of four dermatologists\nwas analyzed using McNemar's test and Hamming distance. Results: Dermatologists\nachieve perfect agreement in the diagnosis of BCC (Fleiss-Kappa=0.9079), and a\nhigh correlation with the biopsy (PPV=0.9670). However, there is low agreement\nin detecting some dermoscopic criteria. Statistical differences were found in\nthe performance of the AI tool trained using the ground-truth of one\ndermatologist versus the ground-truth statistically inferred from the consensus\nof four dermatologists. Conclusions: Care should be taken when training an AI\ntool to determine the BCC patterns present in a lesion. Ground-truth should be\nestablished from multiple dermatologists.\n","authors":["Francisca Silva-Clavería","Carmen Serrano","Iván Matas","Amalia Serrano","Tomás Toledo-Pastrana","David Moreno-Ramírez","Begoña Acha"],"pdf_url":"https://arxiv.org/pdf/2406.18240v1.pdf","comment":"Manuscript word count: 3000, Number of figures: 2, Number of tables:\n  3"},{"id":"http://arxiv.org/abs/2404.03425v5","updated":"2024-06-26T10:38:29Z","published":"2024-04-04T13:06:25Z","title":"ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model","summary":"  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings: CNN are constrained by a limited\nreceptive field that may hinder their ability to capture broader spatial\ncontexts, while Transformers are computationally intensive, making them costly\nto train and deploy on large datasets. Recently, the Mamba architecture, based\non state space models, has shown remarkable performance in a series of natural\nlanguage processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n","authors":["Hongruixuan Chen","Jian Song","Chengxi Han","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2404.03425v5.pdf","comment":"Accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2406.18236v1","updated":"2024-06-26T10:37:02Z","published":"2024-06-26T10:37:02Z","title":"CoDA: Interactive Segmentation and Morphological Analysis of Dendroid\n  Structures Exemplified on Stony Cold-Water Corals","summary":"  Herein, we present CoDA, the Coral Dendroid structure Analyzer, a visual\nanalytics suite that allows for the first time to investigate the ontogenetic\nmorphological development of complex dendroid coral colonies, exemplified on\nthree important framework-forming dendroid cold-water corals: Lophelia pertusa\n(Linnaeus, 1758), Madrepora oculata (Linnaeus, 1758), and Goniocorella dumosa\n(Alcock, 1902). Input to CoDA is an initial instance segmentation of the coral\npolyp cavities (calices), from which it estimates the skeleton tree of the\ncolony and extracts classical morphological measurements and advanced shape\nfeatures of the individual corallites. CoDA also works as a proofreading and\nerror correction tool by helping to identify wrong parts in the skeleton tree\nand providing tools to quickly correct these errors. The final skeleton tree\nenables the derivation of additional information about the calices/corallite\ninstances that otherwise could not be obtained, including their ontogenetic\ngeneration and branching patterns - the basis of a fully quantitative\nstatistical analysis of the coral colony morphology. Part of CoDA is CoDAGraph,\na feature-rich link-and-brush user interface for visualizing the extracted\nfeatures and 2D graph layouts of the skeleton tree, enabling the real-time\nexploration of complex coral colonies and their building blocks, the individual\ncorallites and branches.\n  In the future, we expect CoDA to greatly facilitate the analysis of large\nstony corals of different species and morphotypes, as well as other dendroid\nstructures, enabling new insights into the influence of genetic and\nenvironmental factors on their ontogenetic morphological development.\n","authors":["Kira Schmitt","Jürgen Titschack","Daniel Baum"],"pdf_url":"https://arxiv.org/pdf/2406.18236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02674v3","updated":"2024-06-26T10:31:54Z","published":"2023-10-04T09:26:44Z","title":"ObjFormer: Learning Land-Cover Changes From Paired OSM Data and Optical\n  High-Resolution Imagery via Object-Guided Transformer","summary":"  Optical high-resolution imagery and OSM data are two important data sources\nof change detection (CD). Previous related studies focus on utilizing the\ninformation in OSM data to aid the CD on optical high-resolution images. This\npaper pioneers the direct detection of land-cover changes utilizing paired OSM\ndata and optical imagery, thereby expanding the scope of CD tasks. To this end,\nwe propose an object-guided Transformer (ObjFormer) by naturally combining the\nobject-based image analysis (OBIA) technique with the advanced vision\nTransformer architecture. This combination can significantly reduce the\ncomputational overhead in the self-attention module without adding extra\nparameters or layers. ObjFormer has a hierarchical pseudo-siamese encoder\nconsisting of object-guided self-attention modules that extracts multi-level\nheterogeneous features from OSM data and optical images; a decoder consisting\nof object-guided cross-attention modules can recover land-cover changes from\nthe extracted heterogeneous features. Beyond basic binary change detection,\nthis paper raises a new semi-supervised semantic change detection task that\ndoes not require any manually annotated land-cover labels to train semantic\nchange detectors. Two lightweight semantic decoders are added to ObjFormer to\naccomplish this task efficiently. A converse cross-entropy loss is designed to\nfully utilize negative samples, contributing to the great performance\nimprovement in this task. A large-scale benchmark dataset called OpenMapCD\ncontaining 1,287 samples covering 40 regions on six continents is constructed\nto conduct detailed experiments. The results show the effectiveness of our\nmethods in this new kind of CD task. Additionally, case studies in Japanese\ncities demonstrate the framework's generalizability and practical potential.\nThe OpenMapCD and source code are available in\nhttps://github.com/ChenHongruixuan/ObjFormer\n","authors":["Hongruixuan Chen","Cuiling Lan","Jian Song","Clifford Broni-Bediako","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2310.02674v3.pdf","comment":"Accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2406.18227v1","updated":"2024-06-26T10:24:00Z","published":"2024-06-26T10:24:00Z","title":"GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension","summary":"  There are substantial instructional videos on the Internet, which provide us\ntutorials for completing various tasks. Existing instructional video datasets\nonly focus on specific steps at the video level, lacking experiential\nguidelines at the task level, which can lead to beginners struggling to learn\nnew tasks due to the lack of relevant experience. Moreover, the specific steps\nwithout guidelines are trivial and unsystematic, making it difficult to provide\na clear tutorial. To address these problems, we present the GUIDE\n(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional\ntasks in 8 domains related to our daily life. Specifically, we annotate each\ninstructional task with a guideline, representing a common pattern shared by\nall task-related videos. On this basis, we annotate systematic specific steps,\nincluding their associated guideline steps, specific step descriptions and\ntimestamps. Our proposed benchmark consists of three sub-tasks to evaluate\ncomprehension ability of models: (1) Step Captioning: models have to generate\ncaptions for specific steps from videos. (2) Guideline Summarization: models\nhave to mine the common pattern in task-related videos and summarize a\nguideline from them. (3) Guideline-Guided Captioning: models have to generate\ncaptions for specific steps under the guide of guideline. We evaluate plenty of\nfoundation models with GUIDE and perform in-depth analysis. Given the diversity\nand practicality of GUIDE, we believe that it can be used as a better benchmark\nfor instructional video comprehension.\n","authors":["Jiafeng Liang","Shixin Jiang","Zekun Wang","Haojie Pan","Zerui Chen","Zheng Chu","Ming Liu","Ruiji Fu","Zhongyuan Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.18227v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.13870v2","updated":"2024-06-26T10:20:11Z","published":"2024-06-19T22:20:03Z","title":"Splatter a Video: Video Gaussian Representation for Versatile Processing","summary":"  Video representation is a long-standing problem that is crucial for various\ndown-stream tasks, such as tracking,depth prediction,segmentation,view\nsynthesis,and editing. However, current methods either struggle to model\ncomplex motions due to the absence of 3D structure or rely on implicit 3D\nrepresentations that are ill-suited for manipulation tasks. To address these\nchallenges, we introduce a novel explicit 3D representation-video Gaussian\nrepresentation -- that embeds a video into 3D Gaussians. Our proposed\nrepresentation models video appearance in a 3D canonical space using explicit\nGaussians as proxies and associates each Gaussian with 3D motions for video\nmotion. This approach offers a more intrinsic and explicit representation than\nlayered atlas or volumetric pixel matrices. To obtain such a representation, we\ndistill 2D priors, such as optical flow and depth, from foundation models to\nregularize learning in this ill-posed setting. Extensive applications\ndemonstrate the versatility of our new video representation. It has been proven\neffective in numerous video processing tasks, including tracking, consistent\nvideo depth and feature refinement, motion and appearance editing, and\nstereoscopic video generation. Project page:\nhttps://sunyangtian.github.io/spatter_a_video_web/\n","authors":["Yang-Tian Sun","Yi-Hua Huang","Lin Ma","Xiaoyang Lyu","Yan-Pei Cao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.13870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02000v3","updated":"2024-06-26T10:17:08Z","published":"2022-09-05T14:57:03Z","title":"Visual Odometry with Neuromorphic Resonator Networks","summary":"  Visual Odometry (VO) is a method to estimate self-motion of a mobile robot\nusing visual sensors. Unlike odometry based on integrating differential\nmeasurements that can accumulate errors, such as inertial sensors or wheel\nencoders, visual odometry is not compromised by drift. However, image-based VO\nis computationally demanding, limiting its application in use cases with\nlow-latency, -memory, and -energy requirements. Neuromorphic hardware offers\nlow-power solutions to many vision and AI problems, but designing such\nsolutions is complicated and often has to be assembled from scratch. Here we\npropose to use Vector Symbolic Architecture (VSA) as an abstraction layer to\ndesign algorithms compatible with neuromorphic hardware. Building from a VSA\nmodel for scene analysis, described in our companion paper, we present a\nmodular neuromorphic algorithm that achieves state-of-the-art performance on\ntwo-dimensional VO tasks. Specifically, the proposed algorithm stores and\nupdates a working memory of the presented visual environment. Based on this\nworking memory, a resonator network estimates the changing location and\norientation of the camera. We experimentally validate the neuromorphic\nVSA-based approach to VO with two benchmarks: one based on an event camera\ndataset and the other in a dynamic scene with a robotic task.\n","authors":["Alpha Renner","Lazar Supic","Andreea Danielescu","Giacomo Indiveri","E. Paxon Frady","Friedrich T. Sommer","Yulia Sandamirskaya"],"pdf_url":"https://arxiv.org/pdf/2209.02000v3.pdf","comment":"19 pages, 5 figures, minor revisions, added results for\n  shapes_translation dataset"},{"id":"http://arxiv.org/abs/2208.12880v4","updated":"2024-06-26T10:16:08Z","published":"2022-08-26T22:17:52Z","title":"Neuromorphic Visual Scene Understanding with Resonator Networks","summary":"  Analyzing a visual scene by inferring the configuration of a generative model\nis widely considered the most flexible and generalizable approach to scene\nunderstanding. Yet, one major problem is the computational challenge of the\ninference procedure, involving a combinatorial search across object identities\nand poses. Here we propose a neuromorphic solution exploiting three key\nconcepts: (1) a computational framework based on Vector Symbolic Architectures\n(VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator\nNetworks (HRN) to factorize the non-commutative transforms translation and\nrotation in visual scenes; (3) the design of a multi-compartment spiking phasor\nneuron model for implementing complex-valued resonator networks on neuromorphic\nhardware. The VSA framework uses vector binding operations to form a generative\nimage model in which binding acts as the equivariant operation for geometric\ntransformations. A scene can, therefore, be described as a sum of vector\nproducts, which can then be efficiently factorized by a resonator network to\ninfer objects and their poses. The HRN features a partitioned architecture in\nwhich vector binding is equivariant for horizontal and vertical translation\nwithin one partition and for rotation and scaling within the other partition.\nThe spiking neuron model allows mapping the resonator network onto efficient\nand low-power neuromorphic hardware. Our approach is demonstrated on synthetic\nscenes composed of simple 2D shapes undergoing rigid geometric transformations\nand color changes. A companion paper demonstrates the same approach in\nreal-world application scenarios for machine vision and robotics.\n","authors":["Alpha Renner","Lazar Supic","Andreea Danielescu","Giacomo Indiveri","Bruno A. Olshausen","Yulia Sandamirskaya","Friedrich T. Sommer","E. Paxon Frady"],"pdf_url":"https://arxiv.org/pdf/2208.12880v4.pdf","comment":"23 pages, 8 figures, minor revisions and extended supplementary\n  material"},{"id":"http://arxiv.org/abs/2406.18220v1","updated":"2024-06-26T10:08:24Z","published":"2024-06-26T10:08:24Z","title":"Guiding Video Prediction with Explicit Procedural Knowledge","summary":"  We propose a general way to integrate procedural knowledge of a domain into\ndeep learning models. We apply it to the case of video prediction, building on\ntop of object-centric deep models and show that this leads to a better\nperformance than using data-driven models alone. We develop an architecture\nthat facilitates latent space disentanglement in order to use the integrated\nprocedural knowledge, and establish a setup that allows the model to learn the\nprocedural interface in the latent space using the downstream task of video\nprediction. We contrast the performance to a state-of-the-art data-driven\napproach and show that problems where purely data-driven approaches struggle\ncan be handled by using knowledge about the domain, providing an alternative to\nsimply collecting more data.\n","authors":["Patrick Takenaka","Johannes Maucher","Marco F. Huber"],"pdf_url":"https://arxiv.org/pdf/2406.18220v1.pdf","comment":"Published in 2023 IEEE/CVF International Conference on Computer\n  Vision Workshops (ICCVW)"},{"id":"http://arxiv.org/abs/2406.18215v1","updated":"2024-06-26T09:58:05Z","published":"2024-06-26T09:58:05Z","title":"Unlocking the Potential of Operations Research for Multi-Graph Matching","summary":"  We consider the incomplete multi-graph matching problem, which is a\ngeneralization of the NP-hard quadratic assignment problem for matching\nmultiple finite sets. Multi-graph matching plays a central role in computer\nvision, e.g., for matching images or shapes, so that a number of dedicated\noptimization techniques have been proposed. While the closely related NP-hard\nmulti-dimensional assignment problem (MDAP) has been studied for decades in the\noperations research community, it only considers complete matchings and has a\ndifferent cost structure. We bridge this gap and transfer well-known\napproximation algorithms for the MDAP to incomplete multi-graph matching. To\nthis end, we revisit respective algorithms, adapt them to incomplete\nmulti-graph matching, and propose their extended and parallelized versions. Our\nexperimental validation shows that our new method substantially outperforms the\nprevious state of the art in terms of objective and runtime. Our algorithm\nmatches, for example, 29 images with more than 500 keypoints each in less than\ntwo minutes, whereas the fastest considered competitor requires at least half\nan hour while producing far worse results.\n","authors":["Max Kahl","Sebastian Stricker","Lisa Hutschenreiter","Florian Bernard","Bogdan Savchynskyy"],"pdf_url":"https://arxiv.org/pdf/2406.18215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18214v1","updated":"2024-06-26T09:57:55Z","published":"2024-06-26T09:57:55Z","title":"Trimming the Fat: Efficient Compression of 3D Gaussian Splats through\n  Pruning","summary":"  In recent times, the utilization of 3D models has gained traction, owing to\nthe capacity for end-to-end training initially offered by Neural Radiance\nFields and more recently by 3D Gaussian Splatting (3DGS) models. The latter\nholds a significant advantage by inherently easing rapid convergence during\ntraining and offering extensive editability. However, despite rapid\nadvancements, the literature still lives in its infancy regarding the\nscalability of these models. In this study, we take some initial steps in\naddressing this gap, showing an approach that enables both the memory and\ncomputational scalability of such models. Specifically, we propose \"Trimming\nthe fat\", a post-hoc gradient-informed iterative pruning technique to eliminate\nredundant information encoded in the model. Our experimental findings on widely\nacknowledged benchmarks attest to the effectiveness of our approach, revealing\nthat up to 75% of the Gaussians can be removed while maintaining or even\nimproving upon baseline performance. Our approach achieves around 50$\\times$\ncompression while preserving performance similar to the baseline model, and is\nable to speed-up computation up to 600~FPS.\n","authors":["Muhammad Salman Ali","Maryam Qamar","Sung-Ho Bae","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2406.18214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18212v1","updated":"2024-06-26T09:56:29Z","published":"2024-06-26T09:56:29Z","title":"Joint Stream: Malignant Region Learning for Breast Cancer Diagnosis","summary":"  Early diagnosis of breast cancer (BC) significantly contributes to reducing\nthe mortality rate worldwide. The detection of different factors and biomarkers\nsuch as Estrogen receptor (ER), Progesterone receptor (PR), Human epidermal\ngrowth factor receptor 2 (HER2) gene, Histological grade (HG), Auxiliary lymph\nnode (ALN) status, and Molecular subtype (MS) can play a significant role in\nimproved BC diagnosis. However, the existing methods predict only a single\nfactor which makes them less suitable to use in diagnosis and designing a\nstrategy for treatment. In this paper, we propose to classify the six essential\nindicating factors (ER, PR, HER2, ALN, HG, MS) for early BC diagnosis using\nH\\&E stained WSI's. To precisely capture local neighboring relationships, we\nuse spatial and frequency domain information from the large patch size of WSI's\nmalignant regions. Furthermore, to cater the variable number of regions of\ninterest sizes and give due attention to each region, we propose a malignant\nregion learning attention network. Our experimental results demonstrate that\ncombining spatial and frequency information using the malignant region learning\nmodule significantly improves multi-factor and single-factor classification\nperformance on publicly available datasets.\n","authors":["Abdul Rehman","Sarfaraz Hussein","Waqas Sultani"],"pdf_url":"https://arxiv.org/pdf/2406.18212v1.pdf","comment":"Under Review (Biomedical Signal Processing and Control)"},{"id":"http://arxiv.org/abs/2406.17536v2","updated":"2024-06-26T09:52:47Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18201v1","updated":"2024-06-26T09:33:51Z","published":"2024-06-26T09:33:51Z","title":"EFCNet: Every Feature Counts for Small Medical Object Segmentation","summary":"  This paper explores the segmentation of very small medical objects with\nsignificant clinical value. While Convolutional Neural Networks (CNNs),\nparticularly UNet-like models, and recent Transformers have shown substantial\nprogress in image segmentation, our empirical findings reveal their poor\nperformance in segmenting the small medical objects and lesions concerned in\nthis paper. This limitation may be attributed to information loss during their\nencoding and decoding process. In response to this challenge, we propose a\nnovel model named EFCNet for small object segmentation in medical images. Our\nmodel incorporates two modules: the Cross-Stage Axial Attention Module (CSAA)\nand the Multi-Precision Supervision Module (MPS). These modules address\ninformation loss during encoding and decoding procedures, respectively.\nSpecifically, CSAA integrates features from all stages of the encoder to\nadaptively learn suitable information needed in different decoding stages,\nthereby reducing information loss in the encoder. On the other hand, MPS\nintroduces a novel multi-precision supervision mechanism to the decoder. This\nmechanism prioritizes attention to low-resolution features in the initial\nstages of the decoder, mitigating information loss caused by subsequent\nconvolution and sampling processes and enhancing the model's global perception.\nWe evaluate our model on two benchmark medical image datasets. The results\ndemonstrate that EFCNet significantly outperforms previous segmentation methods\ndesigned for both medical and normal images.\n","authors":["Lingjie Kong","Qiaoling Wei","Chengming Xu","Han Chen","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2406.18201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18199v1","updated":"2024-06-26T09:29:56Z","published":"2024-06-26T09:29:56Z","title":"GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D\n  Reconstruction Under Strong Lighting","summary":"  The 3D Gaussian Splatting technique has significantly advanced the\nconstruction of radiance fields from multi-view images, enabling real-time\nrendering. While point-based rasterization effectively reduces computational\ndemands for rendering, it often struggles to accurately reconstruct the\ngeometry of the target object, especially under strong lighting. To address\nthis challenge, we introduce a novel approach that combines octree-based\nimplicit surface representations with Gaussian splatting. Our method consists\nof four stages. Initially, it reconstructs a signed distance field (SDF) and a\nradiance field through volume rendering, encoding them in a low-resolution\noctree. The initial SDF represents the coarse geometry of the target object.\nSubsequently, it introduces 3D Gaussians as additional degrees of freedom,\nwhich are guided by the SDF. In the third stage, the optimized Gaussians\nfurther improve the accuracy of the SDF, allowing it to recover finer geometric\ndetails compared to the initial SDF obtained in the first stage. Finally, it\nadopts the refined SDF to further optimize the 3D Gaussians via splatting,\neliminating those that contribute little to visual appearance. Experimental\nresults show that our method, which leverages the distribution of 3D Gaussians\nwith SDFs, reconstructs more accurate geometry, particularly in images with\nspecular highlights caused by strong lighting.\n","authors":["Jiaze Li","Zhengyu Wen","Luo Zhang","Jiangbei Hu","Fei Hou","Zhebin Zhang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2406.18199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18198v1","updated":"2024-06-26T09:29:21Z","published":"2024-06-26T09:29:21Z","title":"VDG: Vision-Only Dynamic Gaussian for Driving Simulation","summary":"  Dynamic Gaussian splatting has led to impressive scene reconstruction and\nimage synthesis advances in novel views. Existing methods, however, heavily\nrely on pre-computed poses and Gaussian initialization by Structure from Motion\n(SfM) algorithms or expensive sensors. For the first time, this paper addresses\nthis issue by integrating self-supervised VO into our pose-free dynamic\nGaussian method (VDG) to boost pose and depth initialization and static-dynamic\ndecomposition. Moreover, VDG can work with only RGB image input and construct\ndynamic scenes at a faster speed and larger scenes compared with the pose-free\ndynamic view-synthesis method. We demonstrate the robustness of our approach\nvia extensive quantitative and qualitative experiments. Our results show\nfavorable performance over the state-of-the-art dynamic view synthesis methods.\nAdditional video and source code will be posted on our project page at\nhttps://3d-aigc.github.io/VDG.\n","authors":["Hao Li","Jingfeng Li","Dingwen Zhang","Chenming Wu","Jieqi Shi","Chen Zhao","Haocheng Feng","Errui Ding","Jingdong Wang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2406.18198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18197v1","updated":"2024-06-26T09:29:05Z","published":"2024-06-26T09:29:05Z","title":"Human-free Prompted Based Anomaly Detection: prompt optimization with\n  Meta-guiding prompt scheme","summary":"  Pre-trained vision-language models (VLMs) are highly adaptable to various\ndownstream tasks through few-shot learning, making prompt-based anomaly\ndetection a promising approach. Traditional methods depend on human-crafted\nprompts that require prior knowledge of specific anomaly types. Our goal is to\ndevelop a human-free prompt-based anomaly detection framework that optimally\nlearns prompts through data-driven methods, eliminating the need for human\nintervention. The primary challenge in this approach is the lack of anomalous\nsamples during the training phase. Additionally, the Vision Transformer\n(ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly\nsegmentation due to a locality feature mismatch between the original image and\nthe output feature map. To tackle the first challenge, we have developed the\nObject-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples\nfor training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS)\niteratively adjusts the gradient-based optimization direction of learnable\nprompts to avoid overfitting to the synthesized anomalies. For the second\nchallenge, we propose Locality-Aware Attention, which ensures that each local\npatch feature attends only to nearby patch features, preserving the locality\nfeatures corresponding to their original locations. This framework allows for\nthe optimal prompt embeddings by searching in the continuous latent space via\nbackpropagation, free from human semantic constraints. Additionally, the\nmodified locality-aware attention improves the precision of pixel-wise anomaly\nsegmentation.\n","authors":["Pi-Wei Chen","Jerry Chun-Wei Lin","Jia Ji","Feng-Hao Yeh","Chao-Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18193v1","updated":"2024-06-26T09:17:27Z","published":"2024-06-26T09:17:27Z","title":"MammothModa: Multi-Modal Large Language Model","summary":"  In this report, we introduce MammothModa, yet another multi-modal large\nlanguage model (MLLM) designed to achieve state-of-the-art performance starting\nfrom an elementary baseline. We focus on three key design insights: (i)\nIntegrating Visual Capabilities while Maintaining Complex Language\nUnderstanding: In addition to the vision encoder, we incorporated the Visual\nAttention Experts into the LLM to enhance its visual capabilities. (ii)\nExtending Context Window for High-Resolution and Long-Duration Visual Feature:\nWe explore the Visual Merger Module to effectively reduce the token number of\nhigh-resolution images and incorporated frame position ids to avoid position\ninterpolation. (iii) High-Quality Bilingual Datasets: We meticulously curated\nand filtered a high-quality bilingual multimodal dataset to reduce visual\nhallucinations. With above recipe we build MammothModa that consistently\noutperforms the state-of-the-art models, e.g., LLaVA-series, across main\nreal-world visual language benchmarks without bells and whistles.\n","authors":["Qi She","Junwen Pan","Xin Wan","Rui Zhang","Dawei Lu","Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18193v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.16901v2","updated":"2024-06-26T08:54:40Z","published":"2024-05-31T15:17:12Z","title":"ECGrecover: a Deep Learning Approach for Electrocardiogram Signal\n  Completion","summary":"  In this work, we address the challenge of reconstructing the complete 12-lead\nECG signal from incomplete parts of it. We focus on two main scenarii: (i)\nreconstructing missing signal segments within an ECG lead and (ii) recovering\nmissing leads from a single-lead. We propose a model with a U-Net architecture\ntrained on a novel objective function to address the reconstruction problem.\nThis function incorporates both spatial and temporal aspects of the ECG by\ncombining the distance in amplitude between the reconstructed and real signals\nwith the signal trend. Through comprehensive assessments using both a real-life\ndataset and a publicly accessible one, we demonstrate that the proposed\napproach consistently outperforms state-of-the-art methods based on generative\nadversarial networks and a CopyPaste strategy. Our proposed model demonstrates\nsuperior performance in standard distortion metrics and preserves critical ECG\ncharacteristics, particularly the P, Q, R, S, and T wave coordinates. Two\nemerging clinical applications emphasize the relevance of our work. The first\nis the increasing need to digitize paper-stored ECGs for utilization in\nAI-based applications (automatic annotation and risk-quantification), often\nlimited to digital ECG complete 10s recordings. The second is the widespread\nuse of wearable devices that record ECGs but typically capture only a small\nsubset of the 12 standard leads. In both cases, a non-negligible amount of\ninformation is lost or not recorded, which our approach aims to recover to\novercome these limitations.\n","authors":["Alex Lence","Ahmad Fall","Federica Granese","Blaise Hanczar","Joe-Elie Salem","Jean-Daniel Zucker","Edi Prifti"],"pdf_url":"https://arxiv.org/pdf/2406.16901v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18176v1","updated":"2024-06-26T08:50:51Z","published":"2024-06-26T08:50:51Z","title":"VIPriors 4: Visual Inductive Priors for Data-Efficient Deep Learning\n  Challenges","summary":"  The fourth edition of the \"VIPriors: Visual Inductive Priors for\nData-Efficient Deep Learning\" workshop features two data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate inductive biases to improve the data\nefficiency of deep learning models. Significant advancements are made compared\nto the provided baselines, where winning solutions surpass the baselines by a\nconsiderable margin in both tasks. As in previous editions, these achievements\nare primarily attributed to heavy use of data augmentation policies and large\nmodel ensembles, though novel prior-based methods seem to contribute more to\nsuccessful solutions compared to last year. This report highlights the key\naspects of the challenges and their outcomes.\n","authors":["Robert-Jan Bruintjes","Attila Lengyel","Marcos Baptista Rios","Osman Semih Kayhan","Davide Zambrano","Nergis Tomen","Jan van Gemert"],"pdf_url":"https://arxiv.org/pdf/2406.18176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05119v2","updated":"2024-06-26T08:25:49Z","published":"2023-02-10T08:49:36Z","title":"Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal\n  Gradient and Low-rank Approximation","summary":"  Tensor decomposition is a fundamental technique widely applied in signal\nprocessing, machine learning, and various other fields. However, traditional\ntensor decomposition methods encounter limitations when jointly analyzing\nmulti-block tensors, as they often struggle to effectively explore shared\ninformation among tensors. In this study, we first introduce a novel coupled\nnonnegative CANDECOMP/PARAFAC decomposition algorithm optimized by the\nalternating proximal gradient method (CoNCPD-APG). This algorithm is specially\ndesigned to address the challenges of jointly decomposing different tensors\nthat are partially or fully linked, while simultaneously extracting common\ncomponents, individual components and, core tensors. Recognizing the\ncomputational challenges inherent in optimizing nonnegative constraints over\nhigh-dimensional tensor data, we further propose the lraCoNCPD-APG algorithm.\nBy integrating low-rank approximation with the proposed CoNCPD-APG method, the\nproposed algorithm can significantly decrease the computational burden without\ncompromising decomposition quality, particularly for multi-block large-scale\ntensors. Simulation experiments conducted on synthetic data, real-world face\nimage data, and two kinds of electroencephalography (EEG) data demonstrate the\npracticality and superiority of the proposed algorithms for coupled nonnegative\ntensor decomposition problems. Our results underscore the efficacy of our\nmethods in uncovering meaningful patterns and structures from complex\nmulti-block tensor data, thereby offering valuable insights for future\napplications.\n","authors":["Xiulin Wang","Jing Liu","Fengyu Cong"],"pdf_url":"https://arxiv.org/pdf/2302.05119v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.08861v2","updated":"2024-06-26T08:22:31Z","published":"2023-10-13T05:08:35Z","title":"Re-initialization-free Level Set Method via Molecular Beam Epitaxy\n  Equation Regularization for Image Segmentation","summary":"  Variational level set method has become a powerful tool in image segmentation\ndue to its ability to handle complex topological changes and maintain\ncontinuity and smoothness in the process of evolution. However its evolution\nprocess can be unstable, which results in over flatted or over sharpened\ncontours and segmentation failure. To improve the accuracy and stability of\nevolution, we propose a high-order level set variational segmentation method\nintegrated with molecular beam epitaxy (MBE) equation regularization. This\nmethod uses the crystal growth in the MBE process to limit the evolution of the\nlevel set function, and thus can avoid the re-initialization in the evolution\nprocess and regulate the smoothness of the segmented curve. It also works for\nnoisy images with intensity inhomogeneity, which is a challenge in image\nsegmentation. To solve the variational model, we derive the gradient flow and\ndesign scalar auxiliary variable (SAV) scheme coupled with fast Fourier\ntransform (FFT), which can significantly improve the computational efficiency\ncompared with the traditional semi-implicit and semi-explicit scheme. Numerical\nexperiments show that the proposed method can generate smooth segmentation\ncurves, retain fine segmentation targets and obtain robust segmentation results\nof small objects. Compared to existing level set methods, this model is\nstate-of-the-art in both accuracy and efficiency.\n","authors":["Fanghui Song","Jiebao Sun","Shengzhu Shi","Zhichang Guo","Dazhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18159v1","updated":"2024-06-26T08:18:39Z","published":"2024-06-26T08:18:39Z","title":"Human-Aware 3D Scene Generation with Spatially-constrained Diffusion\n  Models","summary":"  Generating 3D scenes from human motion sequences supports numerous\napplications, including virtual reality and architectural design. However,\nprevious auto-regression-based human-aware 3D scene generation methods have\nstruggled to accurately capture the joint distribution of multiple objects and\ninput humans, often resulting in overlapping object generation in the same\nspace. To address this limitation, we explore the potential of diffusion models\nthat simultaneously consider all input humans and the floor plan to generate\nplausible 3D scenes. Our approach not only satisfies all input human\ninteractions but also adheres to spatial constraints with the floor plan.\nFurthermore, we introduce two spatial collision guidance mechanisms:\nhuman-object collision avoidance and object-room boundary constraints. These\nmechanisms help avoid generating scenes that conflict with human motions while\nrespecting layout constraints. To enhance the diversity and accuracy of\nhuman-guided scene generation, we have developed an automated pipeline that\nimproves the variety and plausibility of human-object interactions in the\nexisting 3D FRONT HUMAN dataset. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that our framework can generate more natural\nand plausible 3D scenes with precise human-scene interactions, while\nsignificantly reducing human-object collisions compared to previous\nstate-of-the-art methods. Our code and data will be made publicly available\nupon publication of this work.\n","authors":["Xiaolin Hong","Hongwei Yi","Fazhi He","Qiong Cao"],"pdf_url":"https://arxiv.org/pdf/2406.18159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18158v1","updated":"2024-06-26T08:17:59Z","published":"2024-06-26T08:17:59Z","title":"3D-MVP: 3D Multiview Pretraining for Robotic Manipulation","summary":"  Recent works have shown that visual pretraining on egocentric datasets using\nmasked autoencoders (MAE) can improve generalization for downstream robotics\ntasks. However, these approaches pretrain only on 2D images, while many\nrobotics applications require 3D scene understanding. In this work, we propose\n3D-MVP, a novel approach for 3D multi-view pretraining using masked\nautoencoders. We leverage Robotic View Transformer (RVT), which uses a\nmulti-view transformer to understand the 3D scene and predict gripper pose\nactions. We split RVT's multi-view transformer into visual encoder and action\ndecoder, and pretrain its visual encoder using masked autoencoding on\nlarge-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of\nvirtual robot manipulation tasks and demonstrate improved performance over\nbaselines. We also show promising results on a real robot platform with minimal\nfinetuning. Our results suggest that 3D-aware pretraining is a promising\napproach to improve sample efficiency and generalization of vision-based\nrobotic manipulation policies. We will release code and pretrained models for\n3D-MVP to facilitate future research. Project site:\nhttps://jasonqsy.github.io/3DMVP\n","authors":["Shengyi Qian","Kaichun Mo","Valts Blukis","David F. Fouhey","Dieter Fox","Ankit Goyal"],"pdf_url":"https://arxiv.org/pdf/2406.18158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05404v3","updated":"2024-06-26T08:10:43Z","published":"2023-08-10T07:53:06Z","title":"Enhancing Low-light Light Field Images with A Deep Compensation\n  Unfolding Network","summary":"  This paper presents a novel and interpretable end-to-end learning framework,\ncalled the deep compensation unfolding network (DCUNet), for restoring light\nfield (LF) images captured under low-light conditions. DCUNet is designed with\na multi-stage architecture that mimics the optimization process of solving an\ninverse imaging problem in a data-driven fashion. The framework uses the\nintermediate enhanced result to estimate the illumination map, which is then\nemployed in the unfolding process to produce a new enhanced result.\nAdditionally, DCUNet includes a content-associated deep compensation module at\neach optimization stage to suppress noise and illumination map estimation\nerrors. To properly mine and leverage the unique characteristics of LF images,\nthis paper proposes a pseudo-explicit feature interaction module that\ncomprehensively exploits redundant information in LF images. The experimental\nresults on both simulated and real datasets demonstrate the superiority of our\nDCUNet over state-of-the-art methods, both qualitatively and quantitatively.\nMoreover, DCUNet preserves the essential geometric structure of enhanced LF\nimages much better. The code will be publicly available at\nhttps://github.com/lyuxianqiang/LFLL-DCU.\n","authors":["Xianqiang Lyu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2308.05404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18151v1","updated":"2024-06-26T08:04:42Z","published":"2024-06-26T08:04:42Z","title":"SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from\n  Monocular Remote Sensing Imagery","summary":"  Global semantic 3D understanding from single-view high-resolution remote\nsensing (RS) imagery is crucial for Earth Observation (EO). However, this task\nfaces significant challenges due to the high costs of annotations and data\ncollection, as well as geographically restricted data availability. To address\nthese challenges, synthetic data offer a promising solution by being easily\naccessible and thus enabling the provision of large and diverse datasets. We\ndevelop a specialized synthetic data generation pipeline for EO and introduce\nSynRS3D, the largest synthetic RS 3D dataset. SynRS3D comprises 69,667\nhigh-resolution optical images that cover six different city styles worldwide\nand feature eight land cover types, precise height information, and building\nchange masks. To further enhance its utility, we develop a novel multi-task\nunsupervised domain adaptation (UDA) method, RS3DAda, coupled with our\nsynthetic dataset, which facilitates the RS-specific transition from synthetic\nto real scenarios for land cover mapping and height estimation tasks,\nultimately enabling global monocular 3D semantic understanding based on\nsynthetic data. Extensive experiments on various real-world datasets\ndemonstrate the adaptability and effectiveness of our synthetic dataset and\nproposed RS3DAda method. SynRS3D and related codes will be available.\n","authors":["Jian Song","Hongruixuan Chen","Weihao Xuan","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2406.18151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05935v2","updated":"2024-06-26T07:59:03Z","published":"2024-02-08T18:59:48Z","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models","summary":"  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory\n","authors":["Dongyang Liu","Renrui Zhang","Longtian Qiu","Siyuan Huang","Weifeng Lin","Shitian Zhao","Shijie Geng","Ziyi Lin","Peng Jin","Kaipeng Zhang","Wenqi Shao","Chao Xu","Conghui He","Junjun He","Hao Shao","Pan Lu","Hongsheng Li","Yu Qiao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2402.05935v2.pdf","comment":"Accepted by ICML 2024. Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2406.18146v1","updated":"2024-06-26T07:56:17Z","published":"2024-06-26T07:56:17Z","title":"A Refer-and-Ground Multimodal Large Language Model for Biomedicine","summary":"  With the rapid development of multimodal large language models (MLLMs),\nespecially their capabilities in visual chat through refer and ground\nfunctionalities, their significance is increasingly recognized. However, the\nbiomedical field currently exhibits a substantial gap in this area, primarily\ndue to the absence of a dedicated refer and ground dataset for biomedical\nimages. To address this challenge, we devised the Med-GRIT-270k dataset. It\ncomprises 270k question-and-answer pairs and spans eight distinct medical\nimaging modalities. Most importantly, it is the first dedicated to the\nbiomedical domain and integrating refer and ground conversations. The key idea\nis to sample large-scale biomedical image-mask pairs from medical segmentation\ndatasets and generate instruction datasets from text using chatGPT.\nAdditionally, we introduce a Refer-and-Ground Multimodal Large Language Model\nfor Biomedicine (BiRD) by using this dataset and multi-task instruction\nlearning. Extensive experiments have corroborated the efficacy of the\nMed-GRIT-270k dataset and the multi-modal, fine-grained interactive\ncapabilities of the BiRD model. This holds significant reference value for the\nexploration and development of intelligent biomedical assistants.\n","authors":["Xiaoshuang Huang","Haifeng Huang","Lingdong Shen","Yehui Yang","Fangxin Shang","Junwei Liu","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18146v1.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2406.18144v1","updated":"2024-06-26T07:50:58Z","published":"2024-06-26T07:50:58Z","title":"Artificial Immune System of Secure Face Recognition Against Adversarial\n  Attacks","summary":"  Insect production for food and feed presents a promising supplement to ensure\nfood safety and address the adverse impacts of agriculture on climate and\nenvironment in the future. However, optimisation is required for insect\nproduction to realise its full potential. This can be by targeted improvement\nof traits of interest through selective breeding, an approach which has so far\nbeen underexplored and underutilised in insect farming. Here we present a\ncomprehensive review of the selective breeding framework in the context of\ninsect production. We systematically evaluate adjustments of selective breeding\ntechniques to the realm of insects and highlight the essential components\nintegral to the breeding process. The discussion covers every step of a\nconventional breeding scheme, such as formulation of breeding objectives,\nphenotyping, estimation of genetic parameters and breeding values, selection of\nappropriate breeding strategies, and mitigation of issues associated with\ngenetic diversity depletion and inbreeding. This review combines knowledge from\ndiverse disciplines, bridging the gap between animal breeding, quantitative\ngenetics, evolutionary biology, and entomology, offering an integrated view of\nthe insect breeding research area and uniting knowledge which has previously\nremained scattered across diverse fields of expertise.\n","authors":["Min Ren","Yunlong Wang","Yuhao Zhu","Yongzhen Huang","Zhenan Sun","Qi Li","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.18144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18140v1","updated":"2024-06-26T07:44:27Z","published":"2024-06-26T07:44:27Z","title":"Exclusive Style Removal for Cross Domain Novel Class Discovery","summary":"  As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.\n","authors":["Yicheng Wang","Feng Liu","Junmin Liu","Zhen Fang","Kai Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18139v1","updated":"2024-06-26T07:44:24Z","published":"2024-06-26T07:44:24Z","title":"LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal\n  Long-Context Inference","summary":"  Long-context Multimodal Large Language Models (MLLMs) demand substantial\ncomputational resources for inference as the growth of their multimodal\nKey-Value (KV) cache, in response to increasing input lengths, challenges\nmemory and time efficiency. Unlike single-modality LLMs that manage only\ntextual contexts, the KV cache of long-context MLLMs includes representations\nfrom multiple images with temporal and spatial relationships and related\ntextual contexts. The predominance of image tokens means traditional\noptimizations for LLMs' KV caches are unsuitable for multimodal long-context\nsettings, and no prior works have addressed this challenge. In this work, we\nintroduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently\nreduces the multimodal KV cache size while maintaining performance comparable\nto a full cache. We observe that during prompt prefill, the model prioritizes\nmore textual attention over image features, and based on the multimodal\ninteraction observation, a new proposed text-prior method is explored to\ncompress the KV cache. Furthermore, to mitigate the degradation of image\ncontextual information, we propose several compensatory strategies using KV\npairs merging. LOOK-M demonstrates that with a significant reduction in KV\nCache memory usage, such as reducing it by 80% in some cases, it not only\nachieves up to 1.5x faster decoding but also maintains or even enhances\nperformance across a variety of long context multimodal tasks.\n","authors":["Zhongwei Wan","Ziang Wu","Che Liu","Jinfa Huang","Zhihong Zhu","Peng Jin","Longyue Wang","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18129v1","updated":"2024-06-26T07:31:16Z","published":"2024-06-26T07:31:16Z","title":"CTS: Sim-to-Real Unsupervised Domain Adaptation on 3D Detection","summary":"  Simulation data can be accurately labeled and have been expected to improve\nthe performance of data-driven algorithms, including object detection. However,\ndue to the various domain inconsistencies from simulation to reality\n(sim-to-real), cross-domain object detection algorithms usually suffer from\ndramatic performance drops. While numerous unsupervised domain adaptation (UDA)\nmethods have been developed to address cross-domain tasks between real-world\ndatasets, progress in sim-to-real remains limited. This paper presents a novel\nComplex-to-Simple (CTS) framework to transfer models from labeled simulation\n(source) to unlabeled reality (target) domains. Based on a two-stage detector,\nthe novelty of this work is threefold: 1) developing fixed-size anchor heads\nand RoI augmentation to address size bias and feature diversity between two\ndomains, thereby improving the quality of pseudo-label; 2) developing a novel\ncorner-format representation of aleatoric uncertainty (AU) for the bounding\nbox, to uniformly quantify pseudo-label quality; 3) developing a noise-aware\nmean teacher domain adaptation method based on AU, as well as object-level and\nframe-level sampling strategies, to migrate the impact of noisy labels.\nExperimental results demonstrate that our proposed approach significantly\nenhances the sim-to-real domain adaptation capability of 3D object detection\nmodels, outperforming state-of-the-art cross-domain algorithms, which are\nusually developed for real-to-real UDA tasks.\n","authors":["Meiying Zhang","Weiyuan Peng","Guangyao Ding","Chenyang Lei","Chunlin Ji","Qi Hao"],"pdf_url":"https://arxiv.org/pdf/2406.18129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18115v1","updated":"2024-06-26T07:06:42Z","published":"2024-06-26T07:06:42Z","title":"Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with\n  3D Semantic Maps","summary":"  Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for\nautonomous robots, especially when faced with the challenges posed by unknown\nand dynamic environments. This task requires robots to explore and build a\nsemantic understanding of their surroundings, generate feasible plans to\nachieve manipulation goals, adapt to environmental changes, and comprehend\nnatural language instructions from humans. To address these challenges, we\npropose a novel framework that leverages the zero-shot detection and grounded\nrecognition capabilities of pretraining visual-language models (VLMs) combined\nwith dense 3D entity reconstruction to build 3D semantic maps. Additionally, we\nutilize large language models (LLMs) for spatial region abstraction and online\nplanning, incorporating human instructions and spatial semantic context. We\nhave built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated\nin real-world robot experiments that our proposed framework can effectively\ncapture spatial semantics and process natural language user instructions for\nzero-shot OVMM tasks under dynamic environment settings, with an overall\nnavigation and task success rate of 80.95% and 73.33% over 105 episodes, and\nbetter SFT and SPL by 157.18% and 19.53% respectively compared to the baseline.\nFurthermore, the framework is capable of replanning towards the next most\nprobable candidate location based on the spatial semantic context derived from\nthe 3D semantic map when initial plans fail, keeping an average success rate of\n76.67%.\n","authors":["Dicong Qiu","Wenzong Ma","Zhenfu Pan","Hui Xiong","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2406.18115v1.pdf","comment":"Open-vocabulary, Mobile Manipulation, Dynamic Environments, 3D\n  Semantic Maps, Zero-shot, LLMs, VLMs, 18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.13561v2","updated":"2024-06-26T07:05:21Z","published":"2024-02-21T06:34:46Z","title":"Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension\n  with Enhanced Visual Knowledge Alignment","summary":"  Evaluating and Rethinking the current landscape of Large Multimodal Models\n(LMMs), we observe that widely-used visual-language projection approaches\n(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet\nignore the visual knowledge-dimension alignment, i.e., connecting visuals to\ntheir relevant knowledge. Visual knowledge plays a significant role in\nanalyzing, inferring, and interpreting information from visuals, helping\nimprove the accuracy of answers to knowledge-based visual questions. In this\npaper, we mainly explore improving LMMs with visual-language knowledge\nalignment, especially aimed at challenging knowledge-based visual question\nanswering (VQA). To this end, we present a Cognitive Visual-Language Mapper\n(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a\nFine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning\nstage. Specifically, we design the VKA based on the interaction between a small\nlanguage model and a visual encoder, training it on collected image-knowledge\npairs to achieve visual knowledge acquisition and projection. FKA is employed\nto distill the fine-grained visual knowledge of an image and inject it into\nLarge Language Models (LLMs). We conduct extensive experiments on\nknowledge-based VQA benchmarks and experimental results show that CVLM\nsignificantly improves the performance of LMMs on knowledge-based VQA (average\ngain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,\nrespectively. The codes are available at\nhttps://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper\n","authors":["Yunxin Li","Xinyu Chen","Baotian Hu","Haoyuan Shi","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13561v2.pdf","comment":"12 pages,4 figures; Accepted by ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.18113v1","updated":"2024-06-26T06:59:09Z","published":"2024-06-26T06:59:09Z","title":"The Surprising Effectiveness of Multimodal Large Language Models for\n  Video Moment Retrieval","summary":"  Recent studies have shown promising results in utilizing multimodal large\nlanguage models (MLLMs) for computer vision tasks such as object detection and\nsemantic segmentation. However, many challenging video tasks remain\nunder-explored. Video-language tasks necessitate spatial and temporal\ncomprehension and require significant compute. Therefore, prior works have\ndeveloped complex, highly specialized architectures or leveraged additional\ninput signals such as video transcripts to best encode contextual and temporal\ninformation, which limits their generality and can be impractical. One\nparticularly challenging task is video moment retrieval, which requires precise\ntemporal and contextual grounding. This work demonstrates the surprising\neffectiveness of leveraging image-text pretrained MLLMs for moment retrieval.\nWe introduce Mr. BLIP (Mr. as in Moment Retrieval), a multimodal, single-stage\nmodel that requires no expensive video-language pretraining, no additional\ninput signal (e.g., no transcript or audio), and has a simpler and more\nversatile design than prior state-of-the-art methods. We achieve a new\nstate-of-the-art in moment retrieval on the widely used benchmarks\nCharades-STA, QVHighlights, and ActivityNet Captions and illustrate our\nmethod's versatility with a new state-of-the-art in temporal action\nlocalization on ActivityNet. Notably, we attain over 9% (absolute) higher\nRecall (at 0.5 and 0.7 IoU) on the challenging long-video multi-moment\nQVHighlights benchmark. Our code is publicly available.\n","authors":["Meinardus Boris","Batra Anil","Rohrbach Anna","Rohrbach Marcus"],"pdf_url":"https://arxiv.org/pdf/2406.18113v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.07189v3","updated":"2024-06-26T06:39:49Z","published":"2024-06-11T12:01:11Z","title":"RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker","summary":"  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n","authors":["Yunfeng Li","Bo Wang","Jiuran Sun","Xueyi Wu","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2406.07189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18102v1","updated":"2024-06-26T06:39:11Z","published":"2024-06-26T06:39:11Z","title":"A Lung Nodule Dataset with Histopathology-based Cancer Type Annotation","summary":"  Recently, Computer-Aided Diagnosis (CAD) systems have emerged as\nindispensable tools in clinical diagnostic workflows, significantly alleviating\nthe burden on radiologists. Nevertheless, despite their integration into\nclinical settings, CAD systems encounter limitations. Specifically, while CAD\nsystems can achieve high performance in the detection of lung nodules, they\nface challenges in accurately predicting multiple cancer types. This limitation\ncan be attributed to the scarcity of publicly available datasets annotated with\nexpert-level cancer type information. This research aims to bridge this gap by\nproviding publicly accessible datasets and reliable tools for medical\ndiagnosis, facilitating a finer categorization of different types of lung\ndiseases so as to offer precise treatment recommendations. To achieve this\nobjective, we curated a diverse dataset of lung Computed Tomography (CT)\nimages, comprising 330 annotated nodules (nodules are labeled as bounding\nboxes) from 95 distinct patients. The quality of the dataset was evaluated\nusing a variety of classical classification and detection models, and these\npromising results demonstrate that the dataset has a feasible application and\nfurther facilitate intelligent auxiliary diagnosis.\n","authors":["Muwei Jian","Hongyu Chen","Zaiyong Zhang","Nan Yang","Haorang Zhang","Lifu Ma","Wenjing Xu","Huixiang Zhi"],"pdf_url":"https://arxiv.org/pdf/2406.18102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10237v2","updated":"2024-06-26T06:04:51Z","published":"2024-04-16T02:35:17Z","title":"Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models","summary":"  Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method.\n","authors":["Songtao Jiang","Tuo Zheng","Yan Zhang","Yeying Jin","Li Yuan","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16464v2","updated":"2024-06-26T05:40:16Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Current multi-modal sarcasm detection methods have been\nproven to struggle with biases from spurious cues, leading to a superficial\nunderstanding of the complex interactions between text and image. To address\nthese issues, we propose InterCLIP-MEP, a robust framework for multi-modal\nsarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,\nInteractive CLIP (InterCLIP), as the backbone, enhancing sample representations\nby embedding cross-modality information in each encoder. Furthermore, a novel\ntraining strategy is designed to adapt InterCLIP for a Memory-Enhanced\nPredictor (MEP). MEP uses dynamic dual-channel memory to store valuable\nhistorical knowledge of test samples and then leverages this memory as a\nnon-parametric classifier to derive the final prediction. By using InterCLIP to\nencode text-image interactions more effectively and incorporating MEP,\nInterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.\nExperiments demonstrate that InterCLIP-MEP achieves state-of-the-art\nperformance on the MMSD2.0 benchmark. Code and data are available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Subin Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v2.pdf","comment":"8 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.18079v1","updated":"2024-06-26T05:31:36Z","published":"2024-06-26T05:31:36Z","title":"MFDNet: Multi-Frequency Deflare Network for Efficient Nighttime Flare\n  Removal","summary":"  When light is scattered or reflected accidentally in the lens, flare\nartifacts may appear in the captured photos, affecting the photos' visual\nquality. The main challenge in flare removal is to eliminate various flare\nartifacts while preserving the original content of the image. To address this\nchallenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet)\nbased on the Laplacian Pyramid. Our network decomposes the flare-corrupted\nimage into low and high-frequency bands, effectively separating the\nillumination and content information in the image. The low-frequency part\ntypically contains illumination information, while the high-frequency part\ncontains detailed content information. So our MFDNet consists of two main\nmodules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in\nthe low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM)\nto reconstruct the flare-free image. Specifically, to perceive flare from a\nglobal perspective while retaining detailed information for image restoration,\nLFFPM utilizes Transformer to extract global information while utilizing a\nconvolutional neural network to capture detailed local features. Then HFRM\ngradually fuses the outputs of LFFPM with the high-frequency component of the\nimage through feature aggregation. Moreover, our MFDNet can reduce the\ncomputational cost by processing in multiple frequency bands instead of\ndirectly removing the flare on the input image. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in removing\nnighttime flare on real-world and synthetic images from the Flare7K dataset.\nFurthermore, the computational complexity of our model is remarkably low.\n","authors":["Yiguo Jiang","Xuhang Chen","Chi-Man Pun","Shuqiang Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18079v1.pdf","comment":"Accepted by The Visual Computer journal"},{"id":"http://arxiv.org/abs/2406.12837v2","updated":"2024-06-26T05:28:12Z","published":"2024-06-18T17:55:15Z","title":"LayerMerge: Neural Network Depth Compression through Layer Pruning and\n  Merging","summary":"  Recent works show that reducing the number of layers in a convolutional\nneural network can enhance efficiency while maintaining the performance of the\nnetwork. Existing depth compression methods remove redundant non-linear\nactivation functions and merge the consecutive convolution layers into a single\nlayer. However, these methods suffer from a critical drawback; the kernel size\nof the merged layers becomes larger, significantly undermining the latency\nreduction gained from reducing the depth of the network. We show that this\nproblem can be addressed by jointly pruning convolution layers and activation\nfunctions. To this end, we propose LayerMerge, a novel depth compression method\nthat selects which activation layers and convolution layers to remove, to\nachieve a desired inference speed-up while minimizing performance loss. Since\nthe corresponding selection problem involves an exponential search space, we\nformulate a novel surrogate optimization problem and efficiently solve it via\ndynamic programming. Empirical results demonstrate that our method consistently\noutperforms existing depth compression and layer pruning methods on various\nnetwork architectures, both on image classification and generation tasks. We\nrelease the code at https://github.com/snu-mllab/LayerMerge.\n","authors":["Jinuk Kim","Marwa El Halabi","Mingi Ji","Hyun Oh Song"],"pdf_url":"https://arxiv.org/pdf/2406.12837v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.18074v1","updated":"2024-06-26T05:06:14Z","published":"2024-06-26T05:06:14Z","title":"Few-Shot Medical Image Segmentation with High-Fidelity Prototypes","summary":"  Few-shot Semantic Segmentation (FSS) aims to adapt a pretrained model to new\nclasses with as few as a single labelled training sample per class. Despite the\nprototype based approaches have achieved substantial success, existing models\nare limited to the imaging scenarios with considerably distinct objects and not\nhighly complex background, e.g., natural images. This makes such models\nsuboptimal for medical imaging with both conditions invalid. To address this\nproblem, we propose a novel Detail Self-refined Prototype Network (DSPNet) to\nconstructing high-fidelity prototypes representing the object foreground and\nthe background more comprehensively. Specifically, to construct global\nsemantics while maintaining the captured detail semantics, we learn the\nforeground prototypes by modelling the multi-modal structures with clustering\nand then fusing each in a channel-wise manner. Considering that the background\noften has no apparent semantic relation in the spatial dimensions, we integrate\nchannel-specific structural information under sparse channel-aware regulation.\nExtensive experiments on three challenging medical image benchmarks show the\nsuperiority of DSPNet over previous state-of-the-art methods.\n","authors":["Song Tang","Shaxu Yan","Xiaozhi Qi","Jianxin Gao","Mao Ye","Jianwei Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.18074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18070v1","updated":"2024-06-26T05:01:37Z","published":"2024-06-26T05:01:37Z","title":"EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation","summary":"  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n","authors":["Baoqi Pei","Guo Chen","Jilan Xu","Yuping He","Yicheng Liu","Kanghua Pan","Yifei Huang","Yali Wang","Tong Lu","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2406.18070v1.pdf","comment":"Champion solutions in the EgoVis CVPR 2024 workshop"},{"id":"http://arxiv.org/abs/2406.18068v1","updated":"2024-06-26T04:53:11Z","published":"2024-06-26T04:53:11Z","title":"Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective\n  Face and Body Expressions from Affordable Inputs","summary":"  We present a multimodal learning-based method to simultaneously synthesize\nco-speech facial expressions and upper-body gestures for digital characters\nusing RGB video data captured using commodity cameras. Our approach learns from\nsparse face landmarks and upper-body joints, estimated directly from video\ndata, to generate plausible emotive character motions. Given a speech audio\nwaveform and a token sequence of the speaker's face landmark motion and\nbody-joint motion computed from a video, our method synthesizes the motion\nsequences for the speaker's face landmarks and body joints to match the content\nand the affect of the speech. We design a generator consisting of a set of\nencoders to transform all the inputs into a multimodal embedding space\ncapturing their correlations, followed by a pair of decoders to synthesize the\ndesired face and pose motions. To enhance the plausibility of synthesis, we use\nan adversarial discriminator that learns to differentiate between the face and\npose motions computed from the original videos and our synthesized motions\nbased on their affective expressions. To evaluate our approach, we extend the\nTED Gesture Dataset to include view-normalized, co-speech face landmarks in\naddition to body gestures. We demonstrate the performance of our method through\nthorough quantitative and qualitative experiments on multiple evaluation\nmetrics and via a user study. We observe that our method results in low\nreconstruction error and produces synthesized samples with diverse facial\nexpressions and body gestures for digital characters.\n","authors":["Uttaran Bhattacharya","Aniket Bera","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.18068v1.pdf","comment":"14 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.18054v1","updated":"2024-06-26T04:12:34Z","published":"2024-06-26T04:12:34Z","title":"Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image\n  Translation","summary":"  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology\nare Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides\noffer high quality histopathological images but require a labor-intensive\nacquisition process. In contrast, FF slides can be prepared quickly, but the\nimage quality is relatively poor. Our task is to translate FF images into FFPE\nstyle, thereby improving the image quality for diagnostic purposes. In this\npaper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological\nimage translation using a pre-trained diffusion model. Specifically, we employ\na one-step diffusion model as the generator and fine-tune it with LoRA adapters\nusing adversarial learning objectives. To ensure that the model effectively\ncaptures both global structural information and local details, we propose a\nmulti-scale feature fusion (MFF) module. This module utilizes two VAE encoders\nto extract features of varying image sizes and performs feature fusion before\nfeeding them into the UNet. Furthermore, we utilize a pre-trained\nvision-language model for histopathology as the backbone for the discriminator\nto further improve performance We conducted FF-to-FFPE translation experiments\non the TCGA-NSCLC datasets, and our method achieved better performance compared\nto other methods. The code and models are released at\nhttps://github.com/QilaiZhang/Diffusion-FFPE.\n","authors":["Qilai Zhang","Jiawen Li","Peiran Liao","Jiali Hu","Tian Guan","Anjia Han","Yonghong He"],"pdf_url":"https://arxiv.org/pdf/2406.18054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18051v1","updated":"2024-06-26T04:01:19Z","published":"2024-06-26T04:01:19Z","title":"ViT-1.58b: Mobile Vision Transformers in the 1-bit Era","summary":"  Vision Transformers (ViTs) have achieved remarkable performance in various\nimage classification tasks by leveraging the attention mechanism to process\nimage patches as tokens. However, the high computational and memory demands of\nViTs pose significant challenges for deployment in resource-constrained\nenvironments. This paper introduces ViT-1.58b, a novel 1.58-bit quantized ViT\nmodel designed to drastically reduce memory and computational overhead while\npreserving competitive performance. ViT-1.58b employs ternary quantization,\nwhich refines the balance between efficiency and accuracy by constraining\nweights to {-1, 0, 1} and quantizing activations to 8-bit precision. Our\napproach ensures efficient scaling in terms of both memory and computation.\nExperiments on CIFAR-10 and ImageNet-1k demonstrate that ViT-1.58b maintains\ncomparable accuracy to full-precision Vit, with significant reductions in\nmemory usage and computational costs. This paper highlights the potential of\nextreme quantization techniques in developing sustainable AI solutions and\ncontributes to the broader discourse on efficient model deployment in practical\napplications. Our code and weights are available at\nhttps://github.com/DLYuanGod/ViT-1.58b.\n","authors":["Zhengqing Yuan","Rong Zhou","Hongyi Wang","Lifang He","Yanfang Ye","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18050v1","updated":"2024-06-26T03:59:21Z","published":"2024-06-26T03:59:21Z","title":"A Multi-Stage Goal-Driven Network for Pedestrian Trajectory Prediction","summary":"  Pedestrian trajectory prediction plays a pivotal role in ensuring the safety\nand efficiency of various applications, including autonomous vehicles and\ntraffic management systems. This paper proposes a novel method for pedestrian\ntrajectory prediction, called multi-stage goal-driven network (MGNet).\nDiverging from prior approaches relying on stepwise recursive prediction and\nthe singular forecasting of a long-term goal, MGNet directs trajectory\ngeneration by forecasting intermediate stage goals, thereby reducing prediction\nerrors. The network comprises three main components: a conditional variational\nautoencoder (CVAE), an attention module, and a multi-stage goal evaluator.\nTrajectories are encoded using conditional variational autoencoders to acquire\nknowledge about the approximate distribution of pedestrians' future\ntrajectories, and combined with an attention mechanism to capture the temporal\ndependency between trajectory sequences. The pivotal module is the multi-stage\ngoal evaluator, which utilizes the encoded feature vectors to predict\nintermediate goals, effectively minimizing cumulative errors in the recursive\ninference process. The effectiveness of MGNet is demonstrated through\ncomprehensive experiments on the JAAD and PIE datasets. Comparative evaluations\nagainst state-of-the-art algorithms reveal significant performance improvements\nachieved by our proposed method.\n","authors":["Xiuen Wu","Tao Wang","Yuanzheng Cai","Lingyu Liang","George Papageorgiou"],"pdf_url":"https://arxiv.org/pdf/2406.18050v1.pdf","comment":"Paper accepted by 5th International Conference on Computer Vision,\n  Image and Deep Learning (CVIDL 2024)"},{"id":"http://arxiv.org/abs/2406.18048v1","updated":"2024-06-26T03:56:03Z","published":"2024-06-26T03:56:03Z","title":"ScanFormer: Referring Expression Comprehension by Iteratively Scanning","summary":"  Referring Expression Comprehension (REC) aims to localize the target objects\nspecified by free-form natural language descriptions in images. While\nstate-of-the-art methods achieve impressive performance, they perform a dense\nperception of images, which incorporates redundant visual regions unrelated to\nlinguistic queries, leading to additional computational overhead. This inspires\nus to explore a question: can we eliminate linguistic-irrelevant redundant\nvisual regions to improve the efficiency of the model? Existing relevant\nmethods primarily focus on fundamental visual tasks, with limited exploration\nin vision-language fields. To address this, we propose a coarse-to-fine\niterative perception framework, called ScanFormer. It can iteratively exploit\nthe image scale pyramid to extract linguistic-relevant visual patches from top\nto bottom. In each iteration, irrelevant patches are discarded by our designed\ninformativeness prediction. Furthermore, we propose a patch selection strategy\nfor discarded patches to accelerate inference. Experiments on widely used\ndatasets, namely RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame, verify the\neffectiveness of our method, which can strike a balance between accuracy and\nefficiency.\n","authors":["Wei Su","Peihan Miao","Huanzhang Dou","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.18048v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2406.18043v1","updated":"2024-06-26T03:41:48Z","published":"2024-06-26T03:41:48Z","title":"Multimodal foundation world models for generalist embodied agents","summary":"  Learning generalist embodied agents, able to solve multitudes of tasks in\ndifferent domains is a long-standing problem. Reinforcement learning (RL) is\nhard to scale up as it requires a complex reward design for each task. In\ncontrast, language can specify tasks in a more natural way. Current foundation\nvision-language models (VLMs) generally require fine-tuning or other\nadaptations to be functional, due to the significant domain gap. However, the\nlack of multimodal data in such domains represents an obstacle toward\ndeveloping foundation models for embodied applications. In this work, we\novercome these problems by presenting multimodal foundation world models, able\nto connect and align the representation of foundation VLMs with the latent\nspace of generative world models for RL, without any language annotations. The\nresulting agent learning framework, GenRL, allows one to specify tasks through\nvision and/or language prompts, ground them in the embodied domain's dynamics,\nand learns the corresponding behaviors in imagination. As assessed through\nlarge-scale multi-task benchmarking, GenRL exhibits strong multi-task\ngeneralization performance in several locomotion and manipulation domains.\nFurthermore, by introducing a data-free RL strategy, it lays the groundwork for\nfoundation model-based RL for generalist embodied agents.\n","authors":["Pietro Mazzaglia","Tim Verbelen","Bart Dhoedt","Aaron Courville","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2406.18043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02085v4","updated":"2024-06-26T03:32:50Z","published":"2024-02-03T08:52:06Z","title":"DeCoF: Generated Video Detection via Frame Consistency: The First\n  Benchmark Dataset","summary":"  The escalating quality of video generated by advanced video generation\nmethods results in new security challenges, while there have been few relevant\nresearch efforts: 1) There is no open-source dataset for generated video\ndetection, 2) No generated video detection method has been proposed so far. To\nthis end, we propose an open-source dataset and a detection method for\ngenerated video for the first time. First, we propose a scalable dataset\nconsisting of 964 prompts, covering various forgery targets, scenes, behaviors,\nand actions, as well as various generation models with different architectures\nand generation methods, including the most popular commercial models like\nOpenAI's Sora and Google's Veo. Second, we found via probing experiments that\nspatial artifact-based detectors lack generalizability. Hence, we propose a\nsimple yet effective \\textbf{de}tection model based on \\textbf{f}rame\n\\textbf{co}nsistency (\\textbf{DeCoF}), which focuses on temporal artifacts by\neliminating the impact of spatial artifacts during feature learning. Extensive\nexperiments demonstrate the efficacy of DeCoF in detecting videos generated by\nunseen video generation models and confirm its powerful generalizability across\nseveral commercially proprietary models. Our code and dataset will be released\nat \\url{https://github.com/wuwuwuyue/DeCoF}.\n","authors":["Long Ma","Jiajia Zhang","Hongping Deng","Ningyu Zhang","Qinglang Guo","Haiyang Yu","Yong Liao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.02085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01886v3","updated":"2024-06-26T03:29:50Z","published":"2023-12-04T13:40:05Z","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models","summary":"  Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical targeted attack scenario that the\nadversary can only know the vision encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed \\textsc{InstructTA}) to deliver the targeted adversarial attack on\nLVLMs with high transferability. Initially, we utilize a public text-to-image\ngenerative model to \"reverse\" the target response into a target image, and\nemploy GPT-4 to infer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the\ntarget response. We then form a local surrogate model (sharing the same vision\nencoder with the victim LVLM) to extract instruction-aware features of an\nadversarial image example and the target image, and minimize the distance\nbetween these two features to optimize the adversarial example. To further\nimprove the transferability with instruction tuning, we augment the instruction\n$\\boldsymbol{p}^\\prime$ with instructions paraphrased from GPT-4. Extensive\nexperiments demonstrate the superiority of our proposed method in targeted\nattack performance and transferability. The code is available at\nhttps://github.com/xunguangwang/InstructTA.\n","authors":["Xunguang Wang","Zhenlan Ji","Pingchuan Ma","Zongjie Li","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01886v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19684v3","updated":"2024-06-26T03:28:15Z","published":"2024-05-30T04:46:40Z","title":"A Comprehensive Survey on Underwater Image Enhancement Based on Deep\n  Learning","summary":"  Underwater image enhancement (UIE) presents a significant challenge within\ncomputer vision research. Despite the development of numerous UIE algorithms, a\nthorough and systematic review is still absent. To foster future advancements,\nwe provide a detailed overview of the UIE task from several perspectives.\nFirstly, we introduce the physical models, data construction processes,\nevaluation metrics, and loss functions. Secondly, we categorize and discuss\nrecent algorithms based on their contributions, considering six aspects:\nnetwork architecture, learning strategy, learning stage, auxiliary tasks,\ndomain perspective, and disentanglement fusion. Thirdly, due to the varying\nexperimental setups in the existing literature, a comprehensive and unbiased\ncomparison is currently unavailable. To address this, we perform both\nquantitative and qualitative evaluations of state-of-the-art algorithms across\nmultiple benchmark datasets. Lastly, we identify key areas for future research\nin UIE. A collection of resources for UIE can be found at\n{https://github.com/YuZhao1999/UIE}.\n","authors":["Xiaofeng Cong","Yu Zhao","Jie Gui","Junming Hou","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2405.19684v3.pdf","comment":"A survey on the underwater image enhancement task"},{"id":"http://arxiv.org/abs/2406.18037v1","updated":"2024-06-26T03:10:57Z","published":"2024-06-26T03:10:57Z","title":"Towards Synchronous Memorizability and Generalizability with\n  Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation","summary":"  The ability to learn sequentially from different data sites is crucial for a\ndeep network in solving practical medical image diagnosis problems due to\nprivacy restrictions and storage limitations. However, adapting on incoming\nsite leads to catastrophic forgetting on past sites and decreases\ngeneralizablity on unseen sites. Existing Continual Learning (CL) and Domain\nGeneralization (DG) methods have been proposed to solve these two challenges\nrespectively, but none of them can address both simultaneously. Recognizing\nthis limitation, this paper proposes a novel training paradigm, learning\ntowards Synchronous Memorizability and Generalizability (SMG-Learning). To\nachieve this, we create the orientational gradient alignment to ensure\nmemorizability on previous sites, and arbitrary gradient alignment to enhance\ngeneralizability on unseen sites. This approach is named as Parallel Gradient\nAlignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives\nusing the first-order Taylor expansion to reduce computational cost of aligning\ngradients. Considering that performing gradient alignments, especially for\nprevious sites, is not feasible due to the privacy constraints, we design a\nSite-Modulated Diffusion (SMD) model to generate images with site-specific\nlearnable prompts, replaying images have similar data distributions as previous\nsites. We evaluate our method on two medical image segmentation tasks, where\ndata from different sites arrive sequentially. Experimental results show that\nour method efficiently enhances both memorizability and generalizablity better\nthan other state-of-the-art methods, delivering satisfactory performance across\nall sites. Our code will be available at:\nhttps://github.com/dyxu-cuhkcse/SMG-Learning.\n","authors":["Dunyuan Xu","Xi Wang","Jingyang Zhang","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.18037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18031v1","updated":"2024-06-26T03:01:18Z","published":"2024-06-26T03:01:18Z","title":"Real-time Structure Flow","summary":"  This article introduces the structure flow field; a flow field that can\nprovide high-speed robo-centric motion information for motion control of highly\ndynamic robotic devices and autonomous vehicles. Structure flow is the angular\n3D velocity of the scene at a given pixel. We show that structure flow posses\nan elegant evolution model in the form of a Partial Differential Equation (PDE)\nthat enables us to create dense flow predictions forward in time. We exploit\nthis structure to design a predictor-update algorithm to compute structure flow\nin real time using image and depth measurements. The prediction stage takes the\nprevious estimate of the structure flow and propagates it forward in time using\na numerical implementation of the structure flow PDE. The predicted flow is\nthen updated using new image and depth data. The algorithm runs up to 600 Hz on\na Desktop GPU machine for 512x512 images with flow values up to 8 pixels. We\nprovide ground truth validation on high-speed synthetic image sequences as well\nas results on real-life video on driving scenarios.\n","authors":["Juan David Adarve","Robert Mahony"],"pdf_url":"https://arxiv.org/pdf/2406.18031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00618v3","updated":"2024-06-26T02:42:30Z","published":"2023-01-02T12:16:18Z","title":"An Event-based Algorithm for Simultaneous 6-DOF Camera Pose Tracking and\n  Mapping","summary":"  Compared to regular cameras, Dynamic Vision Sensors or Event Cameras can\noutput compact visual data based on a change in the intensity in each pixel\nlocation asynchronously. In this paper, we study the application of current\nimage-based SLAM techniques to these novel sensors. To this end, the\ninformation in adaptively selected event windows is processed to form\nmotion-compensated images. These images are then used to reconstruct the scene\nand estimate the 6-DOF pose of the camera. We also propose an inertial version\nof the event-only pipeline to assess its capabilities. We compare the results\nof different configurations of the proposed algorithm against the ground truth\nfor sequences of two publicly available event datasets. We also compare the\nresults of the proposed event-inertial pipeline with the state-of-the-art and\nshow it can produce comparable or more accurate results provided the map\nestimate is reliable.\n","authors":["Masoud Dayani Najafabadi","Mohammad Reza Ahmadzadeh"],"pdf_url":"https://arxiv.org/pdf/2301.00618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17559v2","updated":"2024-06-26T02:23:32Z","published":"2024-06-25T13:54:39Z","title":"Minimal Interaction Edge Tuning: A New Paradigm for Visual Adaptation","summary":"  The rapid scaling of large vision pretrained models makes fine-tuning tasks\nmore and more difficult on edge devices with low computational resources. We\nexplore a new visual adaptation paradigm called edge tuning, which treats large\npretrained models as standalone feature extractors that run on powerful cloud\nservers. The fine-tuning carries out on edge devices with small networks which\nrequire low computational resources. Existing methods that are potentially\nsuitable for our edge tuning paradigm are discussed. But, three major drawbacks\nhinder their application in edge tuning: low adaptation capability, large\nadapter network, and high information transfer overhead. To address these\nissues, we propose Minimal Interaction Edge Tuning, or MIET, which reveals that\nthe sum of intermediate features from pretrained models not only has minimal\ninformation transfer but also has high adaptation capability. With a\nlightweight attention-based adaptor network, MIET achieves information transfer\nefficiency, parameter efficiency, computational and memory efficiency, and at\nthe same time demonstrates competitive results on various visual adaptation\nbenchmarks.\n","authors":["Ningyuan Tang","Minghao Fu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17559v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.17254v2","updated":"2024-06-26T02:21:00Z","published":"2024-06-25T03:42:29Z","title":"Scalp Diagnostic System With Label-Free Segmentation and Training-Free\n  Image Translation","summary":"  Scalp diseases and alopecia affect millions of people around the world,\nunderscoring the urgent need for early diagnosis and management of the disease.\nHowever, the development of a comprehensive AI-based diagnosis system\nencompassing these conditions remains an underexplored domain due to the\nchallenges associated with data imbalance and the costly nature of labeling. To\naddress these issues, we propose ScalpVision, an AI-driven system for the\nholistic diagnosis of scalp diseases and alopecia. In ScalpVision, effective\nhair segmentation is achieved using pseudo image-label pairs and an innovative\nprompting method in the absence of traditional hair masking labels. This\napproach is crucial for extracting key features such as hair thickness and\ncount, which are then used to assess alopecia severity. Additionally,\nScalpVision introduces DiffuseIT-M, a generative model adept at dataset\naugmentation while maintaining hair information, facilitating improved\npredictions of scalp disease severity. Our experimental results affirm\nScalpVision's efficiency in diagnosing a variety of scalp conditions and\nalopecia, showcasing its potential as a valuable tool in dermatological care.\n","authors":["Youngmin Kim","Saejin Kim","Hoyeon Moon","Youngjae Yu","Junhyug Noh"],"pdf_url":"https://arxiv.org/pdf/2406.17254v2.pdf","comment":"IEEE Transactions on Medical Imaging (Under Review)"},{"id":"http://arxiv.org/abs/2404.17876v2","updated":"2024-06-26T02:14:32Z","published":"2024-04-27T12:19:23Z","title":"DF-SLAM: Dictionary Factors Representation for High-Fidelity Neural\n  Implicit Dense Visual SLAM System","summary":"  We introduce a high-fidelity neural implicit dense visual Simultaneous\nLocalization and Mapping (SLAM) system, termed DF-SLAM. In our work, we employ\ndictionary factors for scene representation, encoding the geometry and\nappearance information of the scene as a combination of basis and coefficient\nfactors. Compared to neural implicit dense visual SLAM methods that directly\nencode scene information as features, our method exhibits superior scene detail\nreconstruction capabilities and more efficient memory usage, while our model\nsize is insensitive to the size of the scene map, making our method more\nsuitable for large-scale scenes. Additionally, we employ feature integration\nrendering to accelerate color rendering speed while ensuring color rendering\nquality, further enhancing the real-time performance of our neural SLAM method.\nExtensive experiments on synthetic and real-world datasets demonstrate that our\nmethod is competitive with existing state-of-the-art neural implicit SLAM\nmethods in terms of real-time performance, localization accuracy, and scene\nreconstruction quality. Our source code is available at\nhttps://github.com/funcdecl/DF-SLAM.\n","authors":["Weifeng Wei","Jie Wang","Shuqi Deng","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2404.17876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18012v1","updated":"2024-06-26T01:54:10Z","published":"2024-06-26T01:54:10Z","title":"View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with\n  Adaptive View Synthesis","summary":"  The inspection and monitoring of infrastructure assets typically requires\nidentifying visual anomalies in scenes periodically photographed over time.\nImages collected manually or with robots such as unmanned aerial vehicles from\nthe same scene at different instances in time are typically not perfectly\naligned. Supervised segmentation methods can be applied to identify known\nproblems, but unsupervised anomaly detection approaches are required when\nunknown anomalies occur. Current unsupervised pixel-level anomaly detection\nmethods have mainly been developed for industrial settings where the camera\nposition is known and constant. However, we find that these methods fail to\ngeneralize to the case when images are not perfectly aligned. We term the\nproblem of unsupervised anomaly detection between two such imperfectly aligned\nsets of images as Scene Anomaly Detection (Scene AD). We present a novel\nnetwork termed OmniAD to address the Scene AD problem posed. Specifically, we\nrefine the anomaly detection method reverse distillation to achieve a 40%\nincrease in pixel-level anomaly detection performance. The network's\nperformance is further demonstrated to improve with two new data augmentation\nstrategies proposed that leverage novel view synthesis and camera localization\nto improve generalization. We validate our approach with qualitative and\nquantitative results on a new dataset, ToyCity, the first Scene AD dataset with\nmultiple objects, as well as on the established single object-centric dataset,\nMAD. https://drags99.github.io/OmniAD/\n","authors":["Subin Varghese","Vedhus Hoskere"],"pdf_url":"https://arxiv.org/pdf/2406.18012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18011v1","updated":"2024-06-26T01:48:56Z","published":"2024-06-26T01:48:56Z","title":"Expressive Keypoints for Skeleton-based Action Recognition via Skeleton\n  Transformation","summary":"  In the realm of skeleton-based action recognition, the traditional methods\nwhich rely on coarse body keypoints fall short of capturing subtle human\nactions. In this work, we propose Expressive Keypoints that incorporates hand\nand foot details to form a fine-grained skeletal representation, improving the\ndiscriminative ability for existing models in discerning intricate actions. To\nefficiently model Expressive Keypoints, the Skeleton Transformation strategy is\npresented to gradually downsample the keypoints and prioritize prominent joints\nby allocating the importance weights. Additionally, a plug-and-play Instance\nPooling module is exploited to extend our approach to multi-person scenarios\nwithout surging computation costs. Extensive experimental results over seven\ndatasets present the superiority of our method compared to the state-of-the-art\nfor skeleton-based human action recognition. Code is available at\nhttps://github.com/YijieYang23/SkeleT-GCN.\n","authors":["Yijie Yang","Jinlu Zhang","Jiaxu Zhang","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2406.18011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10912v3","updated":"2024-06-26T01:30:49Z","published":"2023-10-17T01:12:08Z","title":"Towards Training-free Open-world Segmentation via Image Prompt\n  Foundation Models","summary":"  The realm of computer vision has witnessed a paradigm shift with the advent\nof foundational models, mirroring the transformative influence of large\nlanguage models in the domain of natural language processing. This paper delves\ninto the exploration of open-world segmentation, presenting a novel approach\ncalled Image Prompt Segmentation (IPSeg) that harnesses the power of vision\nfoundational models. IPSeg lies the principle of a training-free paradigm,\nwhich capitalizes on image prompt techniques. Specifically, IPSeg utilizes a\nsingle image containing a subjective visual concept as a flexible prompt to\nquery vision foundation models like DINOv2 and Stable Diffusion. Our approach\nextracts robust features for the prompt image and input image, then matches the\ninput representations to the prompt representations via a novel feature\ninteraction module to generate point prompts highlighting target objects in the\ninput image. The generated point prompts are further utilized to guide the\nSegment Anything Model to segment the target object in the input image. The\nproposed method stands out by eliminating the need for exhaustive training\nsessions, thereby offering a more efficient and scalable solution. Experiments\non COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for\nflexible open-world segmentation using intuitive image prompts. This work\npioneers tapping foundation models for open-world understanding through visual\nconcepts conveyed in images.\n","authors":["Lv Tang","Peng-Tao Jiang","Hao-Ke Xiao","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2310.10912v3.pdf","comment":"This paper is accepted by IJCV2024"},{"id":"http://arxiv.org/abs/2406.17998v1","updated":"2024-06-26T01:03:39Z","published":"2024-06-26T01:03:39Z","title":"Changen2: Multi-Temporal Remote Sensing Generative Change Foundation\n  Model","summary":"  Our understanding of the temporal dynamics of the Earth's surface has been\nadvanced by deep vision models, which often require lots of labeled\nmulti-temporal images for training. However, collecting, preprocessing, and\nannotating multi-temporal remote sensing images at scale is non-trivial since\nit is expensive and knowledge-intensive. In this paper, we present change data\ngenerators based on generative models, which are cheap and automatic,\nalleviating these data problems. Our main idea is to simulate a stochastic\nchange process over time. We describe the stochastic change process as a\nprobabilistic graphical model (GPCM), which factorizes the complex simulation\nproblem into two more tractable sub-problems, i.e., change event simulation and\nsemantic change synthesis. To solve these two problems, we present Changen2, a\nGPCM with a resolution-scalable diffusion transformer which can generate time\nseries of images and their semantic and change labels from labeled or unlabeled\nsingle-temporal images. Changen2 is a generative change foundation model that\ncan be trained at scale via self-supervision, and can produce change\nsupervisory signals from unlabeled single-temporal images. Unlike existing\nfoundation models, Changen2 synthesizes change data to train task-specific\nfoundation models for change detection. The resulting model possesses inherent\nzero-shot change detection capabilities and excellent transferability.\nExperiments suggest Changen2 has superior spatiotemporal scalability, e.g.,\nChangen2 model trained on 256$^2$ pixel single-temporal images can yield time\nseries of any length and resolutions of 1,024$^2$ pixels. Changen2 pre-trained\nmodels exhibit superior zero-shot performance (narrowing the performance gap to\n3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to\nfully supervised counterparts) and transferability across multiple types of\nchange tasks.\n","authors":["Zhuo Zheng","Stefano Ermon","Dongjun Kim","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.17998v1.pdf","comment":"The enhanced extension of our ICCV 2023 (Changen)"},{"id":"http://arxiv.org/abs/2406.17988v1","updated":"2024-06-26T00:08:29Z","published":"2024-06-26T00:08:29Z","title":"DICE: End-to-end Deformation Capture of Hand-Face Interactions from a\n  Single Image","summary":"  Reconstructing 3D hand-face interactions with deformations from a single\nimage is a challenging yet crucial task with broad applications in AR, VR, and\ngaming. The challenges stem from self-occlusions during single-view hand-face\ninteractions, diverse spatial relationships between hands and face, complex\ndeformations, and the ambiguity of the single-view setting. The first and only\nmethod for hand-face interaction recovery, Decaf, introduces a global fitting\noptimization guided by contact and deformation estimation networks trained on\nstudio-collected data with 3D annotations. However, Decaf suffers from a\ntime-consuming optimization process and limited generalization capability due\nto its reliance on 3D annotations of hand-face interaction data. To address\nthese issues, we present DICE, the first end-to-end method for\nDeformation-aware hand-face Interaction reCovEry from a single image. DICE\nestimates the poses of hands and faces, contacts, and deformations\nsimultaneously using a Transformer-based architecture. It features\ndisentangling the regression of local deformation fields and global mesh vertex\nlocations into two network branches, enhancing deformation and contact\nestimation for precise and robust hand-face mesh recovery. To improve\ngeneralizability, we propose a weakly-supervised training approach that\naugments the training set using in-the-wild images without 3D ground-truth\nannotations, employing the depths of 2D keypoints estimated by off-the-shelf\nmodels and adversarial priors of poses for supervision. Our experiments\ndemonstrate that DICE achieves state-of-the-art performance on a standard\nbenchmark and in-the-wild data in terms of accuracy and physical plausibility.\nAdditionally, our method operates at an interactive rate (20 fps) on an Nvidia\n4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our\ncode will be publicly available upon publication.\n","authors":["Qingxuan Wu","Zhiyang Dou","Sirui Xu","Soshi Shimada","Chen Wang","Zhengming Yu","Yuan Liu","Cheng Lin","Zeyu Cao","Taku Komura","Vladislav Golyanik","Christian Theobalt","Wenping Wang","Lingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17988v1.pdf","comment":"23 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.18790v1","updated":"2024-06-26T23:21:42Z","published":"2024-06-26T23:21:42Z","title":"MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data","summary":"  We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.\n","authors":["William Berman","Alexander Peysakhovich"],"pdf_url":"https://arxiv.org/pdf/2406.18790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13880v3","updated":"2024-06-26T22:43:05Z","published":"2024-04-22T05:07:02Z","title":"Regional Style and Color Transfer","summary":"  This paper presents a novel contribution to the field of regional style\ntransfer. Existing methods often suffer from the drawback of applying style\nhomogeneously across the entire image, leading to stylistic inconsistencies or\nforeground object twisted when applied to image with foreground elements such\nas person figures. To address this limitation, we propose a new approach that\nleverages a segmentation network to precisely isolate foreground objects within\nthe input image. Subsequently, style transfer is applied exclusively to the\nbackground region. The isolated foreground objects are then carefully\nreintegrated into the style-transferred background. To enhance the visual\ncoherence between foreground and background, a color transfer step is employed\non the foreground elements prior to their rein-corporation. Finally, we utilize\nfeathering techniques to achieve a seamless amalgamation of foreground and\nbackground, resulting in a visually unified and aesthetically pleasing final\ncomposition. Extensive evaluations demonstrate that our proposed approach\nyields significantly more natural stylistic transformations compared to\nconventional methods.\n","authors":["Zhicheng Ding","Panfeng Li","Qikai Yang","Siyang Li","Qingtian Gong"],"pdf_url":"https://arxiv.org/pdf/2404.13880v3.pdf","comment":"Accepted by 2024 5th International Conference on Computer Vision,\n  Image and Deep Learning"},{"id":"http://arxiv.org/abs/2406.14815v2","updated":"2024-06-26T22:23:23Z","published":"2024-06-21T01:32:03Z","title":"Latent diffusion models for parameterization and data assimilation of\n  facies-based geomodels","summary":"  Geological parameterization entails the representation of a geomodel using a\nsmall set of latent variables and a mapping from these variables to grid-block\nproperties such as porosity and permeability. Parameterization is useful for\ndata assimilation (history matching), as it maintains geological realism while\nreducing the number of variables to be determined. Diffusion models are a new\nclass of generative deep-learning procedures that have been shown to outperform\nprevious methods, such as generative adversarial networks, for image generation\ntasks. Diffusion models are trained to \"denoise\", which enables them to\ngenerate new geological realizations from input fields characterized by random\nnoise. Latent diffusion models, which are the specific variant considered in\nthis study, provide dimension reduction through use of a low-dimensional latent\nvariable. The model developed in this work includes a variational autoencoder\nfor dimension reduction and a U-net for the denoising process. Our application\ninvolves conditional 2D three-facies (channel-levee-mud) systems. The latent\ndiffusion model is shown to provide realizations that are visually consistent\nwith samples from geomodeling software. Quantitative metrics involving spatial\nand flow-response statistics are evaluated, and general agreement between the\ndiffusion-generated models and reference realizations is observed. Stability\ntests are performed to assess the smoothness of the parameterization method.\nThe latent diffusion model is then used for ensemble-based data assimilation.\nTwo synthetic \"true\" models are considered. Significant uncertainty reduction,\nposterior P$_{10}$-P$_{90}$ forecasts that generally bracket observed data, and\nconsistent posterior geomodels, are achieved in both cases.\n","authors":["Guido Di Federico","Louis J. Durlofsky"],"pdf_url":"https://arxiv.org/pdf/2406.14815v2.pdf","comment":"- Moved Table 1 from before to after Section 4.2 heading - Renamed\n  output pdf file with paper title"},{"id":"http://arxiv.org/abs/2406.18765v1","updated":"2024-06-26T21:30:41Z","published":"2024-06-26T21:30:41Z","title":"WV-Net: A foundation model for SAR WV-mode satellite imagery trained\n  using contrastive self-supervised learning on 10 million images","summary":"  The European Space Agency's Copernicus Sentinel-1 (S-1) mission is a\nconstellation of C-band synthetic aperture radar (SAR) satellites that provide\nunprecedented monitoring of the world's oceans. S-1's wave mode (WV) captures\n20x20 km image patches at 5 m pixel resolution and is unaffected by cloud cover\nor time-of-day. The mission's open data policy has made SAR data easily\naccessible for a range of applications, but the need for manual image\nannotations is a bottleneck that hinders the use of machine learning methods.\nThis study uses nearly 10 million WV-mode images and contrastive\nself-supervised learning to train a semantic embedding model called WV-Net. In\nmultiple downstream tasks, WV-Net outperforms a comparable model that was\npre-trained on natural images (ImageNet) with supervised learning. Experiments\nshow improvements for estimating wave height (0.50 vs 0.60 RMSE using linear\nprobing), estimating near-surface air temperature (0.90 vs 0.97 RMSE), and\nperforming multilabel-classification of geophysical and atmospheric phenomena\n(0.96 vs 0.95 micro-averaged AUROC). WV-Net embeddings are also superior in an\nunsupervised image-retrieval task and scale better in data-sparse settings.\nTogether, these results demonstrate that WV-Net embeddings can support\ngeophysical research by providing a convenient foundation model for a variety\nof data analysis and exploration tasks.\n","authors":["Yannik Glaser","Justin E. Stopa","Linnea M. Wolniewicz","Ralph Foster","Doug Vandemark","Alexis Mouche","Bertrand Chapron","Peter Sadowski"],"pdf_url":"https://arxiv.org/pdf/2406.18765v1.pdf","comment":"20 pages, 9 figures, submitted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.04929v3","updated":"2024-06-26T20:57:15Z","published":"2024-02-07T14:56:13Z","title":"Source-Free Domain Adaptation with Diffusion-Guided Source Data\n  Generation","summary":"  This paper introduces a novel approach to leverage the generalizability of\nDiffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed\nDMSFDA method involves fine-tuning a pre-trained text-to-image diffusion model\nto generate source domain images using features from the target images to guide\nthe diffusion process. Specifically, the pre-trained diffusion model is\nfine-tuned to generate source samples that minimize entropy and maximize\nconfidence for the pre-trained source model. We then use a diffusion\nmodel-based image mixup strategy to bridge the domain gap between the source\nand target domains. We validate our approach through comprehensive experiments\nacross a range of datasets, including Office-31, Office-Home, and VisDA. The\nresults demonstrate significant improvements in SFDA performance, highlighting\nthe potential of diffusion models in generating contextually relevant,\ndomain-specific images.\n","authors":["Shivang Chopra","Suraj Kothawade","Houda Aynaou","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.04929v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2310.01701"},{"id":"http://arxiv.org/abs/2406.17173v2","updated":"2024-06-26T20:54:45Z","published":"2024-06-24T23:23:18Z","title":"Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT\n  Classification with Transformer Networks","summary":"  The manifestation of symptoms associated with lung diseases can vary in\ndifferent depths for individual patients, highlighting the significance of 3D\ninformation in CT scans for medical image classification. While Vision\nTransformer has shown superior performance over convolutional neural networks\nin image classification tasks, their effectiveness is often demonstrated on\nsufficiently large 2D datasets and they easily encounter overfitting issues on\nsmall medical image datasets. To address this limitation, we propose a\nDiffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent\nspace of the Diffusion model to form the slice sequence for 3D analysis and\nincorporates clustering attention into ViT to aggregate repetitive information\nwithin 3D CT scans, thereby harnessing the power of the advanced transformer in\n3D classification tasks on small datasets. Our method exhibits improved\nperformance on two different scales of small datasets of 3D lung CT scans,\nsurpassing the state of the art 3D methods and other transformer-based\napproaches that emerged during the COVID-19 pandemic, demonstrating its robust\nand superior performance across different scales of data. Experimental results\nunderscore the superiority of our proposed method, indicating its potential for\nenhancing medical image classification tasks in real-world scenarios.\n","authors":["Zihao Jin","Yingying Fang","Jiahao Huang","Caiwen Xu","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17173v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2308.14936v3","updated":"2024-06-26T20:25:03Z","published":"2023-08-28T23:23:53Z","title":"AutoProSAM: Automated Prompting SAM for 3D Multi-Organ Segmentation","summary":"  Segment Anything Model (SAM) is one of the pioneering prompt-based foundation\nmodels for image segmentation and has been rapidly adopted for various medical\nimaging applications. However, in clinical settings, creating effective prompts\nis notably challenging and time-consuming, requiring the expertise of domain\nspecialists such as physicians. This requirement significantly diminishes SAM's\nprimary advantage - its interactive capability with end users - in medical\napplications. Moreover, recent studies have indicated that SAM, originally\ndesigned for 2D natural images, performs sub optimally on 3D medical image\nsegmentation tasks. This subpar performance is attributed to the domain gaps\nbetween natural and medical images and the disparities in spatial arrangements\nbetween 2D and 3D images, particularly in multi-organ segmentation\napplications. To overcome these challenges, we present a novel technique termed\nAutoProSAM. This method automates 3D multi-organ CT-based segmentation by\nleveraging SAM's foundational model capabilities without relying on domain\nexperts for prompts. The approach utilizes parameter-efficient adaptation\ntechniques to adapt SAM for 3D medical imagery and incorporates an effective\nautomatic prompt learning paradigm specific to this domain. By eliminating the\nneed for manual prompts, it enhances SAM's capabilities for 3D medical image\nsegmentation and achieves state-of-the-art (SOTA) performance in CT-based\nmulti-organ segmentation tasks.\n","authors":["Chengyin Li","Prashant Khanduri","Yao Qiang","Rafi Ibn Sultan","Indrin Chetty","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2308.14936v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18742v1","updated":"2024-06-26T20:16:49Z","published":"2024-06-26T20:16:49Z","title":"3D Feature Distillation with Object-Centric Priors","summary":"  Grounding natural language to the physical world is a ubiquitous topic with a\nwide range of applications in computer vision and robotics. Recently, 2D\nvision-language models such as CLIP have been widely popularized, due to their\nimpressive capabilities for open-vocabulary grounding in 2D images. Recent\nworks aim to elevate 2D CLIP features to 3D via feature distillation, but\neither learn neural fields that are scene-specific and hence lack\ngeneralization, or focus on indoor room scan data that require access to\nmultiple camera views, which is not practical in robot manipulation scenarios.\nAdditionally, related methods typically fuse features at pixel-level and assume\nthat all camera views are equally informative. In this work, we show that this\napproach leads to sub-optimal 3D features, both in terms of grounding accuracy,\nas well as segmentation crispness. To alleviate this, we propose a multi-view\nfeature fusion strategy that employs object-centric priors to eliminate\nuninformative views based on semantic information, and fuse features at\nobject-level via instance segmentation masks. To distill our object-centric 3D\nfeatures, we generate a large-scale synthetic multi-view dataset of cluttered\ntabletop scenes, spawning 15k scenes from over 3300 unique object instances,\nwhich we make publicly available. We show that our method reconstructs 3D CLIP\nfeatures with improved grounding capacity and spatial consistency, while doing\nso from single-view RGB-D, thus departing from the assumption of multiple\ncamera views at test time. Finally, we show that our approach can generalize to\nnovel tabletop domains and be re-purposed for 3D instance segmentation without\nfine-tuning, and demonstrate its utility for language-guided robotic grasping\nin clutter\n","authors":["Georgios Tziafas","Yucheng Xu","Zhibin Li","Hamidreza Kasaei"],"pdf_url":"https://arxiv.org/pdf/2406.18742v1.pdf","comment":"Submitted CoRL-24"},{"id":"http://arxiv.org/abs/2404.07097v2","updated":"2024-06-26T20:09:12Z","published":"2024-04-10T15:37:00Z","title":"Fast Encoder-Based 3D from Casual Videos via Point Track Processing","summary":"  This paper addresses the long-standing challenge of reconstructing 3D\nstructures from videos with dynamic content. Current approaches to this problem\nwere not designed to operate on casual videos recorded by standard cameras or\nrequire a long optimization time.\n  Aiming to significantly improve the efficiency of previous approaches, we\npresent TracksTo4D, a learning-based approach that enables inferring 3D\nstructure and camera positions from dynamic content originating from casual\nvideos using a single efficient feed-forward pass. To achieve this, we propose\noperating directly over 2D point tracks as input and designing an architecture\ntailored for processing 2D point tracks. Our proposed architecture is designed\nwith two key principles in mind: (1) it takes into account the inherent\nsymmetries present in the input point tracks data, and (2) it assumes that the\nmovement patterns can be effectively represented using a low-rank\napproximation. TracksTo4D is trained in an unsupervised way on a dataset of\ncasual videos utilizing only the 2D point tracks extracted from the videos,\nwithout any 3D supervision. Our experiments show that TracksTo4D can\nreconstruct a temporal point cloud and camera positions of the underlying video\nwith accuracy comparable to state-of-the-art methods, while drastically\nreducing runtime by up to 95\\%. We further show that TracksTo4D generalizes\nwell to unseen videos of unseen semantic categories at inference time.\n","authors":["Yoni Kasten","Wuyue Lu","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2404.07097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18722v1","updated":"2024-06-26T19:42:08Z","published":"2024-06-26T19:42:08Z","title":"Towards Open-World Grasping with Large Vision-Language Models","summary":"  The ability to grasp objects in-the-wild from open-ended language\ninstructions constitutes a fundamental challenge in robotics. An open-world\ngrasping system should be able to combine high-level contextual with low-level\nphysical-geometric reasoning in order to be applicable in arbitrary scenarios.\nRecent works exploit the web-scale knowledge inherent in large language models\n(LLMs) to plan and reason in robotic context, but rely on external vision and\naction models to ground such knowledge into the environment and parameterize\nactuation. This setup suffers from two major bottlenecks: a) the LLM's\nreasoning capacity is constrained by the quality of visual grounding, and b)\nLLMs do not contain low-level spatial understanding of the world, which is\nessential for grasping in contact-rich scenarios. In this work we demonstrate\nthat modern vision-language models (VLMs) are capable of tackling such\nlimitations, as they are implicitly grounded and can jointly reason about\nsemantics and geometry. We propose OWG, an open-world grasping pipeline that\ncombines VLMs with segmentation and grasp synthesis models to unlock grounded\nworld understanding in three stages: open-ended referring segmentation,\ngrounded grasp planning and grasp ranking via contact reasoning, all of which\ncan be applied zero-shot via suitable visual prompting mechanisms. We conduct\nextensive evaluation in cluttered indoor scene datasets to showcase OWG's\nrobustness in grounding from open-ended language, as well as open-world robotic\ngrasping experiments in both simulation and hardware that demonstrate superior\nperformance compared to previous supervised and zero-shot LLM-based methods.\n","authors":["Georgios Tziafas","Hamidreza Kasaei"],"pdf_url":"https://arxiv.org/pdf/2406.18722v1.pdf","comment":"Submitted CoRL24"},{"id":"http://arxiv.org/abs/2406.18717v1","updated":"2024-06-26T19:37:07Z","published":"2024-06-26T19:37:07Z","title":"Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular\n  Videos","summary":"  Gaussian splatting has become a popular representation for novel-view\nsynthesis, exhibiting clear strengths in efficiency, photometric quality, and\ncompositional edibility. Following its success, many works have extended\nGaussians to 4D, showing that dynamic Gaussians maintain these benefits while\nalso tracking scene geometry far better than alternative representations. Yet,\nthese methods assume dense multi-view videos as supervision, constraining their\nuse to controlled capture settings. In this work, we extend the capability of\nGaussian scene representations to casually captured monocular videos. We show\nthat existing 4D Gaussian methods dramatically fail in this setup because the\nmonocular setting is underconstrained. Building off this finding, we propose\nDynamic Gaussian Marbles (DGMarbles), consisting of three core modifications\nthat target the difficulties of the monocular setting. First, DGMarbles uses\nisotropic Gaussian \"marbles\", reducing the degrees of freedom of each Gaussian,\nand constraining the optimization to focus on motion and appearance over local\nshape. Second, DGMarbles employs a hierarchical divide-and-conquer learning\nstrategy to guide the optimization towards solutions with coherent motion.\nFinally, DGMarbles adds image-level and geometry-level priors into the\noptimization, including a tracking loss that takes advantage of recent progress\nin point tracking. By constraining the optimization in these ways, DGMarbles\nlearns Gaussian trajectories that enable novel-view rendering and accurately\ncapture the 3D motion of the scene elements. We evaluate on the (monocular)\nNvidia Dynamic Scenes dataset and the Dycheck iPhone dataset, and show that\nDGMarbles significantly outperforms other Gaussian baselines in quality, and is\non-par with non-Gaussian representations, all while maintaining the efficiency,\ncompositionality, editability, and tracking benefits of Gaussians.\n","authors":["Colton Stearns","Adam Harley","Mikaela Uy","Florian Dubost","Federico Tombari","Gordon Wetzstein","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2406.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18709v1","updated":"2024-06-26T19:18:36Z","published":"2024-06-26T19:18:36Z","title":"SpY: A Context-Based Approach to Spacecraft Component Detection","summary":"  This paper focuses on autonomously characterizing components such as solar\npanels, body panels, antennas, and thrusters of an unknown resident space\nobject (RSO) using camera feed to aid autonomous on-orbit servicing (OOS) and\nactive debris removal. Significant research has been conducted in this area\nusing convolutional neural networks (CNNs). While CNNs are powerful at learning\npatterns and performing object detection, they struggle with missed detections\nand misclassifications in environments different from the training data, making\nthem unreliable for safety in high-stakes missions like OOS. Additionally,\nfailures exhibited by CNNs are often easily rectifiable by humans using\ncommonsense reasoning and contextual knowledge. Embedding such reasoning in an\nobject detector could improve detection accuracy. To validate this hypothesis,\nthis paper presents an end-to-end object detector called SpaceYOLOv2 (SpY),\nwhich leverages the generalizability of CNNs while incorporating contextual\nknowledge using traditional computer vision techniques. SpY consists of two\nmain components: a shape detector and the SpaceYOLO classifier (SYC). The shape\ndetector uses CNNs to detect primitive shapes of RSOs and SYC associates these\nshapes with contextual knowledge, such as color and texture, to classify them\nas spacecraft components or \"unknown\" if the detected shape is uncertain. SpY's\nmodular architecture allows customizable usage of contextual knowledge to\nimprove detection performance, or SYC as a secondary fail-safe classifier with\nan existing spacecraft component detector. Performance evaluations on\nhardware-in-the-loop images of a mock-up spacecraft demonstrate that SpY is\naccurate and an ensemble of SpY with YOLOv5 trained for satellite component\ndetection improved the performance by 23.4% in recall, demonstrating enhanced\nsafety for vision-based navigation tasks.\n","authors":["Trupti Mahendrakar","Ryan T. White","Madhur Tiwari"],"pdf_url":"https://arxiv.org/pdf/2406.18709v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.18691v1","updated":"2024-06-26T18:52:53Z","published":"2024-06-26T18:52:53Z","title":"Geometric Features Enhanced Human-Object Interaction Detection","summary":"  Cameras are essential vision instruments to capture images for pattern\ndetection and measurement. Human-object interaction (HOI) detection is one of\nthe most popular pattern detection approaches for captured human-centric visual\nscenes. Recently, Transformer-based models have become the dominant approach\nfor HOI detection due to their advanced network architectures and thus\npromising results. However, most of them follow the one-stage design of vanilla\nTransformer, leaving rich geometric priors under-exploited and leading to\ncompromised performance especially when occlusion occurs. Given that geometric\nfeatures tend to outperform visual ones in occluded scenarios and offer\ninformation that complements visual cues, we propose a novel end-to-end\nTransformer-style HOI detection model, i.e., geometric features enhanced HOI\ndetector (GeoHOI). One key part of the model is a new unified self-supervised\nkeypoint learning method named UniPointNet that bridges the gap of consistent\nkeypoint representation across diverse object categories, including humans.\nGeoHOI effectively upgrades a Transformer-based HOI detector benefiting from\nthe keypoints similarities measuring the likelihood of human-object\ninteractions as well as local keypoint patches to enhance interaction query\nrepresentation, so as to boost HOI predictions. Extensive experiments show that\nthe proposed method outperforms the state-of-the-art models on V-COCO and\nachieves competitive performance on HICO-DET. Case study results on the\npost-disaster rescue with vision-based instruments showcase the applicability\nof the proposed GeoHOI in real-world applications.\n","authors":["Manli Zhu","Edmond S. L. Ho","Shuang Chen","Longzhi Yang","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2406.18691v1.pdf","comment":"Accepted to IEEE TIM"},{"id":"http://arxiv.org/abs/2406.18684v1","updated":"2024-06-26T18:42:22Z","published":"2024-06-26T18:42:22Z","title":"CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification","summary":"  In recent years, Joint Communication and Sensing (JC&S), has demonstrated\nsignificant success, particularly in utilizing sub-6 GHz frequencies with\ncommercial-off-the-shelf (COTS) Wi-Fi devices for applications such as\nlocalization, gesture recognition, and pose classification. Deep learning and\nthe existence of large public datasets has been pivotal in achieving such\nresults. However, at mmWave frequencies (30-300 GHz), which has shown potential\nfor more accurate sensing performance, there is a noticeable lack of research\nin the domain of COTS Wi-Fi sensing. Challenges such as limited research\nhardware, the absence of large datasets, limited functionality in COTS\nhardware, and the complexities of data collection present obstacles to a\ncomprehensive exploration of this field. In this work, we aim to address these\nchallenges by developing a method that can generate synthetic mmWave channel\nstate information (CSI) samples. In particular, we use a generative adversarial\nnetwork (GAN) on an existing dataset, to generate 30,000 additional CSI\nsamples. The augmented samples exhibit a remarkable degree of consistency with\nthe original data, as indicated by the notably high GAN-train and GAN-test\nscores. Furthermore, we integrate the augmented samples in training a pose\nclassification model. We observe that the augmented samples complement the real\ndata and improve the generalization of the classification model.\n","authors":["Nabeel Nisar Bhat","Rafael Berkvens Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2406.18684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16693v4","updated":"2024-06-26T18:00:02Z","published":"2023-12-27T19:11:50Z","title":"I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models","summary":"  Text-guided image-to-video (I2V) generation aims to generate a coherent video\nthat preserves the identity of the input image and semantically aligns with the\ninput prompt. Existing methods typically augment pretrained text-to-video (T2V)\nmodels by either concatenating the image with noised video frames channel-wise\nbefore being fed into the model or injecting the image embedding produced by\npretrained image encoders in cross-attention modules. However, the former\napproach often necessitates altering the fundamental weights of pretrained T2V\nmodels, thus restricting the model's compatibility within the open-source\ncommunities and disrupting the model's prior knowledge. Meanwhile, the latter\ntypically fails to preserve the identity of the input image. We present\nI2V-Adapter to overcome such limitations. I2V-Adapter adeptly propagates the\nunnoised input image to subsequent noised frames through a cross-frame\nattention mechanism, maintaining the identity of the input image without any\nchanges to the pretrained T2V model. Notably, I2V-Adapter only introduces a few\ntrainable parameters, significantly alleviating the training cost and also\nensures compatibility with existing community-driven personalized models and\ncontrol tools. Moreover, we propose a novel Frame Similarity Prior to balance\nthe motion amplitude and the stability of generated videos through two\nadjustable control coefficients. Our experimental results demonstrate that\nI2V-Adapter is capable of producing high-quality videos. This performance,\ncoupled with its agility and adaptability, represents a substantial advancement\nin the field of I2V, particularly for personalized and controllable\napplications.\n","authors":["Xun Guo","Mingwu Zheng","Liang Hou","Yuan Gao","Yufan Deng","Pengfei Wan","Di Zhang","Yufan Liu","Weiming Hu","Zhengjun Zha","Haibin Huang","Chongyang Ma"],"pdf_url":"https://arxiv.org/pdf/2312.16693v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18628v1","updated":"2024-06-26T16:58:15Z","published":"2024-06-26T16:58:15Z","title":"IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware\n  Underwater Image Enhancement","summary":"  Underwater image quality is affected by fluorescence, low illumination,\nabsorption, and scattering. Recent works in underwater image enhancement have\nproposed different deep network architectures to handle these problems. Most of\nthese works have proposed a single network to handle all the challenges. We\nbelieve that deep networks trained for specific conditions deliver better\nperformance than a single network learned from all degradation cases.\nAccordingly, the first contribution of this work lies in the proposal of an\niterative framework where a single dominant degradation condition is identified\nand resolved. This proposal considers the following eight degradation\nconditions -- low illumination, low contrast, haziness, blurred image, presence\nof noise and color imbalance in three different channels. A deep network is\ndesigned to identify the dominant degradation condition. Accordingly, an\nappropriate deep network is selected for degradation condition-specific\nenhancement. The second contribution of this work is the construction of\ndegradation condition specific datasets from good quality images of two\nstandard datasets (UIEB and EUVP). This dataset is used to learn the condition\nspecific enhancement networks. The proposed approach is found to outperform\nnine baseline methods on UIEB and EUVP datasets.\n","authors":["Pranjali Singh","Prithwijit Guha"],"pdf_url":"https://arxiv.org/pdf/2406.18628v1.pdf","comment":null}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2406.18460v1","updated":"2024-06-26T16:10:53Z","published":"2024-06-26T16:10:53Z","title":"Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain\n  Human-Machine Conversation","summary":"  Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.\n","authors":["Ahmed Njifenjou","Virgile Sucal","Bassam Jabaian","Fabrice Lefèvre"],"pdf_url":"https://arxiv.org/pdf/2406.18460v1.pdf","comment":"Updated version of a paper originally submitted at SIGDIAL 2023"},{"id":"http://arxiv.org/abs/2402.15340v4","updated":"2024-06-26T14:22:02Z","published":"2024-02-23T14:21:11Z","title":"MetaStates: An Approach for Representing Human Workers'\n  Psychophysiological States in the Industrial Metaverse","summary":"  Photo-realistic avatar is a modern term referring to the digital asset that\nrepresents a human in computer graphic advanced systems such as video games and\nsimulation tools. These avatars utilize the advances in graphic technologies in\nboth software and hardware aspects. While photo-realistic avatars are\nincreasingly used in industrial simulations, representing human factors such as\nhuman workers psychophysiological states, remains a challenge. This article\ncontributes to resolving this issue by introducing the novel concept of\nMetaStates which are the digitization and representation of the\npsychophysiological states of a human worker in the digital world. The\nMetaStates influence the physical representation and performance of a digital\nhuman worker while performing a task. To demonstrate this concept, this study\npresents the development of a photo-realistic avatar enhanced with multi-level\ngraphical representations of psychophysiological states relevant to Industry\n5.0. This approach represents a major step forward in the use of digital humans\nfor industrial simulations, allowing companies to better leverage the benefits\nof the Industrial Metaverse in their daily operations and simulations while\nkeeping human workers at the center of the system.\n","authors":["Aitor Toichoa Eyam","Jose L. Martinez Lastra"],"pdf_url":"https://arxiv.org/pdf/2402.15340v4.pdf","comment":"10 pages, 6 figures, 4 tables, journal"},{"id":"http://arxiv.org/abs/2406.18336v1","updated":"2024-06-26T13:24:08Z","published":"2024-06-26T13:24:08Z","title":"An interactive framework for the evaluation and detection of\n  stereoacuity threshold under ambient lighting","summary":"  Objective: Our study aims to provide a novel framework for the continuous\nevaluation of stereoacuity under ambient lighting conditions using Bayesian\ninference.\n  Methods: We applied a combination of psychophysical and expected entropy\nminimization procedures for the computation of a continuous stereoacuity\nthreshold. Subsequently, we evaluated the effect of ambient lighting during\nstereoacuity testing (ST) by adopting a bisection-matching based adaptive gamma\ncalibration (AGC). Participants ($N=187$) including visually healthy controls\n($N=51$), patients with Intermittent Divergent Squint (IDS; $N=45$), and\ncontrols with induced anisometropia (IA; $N=91$) performed ST with and without\nAGC under two lighting conditions: completely dark (20 cd/m$^2$) and normally\nlit (130 cd/m$^2$) rooms.\n  Results: Our framework demonstrated \"excellent\" reliability ($> 0.9$) and a\npositive correlation with TNO (a clinical stereo test), regardless of whether\nAGC was conducted. However, when AGC is not performed, significant differences\n(Friedman $X_{r}^{2} = 28.015$; $p<0.00001$; Bland-Altman bias: 30 arc-sec)\nwere found in stereoacuity thresholds between dark and light conditions for\nparticipants with IDS and IA. Controls are unaffected by AGC and yield a\nsimilar stereoacuity threshold under both lighting conditions.\n  Conclusion: Our study proves that stereoacuity threshold is significantly\ndeviated particularly in participants with IDS or IA stereo-deficits if ambient\nlighting is not taken into consideration. Moreover, our framework provides a\nquick (approximately 5-10 minutes) assessment of stereoacuity threshold and can\nbe performed within 30 ST and 15 AGC trials.\n  Significance: Our test is useful in planning treatments and monitoring\nprognosis for patients with stereo-deficits by accurately assessing\nstereovision.\n","authors":["Kritika Lohia","Rijul Saurabh Soans","Rohit Saxena","Tapan Kumar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2406.18336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18162v1","updated":"2024-06-26T08:23:13Z","published":"2024-06-26T08:23:13Z","title":"Multimodal Reaching-Position Prediction for ADL Support Using Neural\n  Networks","summary":"  This study aimed to develop daily living support robots for patients with\nhemiplegia and the elderly. To support the daily living activities using robots\nin ordinary households without imposing physical and mental burdens on users,\nthe system must detect the actions of the user and move appropriately according\nto their motions.\n  We propose a reaching-position prediction scheme that targets the motion of\nlifting the upper arm, which is burdensome for patients with hemiplegia and the\nelderly in daily living activities.\n  For this motion, it is difficult to obtain effective features to create a\nprediction model in environments where large-scale sensor system installation\nis not feasible and the motion time is short.\n  We performed motion-collection experiments, revealed the features of the\ntarget motion and built a prediction model using the multimodal motion features\nand deep learning.\n  The proposed model achieved an accuracy of 93 \\% macro average and F1-score\nof 0.69 for a 9-class classification prediction at 35\\% of the motion\ncompletion.\n","authors":["Yutaka Takase","Kimitoshi Yamazaki"],"pdf_url":"https://arxiv.org/pdf/2406.18162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00721v2","updated":"2024-06-26T07:10:49Z","published":"2023-10-30T08:34:12Z","title":"Empathy Detection from Text, Audiovisual, Audio or Physiological\n  Signals: Task Formulations and Machine Learning Methods","summary":"  Empathy indicates an individual's ability to understand others. Over the past\nfew years, empathy has drawn attention from various disciplines, including but\nnot limited to Affective Computing, Cognitive Science and Psychology. Detecting\nempathy has potential applications in society, healthcare and education.\nDespite being a broad and overlapping topic, the avenue of empathy detection\nleveraging Machine Learning remains underexplored from a systematic literature\nreview perspective. We collected 828 papers from 10 well-known databases,\nsystematically screened them and analysed the final 61 papers. Our analyses\nreveal several prominent task formulations $-$ including empathy on localised\nutterances or overall expressions, unidirectional or parallel empathy, and\nemotional contagion $-$ in monadic, dyadic and group interactions. Empathy\ndetection methods are summarised based on four input modalities $-$ text,\naudiovisual, audio and physiological signals $-$ thereby presenting\nmodality-specific network architecture design protocols. We discuss challenges,\nresearch gaps and potential applications in the Affective Computing-based\nempathy domain, which can facilitate new avenues of exploration. We further\nenlist the public availability of datasets and codes. We believe that our work\nis a stepping stone to developing a robust empathy detection system that can be\ndeployed in practice to enhance the overall well-being of human life.\n","authors":["Md Rakibul Hasan","Md Zakir Hossain","Shreya Ghosh","Aneesh Krishna","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2311.00721v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice"},{"id":"http://arxiv.org/abs/2406.18116v1","updated":"2024-06-26T07:07:52Z","published":"2024-06-26T07:07:52Z","title":"BADGE: BADminton report Generation and Evaluation with LLM","summary":"  Badminton enjoys widespread popularity, and reports on matches generally\ninclude details such as player names, game scores, and ball types, providing\naudiences with a comprehensive view of the games. However, writing these\nreports can be a time-consuming task. This challenge led us to explore whether\na Large Language Model (LLM) could automate the generation and evaluation of\nbadminton reports. We introduce a novel framework named BADGE, designed for\nthis purpose using LLM. Our method consists of two main phases: Report\nGeneration and Report Evaluation. Initially, badminton-related data is\nprocessed by the LLM, which then generates a detailed report of the match. We\ntested different Input Data Types, In-Context Learning (ICL), and LLM, finding\nthat GPT-4 performs best when using CSV data type and the Chain of Thought\nprompting. Following report generation, the LLM evaluates and scores the\nreports to assess their quality. Our comparisons between the scores evaluated\nby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.\nSince the application of LLM in badminton reporting remains largely unexplored,\nour research serves as a foundational step for future advancements in this\narea. Moreover, our method can be extended to other sports games, thereby\nenhancing sports promotion. For more details, please refer to\nhttps://github.com/AndyChiangSH/BADGE.\n","authors":["Shang-Hsuan Chiang","Lin-Wei Chao","Kuang-Da Wang","Chih-Chuan Wang","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2406.18116v1.pdf","comment":"Accepted by IJCAI 2024 Workshop: The 2nd International Workshop on\n  Intelligent Technologies for Precision Sports Science (IT4PSS)"},{"id":"http://arxiv.org/abs/2406.18100v1","updated":"2024-06-26T06:31:43Z","published":"2024-06-26T06:31:43Z","title":"Natural Language but Omitted? On the Ineffectiveness of Large Language\n  Models' privacy policy from End-users' Perspective","summary":"  LLMs driven products were increasingly prevalent in our daily lives, With a\nnatural language based interaction style, people may potentially leak their\npersonal private information. Thus, privacy policy and user agreement played an\nimportant role in regulating and alerting people. However, there lacked the\nwork examining the reading of LLM's privacy policy. Thus, we conducted the\nfirst user study to let participants read the privacy policy and user agreement\nwith two different styles (a cursory and detailed style). We found users lack\nimportant information upon cursory reading and even detailed reading. Besides,\ntheir privacy concerns was not solved even upon detailed reading. We provided\nfour design implications based on the findings.\n","authors":["Shuning Zhang","Haobin Xing","Xin Yi","Hewu Li"],"pdf_url":"https://arxiv.org/pdf/2406.18100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18082v1","updated":"2024-06-26T05:40:10Z","published":"2024-06-26T05:40:10Z","title":"Octo-planner: On-device Language Model for Planner-Action Agents","summary":"  AI agents have become increasingly significant in various domains, enabling\nautonomous decision-making and problem-solving. To function effectively, these\nagents require a planning process that determines the best course of action and\nthen executes the planned actions. In this paper, we present an efficient\non-device Planner-Action framework that separates planning and action execution\ninto two distinct components: a planner agent based on Phi-3 Mini, a 3.8\nbillion parameter LLM optimized for edge devices, and an action agent using the\nOctopus model for function execution. The planner agent first responds to user\nqueries by decomposing tasks into a sequence of sub-steps, which are then\nexecuted by the action agent. To optimize performance on resource-constrained\ndevices, we employ model fine-tuning instead of in-context learning, reducing\ncomputational costs and energy consumption while improving response times. Our\napproach involves using GPT-4 to generate diverse planning queries and\nresponses based on available functions, with subsequent validations to ensure\ndata quality. We fine-tune the Phi-3 Mini model on this curated dataset,\nachieving a 97\\% success rate in our in-domain test environment. To address\nmulti-domain planning challenges, we developed a multi-LoRA training method\nthat merges weights from LoRAs trained on distinct function subsets. This\napproach enables flexible handling of complex, multi-domain queries while\nmaintaining computational efficiency on resource-constrained devices. To\nsupport further research, we have open-sourced our model weights at\n\\url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please\nrefer to \\url{https://www.nexa4ai.com/octo-planner}.\n","authors":["Wei Chen","Zhiyuan Li","Zhen Guo","Yikang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.18082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04931v2","updated":"2024-06-26T23:44:48Z","published":"2024-03-07T22:37:49Z","title":"A Survey on Human-AI Teaming with Large Pre-Trained Models","summary":"  In the rapidly evolving landscape of artificial intelligence (AI), the\ncollaboration between human intelligence and AI systems, known as Human-AI\n(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and\ndecision-making processes. The advent of Large Pre-trained Models (LPtM) has\nsignificantly transformed this landscape, offering unprecedented capabilities\nby leveraging vast amounts of data to understand and predict complex patterns.\nThis paper surveys the pivotal integration of LPtMs with HAI, emphasizing how\nthese models enhance collaborative intelligence beyond traditional approaches.\nIt examines the potential of LPtMs in augmenting human capabilities, discussing\nthis collaboration for AI model improvements, effective teaming, ethical\nconsiderations, and their broad applied implications in various sectors.\nThrough this exploration, the study sheds light on the transformative impact of\nLPtM-enhanced HAI Teaming, providing insights for future research, policy\ndevelopment, and strategic implementations aimed at harnessing the full\npotential of this collaboration for research and societal benefit.\n","authors":["Vanshika Vats","Marzia Binta Nizam","Minghao Liu","Ziyuan Wang","Richard Ho","Mohnish Sai Prasad","Vincent Titterton","Sai Venkat Malreddy","Riya Aggarwal","Yanwen Xu","Lei Ding","Jay Mehta","Nathan Grinnell","Li Liu","Sijia Zhong","Devanathan Nallur Gandamani","Xinyi Tang","Rohan Ghosalkar","Celeste Shen","Rachel Shen","Nafisa Hussain","Kesav Ravichandran","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.04931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18718v1","updated":"2024-06-26T19:37:36Z","published":"2024-06-26T19:37:36Z","title":"State-Based Automation for Time-Restricted Eating Adherence","summary":"  Developing and enforcing study protocols is a foundational component of\nmedical research. As study complexity for participant interactions increases,\ntranslating study protocols to supporting application code becomes challenging.\nA collaboration exists between the University of Kentucky and Arizona State\nUniversity to determine the efficacy of time-restricted eating in improving\nmetabolic risk among postmenopausal women. This study utilizes a graph-based\napproach to monitor and support adherence to a designated schedule, enabling\nthe validation and step-wise audit of participants' statuses to derive\ndependable conclusions. A texting service, driven by a participant graph,\nautomatically manages interactions and collects data. Participant data is then\naccessible to the research study team via a website, which enables viewing,\nmanagement, and exportation. This paper presents a system for automatically\nmanaging participants in a time-restricted eating study that eliminates\ntime-consuming interactions with participants.\n","authors":["Samuel E. Armstrong","Aaron D. Mullen","J. Matthew Thomas","Dorothy D. Sears","Julie S. Pendergast","Jeffrey Talbert","Cody Bumgardner"],"pdf_url":"https://arxiv.org/pdf/2406.18718v1.pdf","comment":"8 pages, 4 figures, submitted to AMIA 2024 Annual Symposium"},{"id":"http://arxiv.org/abs/2406.18702v1","updated":"2024-06-26T19:10:51Z","published":"2024-06-26T19:10:51Z","title":"Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling\n  Legislative Behavior and Bipartisanship","summary":"  This study introduces a novel approach to simulating legislative processes\nusing LLM-driven virtual agents, focusing on the U.S. Senate Intelligence\nCommittee. We developed agents representing individual senators and placed them\nin simulated committee discussions. The agents demonstrated the ability to\nengage in realistic debate, provide thoughtful reflections, and find bipartisan\nsolutions under certain conditions. Notably, the simulation also showed promise\nin modeling shifts towards bipartisanship in response to external\nperturbations. Our results indicate that this LLM-driven approach could become\na valuable tool for understanding and potentially improving legislative\nprocesses, supporting a broader pattern of findings highlighting how LLM-based\nagents can usefully model real-world phenomena. Future works will focus on\nenhancing agent complexity, expanding the simulation scope, and exploring\napplications in policy testing and negotiation.\n","authors":["Zachary R. Baker","Zarif L. Azher"],"pdf_url":"https://arxiv.org/pdf/2406.18702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18690v1","updated":"2024-06-26T18:48:50Z","published":"2024-06-26T18:48:50Z","title":"Petal-X: Human-Centered Visual Explanations to Improve Cardiovascular\n  Risk Communication","summary":"  Cardiovascular diseases (CVDs), the leading cause of death worldwide, can be\nprevented in most cases through behavioral interventions. Therefore, effective\ncommunication of CVD risk and projected risk reduction by risk factor\nmodification plays a crucial role in reducing CVD risk at the individual level.\nHowever, despite interest in refining risk estimation with improved prediction\nmodels such as SCORE2, the guidelines for presenting these risk estimations in\nclinical practice remained essentially unchanged in the last few years, with\ngraphical score charts (GSCs) continuing to be one of the prevalent systems.\nThis work describes the design and implementation of Petal-X, a novel tool to\nsupport clinician-patient shared decision-making by explaining the CVD risk\ncontributions of different factors and facilitating what-if analysis. Petal-X\nrelies on a novel visualization, Petal Product Plots, and a tailor-made global\nsurrogate model of SCORE2, whose fidelity is comparable to that of the GSCs\nused in clinical practice. We evaluated Petal-X compared to GSCs in a\ncontrolled experiment with 88 healthcare students, all but one with experience\nwith chronic patients. The results show that Petal-X outperforms GSC in\ncritical tasks, such as comparing the contribution to the patient's 10-year CVD\nrisk of each modifiable risk factor, without a significant loss of perceived\ntransparency, trust, or intent to use. Our study provides an innovative\napproach to the visualization and explanation of risk in clinical practice\nthat, due to its model-agnostic nature, could continue to support\nnext-generation artificial intelligence risk assessment models.\n","authors":["Diego Rojo","Houda Lamqaddam","Lucija Gosak","Katrien Verbert"],"pdf_url":"https://arxiv.org/pdf/2406.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18675v1","updated":"2024-06-26T18:25:06Z","published":"2024-06-26T18:25:06Z","title":"Human-AI Collaborative Taxonomy Construction: A Case Study in\n  Profession-Specific Writing Assistants","summary":"  Large Language Models (LLMs) have assisted humans in several writing tasks,\nincluding text revision and story generation. However, their effectiveness in\nsupporting domain-specific writing, particularly in business contexts, is\nrelatively less explored. Our formative study with industry professionals\nrevealed the limitations in current LLMs' understanding of the nuances in such\ndomain-specific writing. To address this gap, we propose an approach of\nhuman-AI collaborative taxonomy development to perform as a guideline for\ndomain-specific writing assistants. This method integrates iterative feedback\nfrom domain experts and multiple interactions between these experts and LLMs to\nrefine the taxonomy. Through larger-scale experiments, we aim to validate this\nmethodology and thus improve LLM-powered writing assistance, tailoring it to\nmeet the unique requirements of different stakeholder needs.\n","authors":["Minhwa Lee","Zae Myung Kim","Vivek A. Khetan","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2406.18675v1.pdf","comment":"Accepted to CHI 2024 In2Writing Workshop"}]},"2024-06-27T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.19395v1","updated":"2024-06-27T17:59:53Z","published":"2024-06-27T17:59:53Z","title":"Dataset Size Recovery from LoRA Weights","summary":"  Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.\n","authors":["Mohammad Salama","Jonathan Kahana","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2406.19395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19394v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"HUWSOD: Holistic Self-training for Unified Weakly Supervised Object\n  Detection","summary":"  Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.\n","authors":["Liujuan Cao","Jianghang Lin","Zebo Hong","Yunhang Shen","Shaohui Lin","Chao Chen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19393v1","updated":"2024-06-27T17:59:46Z","published":"2024-06-27T17:59:46Z","title":"Looking 3D: Anomaly Detection with 2D-3D Alignment","summary":"  Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.19393v1.pdf","comment":"Accepted at CVPR'24. Codes & dataset available at\n  https://github.com/VICO-UoE/Looking3D"},{"id":"http://arxiv.org/abs/2406.19392v1","updated":"2024-06-27T17:59:45Z","published":"2024-06-27T17:59:45Z","title":"ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos","summary":"  We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.\n","authors":["Jr-Jen Chen","Yu-Chien Liao","Hsi-Che Lin","Yu-Chu Yu","Yen-Chun Chen","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v1","updated":"2024-06-27T17:59:40Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v1.pdf","comment":"The code is publicly available at\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2406.19390v1","updated":"2024-06-27T17:59:06Z","published":"2024-06-27T17:59:06Z","title":"SALVe: Semantic Alignment Verification for Floorplan Reconstruction from\n  Sparse Panoramas","summary":"  We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.\n","authors":["John Lambert","Yuguang Li","Ivaylo Boyadzhiev","Lambert Wixson","Manjunath Narayana","Will Hutchcroft","James Hays","Frank Dellaert","Sing Bing Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19390v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2406.19389v1","updated":"2024-06-27T17:59:01Z","published":"2024-06-27T17:59:01Z","title":"OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding","summary":"  Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.\n","authors":["Tao Zhang","Xiangtai Li","Hao Fei","Haobo Yuan","Shengqiong Wu","Shunping Ji","Chen Change Loy","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19388v1","updated":"2024-06-27T17:58:54Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v1.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2406.19369v1","updated":"2024-06-27T17:49:25Z","published":"2024-06-27T17:49:25Z","title":"Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment\n  Anything Model","summary":"  Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.\n","authors":["Haobo Yuan","Xiangtai Li","Lu Qi","Tao Zhang","Ming-Hsuan Yang","Shuicheng Yan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.19369v1.pdf","comment":"16 pages; 8 figures"},{"id":"http://arxiv.org/abs/2406.19364v1","updated":"2024-06-27T17:46:13Z","published":"2024-06-27T17:46:13Z","title":"SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues","summary":"  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.\n","authors":["Yuxin Xie","Tao Zhou","Yi Zhou","Geng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19362v1","updated":"2024-06-27T17:43:35Z","published":"2024-06-27T17:43:35Z","title":"STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via\n  Collaborating Self-Training and Adversarial Learning","summary":"  Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.\n","authors":["Yanan Zhang","Chao Zhou","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19362v1.pdf","comment":"Accepted by IEEE-TIV"},{"id":"http://arxiv.org/abs/2406.05127v2","updated":"2024-06-27T17:35:45Z","published":"2024-06-07T17:55:43Z","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n","authors":["Shengqiong Wu","Hao Fei","Xiangtai Li","Jiayi Ji","Hanwang Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.05127v2.pdf","comment":"Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/"},{"id":"http://arxiv.org/abs/2406.19353v1","updated":"2024-06-27T17:32:18Z","published":"2024-06-27T17:32:18Z","title":"CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative\n  Object REarrangement","summary":"  Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.\n","authors":["Chengwen Zhang","Yun Liu","Ruofan Xing","Bingda Tang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2406.19353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2406.19341v1","updated":"2024-06-27T17:16:23Z","published":"2024-06-27T17:16:23Z","title":"Learning Visual Conditioning Tokens to Correct Domain Shift for Fully\n  Test-time Adaptation","summary":"  Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.\n","authors":["Yushun Tang","Shuoshuo Chen","Zhehan Kan","Yi Zhang","Qinghai Guo","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2406.19341v1.pdf","comment":"accepted by TMM"},{"id":"http://arxiv.org/abs/2406.19336v1","updated":"2024-06-27T17:10:10Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v1.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.13444v2","updated":"2024-06-27T17:09:24Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v2.pdf","comment":"update reference"},{"id":"http://arxiv.org/abs/2406.19320v1","updated":"2024-06-27T16:54:12Z","published":"2024-06-27T16:54:12Z","title":"Efficient World Models with Context-Aware Tokenization","summary":"  Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.\n","authors":["Vincent Micheli","Eloi Alonso","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2406.19320v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.19316v1","updated":"2024-06-27T16:52:01Z","published":"2024-06-27T16:52:01Z","title":"Enhanced Data Transfer Cooperating with Artificial Triplets for Scene\n  Graph Generation","summary":"  This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.\n","authors":["KuanChao Chu","Satoshi Yamazaki","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2406.19316v1.pdf","comment":"Accepted to IEICE Transactions on Information and Systems in April\n  2024"},{"id":"http://arxiv.org/abs/2406.13642v2","updated":"2024-06-27T16:30:48Z","published":"2024-06-19T15:41:30Z","title":"SpatialBot: Precise Spatial Understanding with Vision Language Models","summary":"  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n","authors":["Wenxiao Cai","Yaroslav Ponomarenko","Jianhao Yuan","Xiaoqi Li","Wankou Yang","Hao Dong","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.13642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19302v1","updated":"2024-06-27T16:17:33Z","published":"2024-06-27T16:17:33Z","title":"Mapping Land Naturalness from Sentinel-2 using Deep Contextual and\n  Geographical Priors","summary":"  In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.\n","authors":["Burak Ekim","Michael Schmitt"],"pdf_url":"https://arxiv.org/pdf/2406.19302v1.pdf","comment":"6 pages, 3 figures, ICLR 2024 Tackling Climate Change with Machine\n  Learning Workshop"},{"id":"http://arxiv.org/abs/2406.19299v1","updated":"2024-06-27T16:15:22Z","published":"2024-06-27T16:15:22Z","title":"PNeRV: A Polynomial Neural Representation for Videos","summary":"  Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.\n","authors":["Sonam Gupta","Snehal Singh Tomar","Grigorios G Chrysos","Sukhendu Das","A. N. Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2406.19299v1.pdf","comment":"25 pages, 17 figures, published at TMLR, Feb 2024"},{"id":"http://arxiv.org/abs/2406.19298v1","updated":"2024-06-27T16:13:34Z","published":"2024-06-27T16:13:34Z","title":"Compositional Image Decomposition with Diffusion Models","summary":"  Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.\n","authors":["Jocelin Su","Nan Liu","Yanbo Wang","Joshua B. Tenenbaum","Yilun Du"],"pdf_url":"https://arxiv.org/pdf/2406.19298v1.pdf","comment":"ICML 2024, Webpage:\n  https://energy-based-model.github.io/decomp-diffusion"},{"id":"http://arxiv.org/abs/2406.19297v1","updated":"2024-06-27T16:12:57Z","published":"2024-06-27T16:12:57Z","title":"Enhancing Continual Learning in Visual Question Answering with\n  Modality-Aware Feature Distillation","summary":"  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n","authors":["Malvina Nikandrou","Georgios Pantazopoulos","Ioannis Konstas","Alessandro Suglia"],"pdf_url":"https://arxiv.org/pdf/2406.19297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19290v1","updated":"2024-06-27T16:04:41Z","published":"2024-06-27T16:04:41Z","title":"Human Modelling and Pose Estimation Overview","summary":"  Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.\n","authors":["Pawel Knap"],"pdf_url":"https://arxiv.org/pdf/2406.19290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.19263v1","updated":"2024-06-27T15:34:16Z","published":"2024-06-27T15:34:16Z","title":"Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding","summary":"  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n","authors":["Yue Fan","Lei Ding","Ching-Chen Kuo","Shan Jiang","Yang Zhao","Xinze Guan","Jie Yang","Yi Zhang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06748v2","updated":"2024-06-27T15:24:23Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v2.pdf","comment":"11 pages, 6 figures, accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19255v1","updated":"2024-06-27T15:23:36Z","published":"2024-06-27T15:23:36Z","title":"Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment","summary":"  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n","authors":["Hao Fei","Shengqiong Wu","Meishan Zhang","Min Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19255v1.pdf","comment":"Accepted by IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2406.19247v1","updated":"2024-06-27T15:14:23Z","published":"2024-06-27T15:14:23Z","title":"Local Manifold Learning for No-Reference Image Quality Assessment","summary":"  Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).\n","authors":["Timin Gao","Wensheng Pan","Yan Zhang","Sicheng Zhao","Shengchuan Zhang","Xiawu Zheng","Ke Li","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01656v2","updated":"2024-06-27T15:07:39Z","published":"2024-05-02T18:26:15Z","title":"S4: Self-Supervised Sensing Across the Spectrum","summary":"  Satellite image time series (SITS) segmentation is crucial for many\napplications like environmental monitoring, land cover mapping and agricultural\ncrop type classification. However, training models for SITS segmentation\nremains a challenging task due to the lack of abundant training data, which\nrequires fine grained annotation. We propose S4 a new self-supervised\npre-training approach that significantly reduces the requirement for labeled\ntraining data by utilizing two new insights: (a) Satellites capture images in\ndifferent parts of the spectrum such as radio frequencies, and visible\nfrequencies. (b) Satellite imagery is geo-registered allowing for fine-grained\nspatial alignment. We use these insights to formulate pre-training tasks in S4.\nWe also curate m2s2-SITS, a large-scale dataset of unlabeled,\nspatially-aligned, multi-modal and geographic specific SITS that serves as\nrepresentative pre-training data for S4. Finally, we evaluate S4 on multiple\nSITS segmentation datasets and demonstrate its efficacy against competing\nbaselines while using limited labeled data.\n","authors":["Jayanth Shenoy","Xingjian Davis Zhang","Shlok Mehrotra","Bill Tao","Rem Yang","Han Zhao","Deepak Vasisht"],"pdf_url":"https://arxiv.org/pdf/2405.01656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19239v1","updated":"2024-06-27T15:02:04Z","published":"2024-06-27T15:02:04Z","title":"ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI","summary":"  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n","authors":["Gianluca Giacchi","Isidoros Iakovidis","Bastien Milani","Matthias Stuber","Micah Murray","Benedetta Franceschiello"],"pdf_url":"https://arxiv.org/pdf/2406.19239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v1","updated":"2024-06-27T15:01:48Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19236v1","updated":"2024-06-27T15:01:42Z","published":"2024-06-27T15:01:42Z","title":"Human-Aware Vision-and-Language Navigation: Bridging Simulation to\n  Reality with Dynamic Human Interactions","summary":"  Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.\n","authors":["Minghan Li","Heng Li","Zhi-Qi Cheng","Yifei Dong","Yuxuan Zhou","Jun-Yan He","Qi Dai","Teruko Mitamura","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19236v1.pdf","comment":"30 pages, 18 figures, Project Page:\n  https://lpercc.github.io/HA3D_simulator/"},{"id":"http://arxiv.org/abs/2406.17382v2","updated":"2024-06-27T14:59:18Z","published":"2024-06-25T08:58:53Z","title":"Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods","summary":"  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets featuring\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition. Surprisingly, all methods except DeepLabCut and MediaPipe have\ncompetitive performance without additional finetuning, with ViTPose performing\nbest. Next to standard performance metrics (object keypoint similarity, average\nprecision and recall), we introduce errors expressed in the neck-mid-hip ratio\nand additionally study missed and redundant detections and the reliability of\nthe internal confidence ratings of the different methods, which are relevant\nfor downstream tasks. Among the networks with competitive performance, only\nAlphaPose could run close to real time (27 fps) on our machine. We provide\ndocumented Docker containers or instructions for all the methods we used, our\nanalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu\nand https://osf.io/x465b/.\n","authors":["Filipe Gama","Matej Misar","Lukas Navara","Sergiu T. Popescu","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2406.17382v2.pdf","comment":"21 pages, 3 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.05668v2","updated":"2024-06-27T14:55:41Z","published":"2024-06-09T06:53:39Z","title":"SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change\n  Detection","summary":"  Change detection (CD) in remote sensing imagery is a crucial task with\napplications in environmental monitoring, urban development, and disaster\nmanagement. CD involves utilizing bi-temporal images to identify changes over\ntime. The bi-temporal spatial relationships between features at the same\nlocation at different times play a key role in this process. However, existing\nchange detection networks often do not fully leverage these spatial\nrelationships during bi-temporal feature extraction and fusion. In this work,\nwe propose SRC-Net: a bi-temporal spatial relationship concerned network for\nCD. The proposed SRC-Net includes a Perception and Interaction Module that\nincorporates spatial relationships and establishes a cross-branch perception\nmechanism to enhance the precision and robustness of feature extraction.\nAdditionally, a Patch-Mode joint Feature Fusion Module is introduced to address\ninformation loss in current methods. It considers different change modes and\nconcerns about spatial relationships, resulting in more expressive fusion\nfeatures. Furthermore, we construct a novel network using these two\nrelationship concerned modules and conducted experiments on the LEVIR-CD and\nWHU Building datasets. The experimental results demonstrate that our network\noutperforms state-of-the-art (SOTA) methods while maintaining a modest\nparameter count. We believe our approach sets a new paradigm for change\ndetection and will inspire further advancements in the field. The code and\nmodels are publicly available at https://github.com/Chnja/SRCNet.\n","authors":["Hongjia Chen","Xin Xu","Fangling Pu"],"pdf_url":"https://arxiv.org/pdf/2406.05668v2.pdf","comment":"13 pages, 12 figures, IEEE Journal of Selected Topics in Applied\n  Earth Observations and Remote Sensing (2024)"},{"id":"http://arxiv.org/abs/2406.19225v1","updated":"2024-06-27T14:50:50Z","published":"2024-06-27T14:50:50Z","title":"ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model\n  for Semantic Segmentation","summary":"  Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.\n","authors":["Nazanin Moradinasab","Laura S. Shankman","Rebecca A. Deaton","Gary K. Owens","Donald E. Brown"],"pdf_url":"https://arxiv.org/pdf/2406.19225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19217v1","updated":"2024-06-27T14:43:50Z","published":"2024-06-27T14:43:50Z","title":"Think Step by Step: Chain-of-Gesture Prompting for Error Detection in\n  Robotic Surgical Videos","summary":"  Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.\n","authors":["Zhimin Shao","Jialang Xu","Danail Stoyanov","Evangelos B. Mazomenos","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19217v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.06196v2","updated":"2024-06-27T14:19:56Z","published":"2024-05-10T02:23:56Z","title":"VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks","summary":"  Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.\n","authors":["Manish Dhakal","Rabin Adhikari","Safal Thapaliya","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2405.06196v2.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2406.19175v1","updated":"2024-06-27T13:51:53Z","published":"2024-06-27T13:51:53Z","title":"Towards Reducing Data Acquisition and Labeling for Defect Detection\n  using Simulated Data","summary":"  In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.\n","authors":["Lukas Malte Kemeter","Rasmus Hvingelby","Paulina Sierak","Tobias Schön","Bishwajit Gosswam"],"pdf_url":"https://arxiv.org/pdf/2406.19175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v1","updated":"2024-06-27T13:29:25Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14574v2","updated":"2024-06-27T13:29:07Z","published":"2023-12-22T10:10:50Z","title":"MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning","summary":"  Prompt learning has demonstrated impressive efficacy in the fine-tuning of\nmultimodal large models to a wide range of downstream tasks. Nonetheless,\napplying existing prompt learning methods for the diagnosis of neurological\ndisorder still suffers from two issues: (i) existing methods typically treat\nall patches equally, despite the fact that only a small number of patches in\nneuroimaging are relevant to the disease, and (ii) they ignore the structural\ninformation inherent in the brain connection network which is crucial for\nunderstanding and diagnosing neurological disorders. To tackle these issues, we\nintroduce a novel prompt learning model by learning graph prompts during the\nfine-tuning process of multimodal large models for diagnosing neurological\ndisorders. Specifically, we first leverage GPT-4 to obtain relevant disease\nconcepts and compute semantic similarity between these concepts and all\npatches. Secondly, we reduce the weight of irrelevant patches according to the\nsemantic similarity between each patch and disease-related concepts. Moreover,\nwe construct a graph among tokens based on these concepts and employ a graph\nconvolutional network layer to extract the structural information of the graph,\nwhich is used to prompt the pre-trained multimodal large models for diagnosing\nneurological disorders. Extensive experiments demonstrate that our method\nachieves superior performance for neurological disorder diagnosis compared with\nstate-of-the-art methods and validated by clinicians.\n","authors":["Liang Peng","Songyue Cai","Zongqian Wu","Huifang Shang","Xiaofeng Zhu","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2312.14574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12223v3","updated":"2024-06-27T13:13:11Z","published":"2023-12-19T15:11:46Z","title":"Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries","summary":"  Group equivariance can overly constrain models if the symmetries in the group\ndiffer from those observed in data. While common methods address this by\ndetermining the appropriate level of symmetry at the dataset level, they are\nlimited to supervised settings and ignore scenarios in which multiple levels of\nsymmetry co-exist in the same dataset. In this paper, we propose a method able\nto detect the level of symmetry of each input without the need for labels. Our\nframework is general enough to accommodate different families of both\ncontinuous and discrete symmetry distributions, such as arbitrary unimodal,\nsymmetric distributions and discrete groups. We validate the effectiveness of\nour approach on synthetic datasets with different per-class levels of\nsymmetries, and demonstrate practical applications such as the detection of\nout-of-distribution symmetries. Our code is publicly available at\nhttps://github.com/aurban0/ssl-sym.\n","authors":["Alonso Urbano","David W. Romero"],"pdf_url":"https://arxiv.org/pdf/2312.12223v3.pdf","comment":"19 pages, 8 figures, corrected typos, revised argument in Appendix\n  B.1, results unchanged"},{"id":"http://arxiv.org/abs/2406.19150v1","updated":"2024-06-27T13:08:35Z","published":"2024-06-27T13:08:35Z","title":"RAVEN: Multitask Retrieval Augmented Vision-Language Learning","summary":"  The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.\n","authors":["Varun Nagaraj Rao","Siddharth Choudhary","Aditya Deshpande","Ravi Kumar Satzoda","Srikar Appalaraju"],"pdf_url":"https://arxiv.org/pdf/2406.19150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19148v1","updated":"2024-06-27T13:06:47Z","published":"2024-06-27T13:06:47Z","title":"BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal\n  Supervision","summary":"  Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix\n","authors":["Kit Mills Bransby","Arian Beqiri","Woo-Jin Cho Kim","Jorge Oliveira","Agisilaos Chartsias","Alberto Gomez"],"pdf_url":"https://arxiv.org/pdf/2406.19148v1.pdf","comment":"Accepted at MICCAI 2024 (Pre-print)"},{"id":"http://arxiv.org/abs/2311.16480v4","updated":"2024-06-27T12:38:12Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(PathText). In specific, we collected nearly 10000 high-quality WSI-text pairs\nfor visual-language models by recognizing and cleaning pathology reports which\nnarrate diagnostic slides in TCGA. On the model end, we propose the multiple\ninstance generative model (MI-Gen) which can produce pathology reports for\ngigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText.\nExperimental results show our model can generate pathology reports which\ncontain multiple clinical clues and achieve competitive performance on certain\nslide-level tasks. We observe that simple semantic extraction from the\npathology reports can achieve the best performance (0.838 of F1 score) on BRCA\nsubtyping surpassing previous state-of-the-art approaches. Our collected\ndataset and related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19131v1","updated":"2024-06-27T12:34:52Z","published":"2024-06-27T12:34:52Z","title":"CELLO: Causal Evaluation of Large Vision-Language Models","summary":"  Causal reasoning is fundamental to human intelligence and crucial for\neffective decision-making in real-world environments. Despite recent\nadvancements in large vision-language models (LVLMs), their ability to\ncomprehend causality remains unclear. Previous work typically focuses on\ncommonsense causality between events and/or actions, which is insufficient for\napplications like embodied agents and lacks the explicitly defined causal\ngraphs required for formal causal reasoning. To overcome these limitations, we\nintroduce a fine-grained and unified definition of causality involving\ninteractions between humans and/or objects. Building on the definition, we\nconstruct a novel dataset, CELLO, consisting of 14,094 causal questions across\nall four levels of causality: discovery, association, intervention, and\ncounterfactual. This dataset surpasses traditional commonsense causality by\nincluding explicit causal graphs that detail the interactions between humans\nand objects. Extensive experiments on CELLO reveal that current LVLMs still\nstruggle with causal reasoning tasks, but they can benefit significantly from\nour proposed CELLO-CoT, a causally inspired chain-of-thought prompting\nstrategy. Both quantitative and qualitative analyses from this study provide\nvaluable insights for future research. Our project page is at\nhttps://github.com/OpenCausaLab/CELLO.\n","authors":["Meiqi Chen","Bo Peng","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2406.19131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19130v1","updated":"2024-06-27T12:29:50Z","published":"2024-06-27T12:29:50Z","title":"Evidential Concept Embedding Models: Towards Reliable Concept\n  Explanations for Skin Disease Diagnosis","summary":"  Due to the high stakes in medical decision-making, there is a compelling\ndemand for interpretable deep learning methods in medical image analysis.\nConcept Bottleneck Models (CBM) have emerged as an active interpretable\nframework incorporating human-interpretable concepts into decision-making.\nHowever, their concept predictions may lack reliability when applied to\nclinical diagnosis, impeding concept explanations' quality. To address this, we\npropose an evidential Concept Embedding Model (evi-CEM), which employs\nevidential learning to model the concept uncertainty. Additionally, we offer to\nleverage the concept uncertainty to rectify concept misalignments that arise\nwhen training CBMs using vision-language models without complete concept\nsupervision. With the proposed methods, we can enhance concept explanations'\nreliability for both supervised and label-efficient settings. Furthermore, we\nintroduce concept uncertainty for effective test-time intervention. Our\nevaluation demonstrates that evi-CEM achieves superior performance in terms of\nconcept prediction, and the proposed concept rectification effectively\nmitigates concept misalignments for label-efficient training. Our code is\navailable at https://github.com/obiyoag/evi-CEM.\n","authors":["Yibo Gao","Zheyao Gao","Xin Gao","Yuanye Liu","Bomin Wang","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2406.19130v1.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2309.15785v2","updated":"2024-06-27T12:05:48Z","published":"2023-09-27T16:58:35Z","title":"BT-Adapter: Video Conversation is Feasible Without Video Instruction\n  Tuning","summary":"  The recent progress in Large Language Models (LLM) has spurred various\nadvancements in image-language conversation agents, while how to build a\nproficient video-based dialogue system is still under exploration. Considering\nthe extensive scale of LLM and visual backbone, minimal GPU memory is left for\nfacilitating effective temporal modeling, which is crucial for comprehending\nand providing feedback on videos. To this end, we propose Branching Temporal\nAdapter (BT-Adapter), a novel method for extending image-language pretrained\nmodels into the video domain. Specifically, BT-Adapter serves as a plug-and-use\ntemporal modeling branch alongside the pretrained visual encoder, which is\ntuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can\nbe seamlessly integrated into all image conversation models using this version\nof CLIP, enabling video conversations without the need for video instructions.\nBesides, we develop a unique asymmetric token masking strategy inside the\nbranch with tailor-made training tasks for BT-Adapter, facilitating faster\nconvergence and better results. Thanks to BT-Adapter, we are able to empower\nexisting multimodal dialogue models with strong video understanding\ncapabilities without incurring excessive GPU costs. Without bells and whistles,\nBT-Adapter achieves (1) state-of-the-art zero-shot results on various video\ntasks using thousands of fewer GPU hours. (2) better performance than current\nvideo chatbots without any video instruction tuning. (3) state-of-the-art\nresults of video chatting using video instruction tuning, outperforming\nprevious SOTAs by a large margin.\n","authors":["Ruyang Liu","Chen Li","Yixiao Ge","Ying Shan","Thomas H. Li","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2309.15785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19107v1","updated":"2024-06-27T11:34:27Z","published":"2024-06-27T11:34:27Z","title":"FDLite: A Single Stage Lightweight Face Detector Network","summary":"  Face detection is frequently attempted by using heavy pre-trained backbone\nnetworks like ResNet-50/101/152 and VGG16/19. Few recent works have also\nproposed lightweight detectors with customized backbones, novel loss functions\nand efficient training strategies. The novelty of this work lies in the design\nof a lightweight detector while training with only the commonly used loss\nfunctions and learning strategies. The proposed face detector grossly follows\nthe established RetinaFace architecture. The first contribution of this work is\nthe design of a customized lightweight backbone network (BLite) having 0.167M\nparameters with 0.52 GFLOPs. The second contribution is the use of two\nindependent multi-task losses. The proposed lightweight face detector (FDLite)\nhas 0.26M parameters with 0.94 GFLOPs. The network is trained on the WIDER FACE\ndataset. FDLite is observed to achieve 92.3\\%, 89.8\\%, and 82.2\\% Average\nPrecision (AP) on the easy, medium, and hard subsets of the WIDER FACE\nvalidation dataset, respectively.\n","authors":["Yogesh Aggarwal","Prithwijit Guha"],"pdf_url":"https://arxiv.org/pdf/2406.19107v1.pdf","comment":"10 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.19101v1","updated":"2024-06-27T11:28:36Z","published":"2024-06-27T11:28:36Z","title":"DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming","summary":"  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception, it also leads to longer sequences\nof visual tokens, increasing computational costs and straining the models'\nability to handle long contexts. To address these challenges, we introduce\nDocKylin, a document-centric MLLM that performs visual content slimming at both\nthe pixel and token levels, thereby reducing token sequence length in VDU\nscenarios. DocKylin utilizes an Adaptive Pixel Slimming (APS) preprocessing\nmodule to perform pixel-level slimming, increasing the proportion of\ninformative pixels. Moreover, DocKylin incorporates a novel Dynamic Token\nSlimming (DTS) module to conduct token-level slimming, filtering essential\ntokens and removing others to create a compressed, adaptive visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks. Notably, both the proposed APS and DTS are parameter-free,\nfacilitating easy integration into existing MLLMs, and our experiments indicate\ntheir potential for broader applications.\n","authors":["Jiaxin Zhang","Wentao Yang","Songxuan Lai","Zecheng Xie","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19087v1","updated":"2024-06-27T11:14:14Z","published":"2024-06-27T11:14:14Z","title":"Dimensions underlying the representational alignment of deep neural\n  networks with humans","summary":"  Determining the similarities and differences between humans and artificial\nintelligence is an important goal both in machine learning and cognitive\nneuroscience. However, similarities in representations only inform us about the\ndegree of alignment, not the factors that determine it. Drawing upon recent\ndevelopments in cognitive science, we propose a generic framework for yielding\ncomparable representations in humans and deep neural networks (DNN). Applying\nthis framework to humans and a DNN model of natural images revealed a\nlow-dimensional DNN embedding of both visual and semantic dimensions. In\ncontrast to humans, DNNs exhibited a clear dominance of visual over semantic\nfeatures, indicating divergent strategies for representing images. While\nin-silico experiments showed seemingly-consistent interpretability of DNN\ndimensions, a direct comparison between human and DNN representations revealed\nsubstantial differences in how they process images. By making representations\ndirectly comparable, our results reveal important challenges for\nrepresentational alignment, offering a means for improving their comparability.\n","authors":["Florian P. Mahner","Lukas Muttenthaler","Umut Güçlü","Martin N. Hebart"],"pdf_url":"https://arxiv.org/pdf/2406.19087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19081v1","updated":"2024-06-27T11:08:42Z","published":"2024-06-27T11:08:42Z","title":"Unsupervised Latent Stain Adaption for Digital Pathology","summary":"  In digital pathology, deep learning (DL) models for tasks such as\nsegmentation or tissue classification are known to suffer from domain shifts\ndue to different staining techniques. Stain adaptation aims to reduce the\ngeneralization error between different stains by training a model on source\nstains that generalizes to target stains. Despite the abundance of target stain\ndata, a key challenge is the lack of annotations. To address this, we propose a\njoint training between artificially labeled and unlabeled data including all\navailable stained images called Unsupervised Latent Stain Adaption (ULSA). Our\nmethod uses stain translation to enrich labeled source images with synthetic\ntarget images in order to increase supervised signals. Moreover, we leverage\nunlabeled target stain images using stain-invariant feature consistency\nlearning. With ULSA we present a semi-supervised strategy for efficient stain\nadaption without access to annotated target stain data. Remarkably, ULSA is\ntask agnostic in patch-level analysis for whole slide images (WSIs). Through\nextensive evaluation on external datasets, we demonstrate that ULSA achieves\nstate-of-the-art (SOTA) performance in kidney tissue segmentation and breast\ncancer classification across a spectrum of staining variations. Our findings\nsuggest that ULSA is an important framework towards stain adaption in digital\npathology.\n","authors":["Daniel Reisenbüchler","Lucas Luttner","Nadine S. Schaadt","Friedrich Feuerhake","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2406.19081v1.pdf","comment":"Accepted in MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19070v1","updated":"2024-06-27T10:40:35Z","published":"2024-06-27T10:40:35Z","title":"FAGhead: Fully Animate Gaussian Head from Monocular Videos","summary":"  High-fidelity reconstruction of 3D human avatars has a wild application in\nvisual reality. In this paper, we introduce FAGhead, a method that enables\nfully controllable human portraits from monocular videos. We explicit the\ntraditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to\nreconstruct with complex expressions. Furthermore, we employ a novel\nPoint-based Learnable Representation Field (PLRF) with learnable Gaussian point\npositions to enhance reconstruction performance. Meanwhile, to effectively\nmanage the edges of avatars, we introduced the alpha rendering to supervise the\nalpha value of each pixel. Extensive experimental results on the open-source\ndatasets and our capturing datasets demonstrate that our approach is able to\ngenerate high-fidelity 3D head avatars and fully control the expression and\npose of the virtual avatars, which is outperforming than existing works.\n","authors":["Yixin Xuan","Xinyang Li","Gongxin Yao","Shiwei Zhou","Donghui Sun","Xiaoxin Chen","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2406.19070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v4","updated":"2024-06-27T10:34:28Z","published":"2023-11-08T14:10:19Z","title":"Examining Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we investigate paradigms in MTL in\nthe context of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Overall,\nwe find surprising similarities between STL and MTL suggesting to consider\nmethods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v4.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2406.19057v1","updated":"2024-06-27T10:08:29Z","published":"2024-06-27T10:08:29Z","title":"Segment Anything Model for automated image data annotation: empirical\n  studies using text prompts from Grounding DINO","summary":"  Grounding DINO and the Segment Anything Model (SAM) have achieved impressive\nperformance in zero-shot object detection and image segmentation, respectively.\nTogether, they have a great potential in revolutionizing zero-shot semantic\nsegmentation or data annotation. Yet, in specialized domains like medical image\nsegmentation, objects of interest (e.g., organs, tissues, and tumors) may not\nfall in existing class names. To address this problem, the referring expression\ncomprehension (REC) ability of Grounding DINO is leveraged to detect arbitrary\ntargets by their language descriptions. However, recent studies have\nhighlighted severe limitation of the REC framework in this application setting\nowing to its tendency to make false positive predictions when the target is\nabsent in the given image. And, while this bottleneck is central to the\nprospect of open-set semantic segmentation, it is still largely unknown how\nmuch improvement can be achieved by studying the prediction errors. To this\nend, we perform empirical studies on eight publicly available datasets and\nreveal that these errors consistently follow a predictable pattern and can,\nthus, be mitigated by a simple strategy. Specifically, we show that these false\npositive detections with appreciable confidence scores generally occupy large\nimage areas and can usually be filtered by their relative sizes. More\nimportantly, we expect these observations to inspire future research in\nimproving REC-based detection and automated segmentation. Using this technique,\nwe evaluate the performance of SAM on multiple datasets from various\nspecialized domains and report significant improvement in segmentation\nperformance and annotation time savings over manual approaches.\n","authors":["Fuseini Mumuni","Alhassan Mumuni"],"pdf_url":"https://arxiv.org/pdf/2406.19057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19055v1","updated":"2024-06-27T10:03:20Z","published":"2024-06-27T10:03:20Z","title":"SimpleFusion: A Simple Fusion Framework for Infrared and Visible Images","summary":"  Integrating visible and infrared images into one high-quality image, also\nknown as visible and infrared image fusion, is a challenging yet critical task\nfor many downstream vision tasks. Most existing works utilize pretrained deep\nneural networks or design sophisticated frameworks with strong priors for this\ntask, which may be unsuitable or lack flexibility. This paper presents\nSimpleFusion, a simple yet effective framework for visible and infrared image\nfusion. Our framework follows the decompose-and-fusion paradigm, where the\nvisible and the infrared images are decomposed into reflectance and\nillumination components via Retinex theory and followed by the fusion of these\ncorresponding elements. The whole framework is designed with two plain\nconvolutional neural networks without downsampling, which can perform image\ndecomposition and fusion efficiently. Moreover, we introduce decomposition loss\nand a detail-to-semantic loss to preserve the complementary information between\nthe two modalities for fusion. We conduct extensive experiments on the\nchallenging benchmarks, verifying the superiority of our method over previous\nstate-of-the-arts. Code is available at\n\\href{https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images}{https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images}\n","authors":["Ming Chen","Yuxuan Cheng","Xinwei He","Xinyue Wang","Yan Aze","Jinhai Xiang"],"pdf_url":"https://arxiv.org/pdf/2406.19055v1.pdf","comment":"code:https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images"},{"id":"http://arxiv.org/abs/2406.19048v1","updated":"2024-06-27T09:56:38Z","published":"2024-06-27T09:56:38Z","title":"BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for\n  Semantic- and Spatial-Aware 3D Object Detection","summary":"  3D object detection is an important task that has been widely applied in\nautonomous driving. Recently, fusing multi-modal inputs, i.e., LiDAR and camera\ndata, to perform this task has become a new trend. Existing methods, however,\neither ignore the sparsity of Lidar features or fail to preserve the original\nspatial structure of LiDAR and the semantic density of camera features\nsimultaneously due to the modality gap. To address issues, this letter proposes\na novel bidirectional complementary Lidar-camera fusion framework, called\nBiCo-Fusion that can achieve robust semantic- and spatial-aware 3D object\ndetection. The key insight is to mutually fuse the multi-modal features to\nenhance the semantics of LiDAR features and the spatial awareness of the camera\nfeatures and adaptatively select features from both modalities to build a\nunified 3D representation. Specifically, we introduce Pre-Fusion consisting of\na Voxel Enhancement Module (VEM) to enhance the semantics of voxel features\nfrom 2D camera features and Image Enhancement Module (IEM) to enhance the\nspatial characteristics of camera features from 3D voxel features. Both VEM and\nIEM are bidirectionally updated to effectively reduce the modality gap. We then\nintroduce Unified Fusion to adaptively weight to select features from the\nenchanted Lidar and camera features to build a unified 3D representation.\nExtensive experiments demonstrate the superiority of our BiCo-Fusion against\nthe prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.\n","authors":["Yang Song","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19048v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19043v1","updated":"2024-06-27T09:50:20Z","published":"2024-06-27T09:50:20Z","title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n","authors":["Zi Wang","Fanwen Wang","Chen Qin","Jun Lyu","Ouyang Cheng","Shuo Wang","Yan Li","Mengyao Yu","Haoyu Zhang","Kunyuan Guo","Zhang Shi","Qirong Li","Ziqiang Xu","Yajing Zhang","Hao Li","Sha Hua","Binghua Chen","Longyu Sun","Mengting Sun","Qin Li","Ying-Hua Chu","Wenjia Bai","Jing Qin","Xiahai Zhuang","Claudia Prieto","Alistair Young","Michael Markl","He Wang","Lianming Wu","Guang Yang","Xiaobo Qu","Chengyan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19043v1.pdf","comment":"19 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.19030v1","updated":"2024-06-27T09:33:24Z","published":"2024-06-27T09:33:24Z","title":"Using diffusion model as constraint: Empower Image Restoration Network\n  Training with Diffusion Model","summary":"  Image restoration has made marvelous progress with the advent of deep\nlearning. Previous methods usually rely on designing powerful network\narchitecture to elevate performance, however, the natural visual effect of the\nrestored results is limited by color and texture distortions. Besides the\nvisual perceptual quality, the semantic perception recovery is an important but\noften overlooked perspective of restored image, which is crucial for the\ndeployment in high-level tasks. In this paper, we propose a new perspective to\nresort these issues by introducing a naturalness-oriented and semantic-aware\noptimization mechanism, dubbed DiffLoss. Specifically, inspired by the powerful\ndistribution coverage capability of the diffusion model for natural image\ngeneration, we exploit the Markov chain sampling property of diffusion model\nand project the restored results of existing networks into the sampling space.\nBesides, we reveal that the bottleneck feature of diffusion models, also dubbed\nh-space feature, is a natural high-level semantic space. We delve into this\nproperty and propose a semantic-aware loss to further unlock its potential of\nsemantic perception recovery, which paves the way to connect image restoration\ntask and downstream high-level recognition task. With these two strategies, the\nDiffLoss can endow existing restoration methods with both more natural and\nsemantic-aware results. We verify the effectiveness of our method on\nsubstantial common image restoration tasks and benchmarks. Code will be\navailable at https://github.com/JosephTiTan/DiffLoss.\n","authors":["Jiangtong Tan","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.19030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12540v3","updated":"2024-06-27T09:03:14Z","published":"2023-12-19T19:19:19Z","title":"Regularized Newton Raphson Inversion for Text-to-Image Diffusion Models","summary":"  Diffusion inversion is the problem of taking an image and a text prompt that\ndescribes it and finding a noise latent that would generate the image. Most\ncurrent inversion techniques operate by approximately solving an implicit\nequation and may converge slowly or yield poor reconstructed images. Here, we\nformulate the problem as finding the roots of an implicit equation and design a\nmethod to solve it efficiently. Our solution is based on Newton-Raphson (NR), a\nwell-known technique in numerical analysis. A naive application of NR may be\ncomputationally infeasible and tends to converge to incorrect solutions. We\ndescribe an efficient regularized formulation that converges quickly to a\nsolution that provides high-quality reconstructions. We also identify a source\nof inconsistency stemming from prompt conditioning during the inversion\nprocess, which significantly degrades the inversion quality. To address this,\nwe introduce a prompt-aware adjustment of the encoding, effectively correcting\nthis issue. Our solution, Regularized Newton-Raphson Inversion, inverts an\nimage within 0.5 sec for latent consistency models, opening the door for\ninteractive image editing. We further demonstrate improved results in image\ninterpolation and generation of rare objects.\n","authors":["Dvir Samuel","Barak Meiri","Nir Darshan","Shai Avidan","Gal Chechik","Rami Ben-Ari"],"pdf_url":"https://arxiv.org/pdf/2312.12540v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18361v2","updated":"2024-06-27T08:53:25Z","published":"2024-06-26T14:01:07Z","title":"Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process","summary":"  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n","authors":["Tianyu Lin","Zhiguang Chen","Zhonghao Yan","Weijiang Yu","Fudan Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.18361v2.pdf","comment":"Accepted at MICCAI 2024. Code and citation info see\n  https://github.com/lin-tianyu/Stable-Diffusion-Seg"},{"id":"http://arxiv.org/abs/2406.19006v1","updated":"2024-06-27T08:45:31Z","published":"2024-06-27T08:45:31Z","title":"VideoMambaPro: A Leap Forward for Mamba in Video Understanding","summary":"  Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.\n","authors":["Hui Lu","Albert Ali Salah","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2406.19006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18999v1","updated":"2024-06-27T08:39:16Z","published":"2024-06-27T08:39:16Z","title":"Improving Taxonomic Image-based Out-of-distribution Detection With DNA\n  Barcodes","summary":"  Image-based species identification could help scaling biodiversity monitoring\nto a global scale. Many challenges still need to be solved in order to\nimplement these systems in real-world applications. A reliable image-based\nmonitoring system must detect out-of-distribution (OOD) classes it has not been\npresented before. This is challenging especially with fine-grained classes.\nEmerging environmental monitoring techniques, DNA metabarcoding and eDNA, can\nhelp by providing information on OOD classes that are present in a sample. In\nthis paper, we study if DNA barcodes can also support in finding the outlier\nimages based on the outlier DNA sequence's similarity to the seen classes. We\npropose a re-ordering approach that can be easily applied on any pre-trained\nmodels and existing OOD detection methods. We experimentally show that the\nproposed approach improves taxonomic OOD detection compared to all common\nbaselines. We also show that the method works thanks to a correlation between\nvisual similarity and DNA barcode proximity. The code and data are available at\nhttps://github.com/mikkoim/dnaimg-ood.\n","authors":["Mikko Impiö","Jenni Raitoharju"],"pdf_url":"https://arxiv.org/pdf/2406.18999v1.pdf","comment":"Accepted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.18996v1","updated":"2024-06-27T08:37:26Z","published":"2024-06-27T08:37:26Z","title":"Zero-shot domain adaptation based on dual-level mix and contrast","summary":"  Zero-shot domain adaptation (ZSDA) is a domain adaptation problem in the\nsituation that labeled samples for a target task (task of interest) are only\navailable from the source domain at training time, but for a task different\nfrom the task of interest (irrelevant task), labeled samples are available from\nboth source and target domains. In this situation, classical domain adaptation\ntechniques can only learn domain-invariant features in the irrelevant task.\nHowever, due to the difference in sample distribution between the two tasks,\ndomain-invariant features learned in the irrelevant task are biased and not\nnecessarily domain-invariant in the task of interest. To solve this problem,\nthis paper proposes a new ZSDA method to learn domain-invariant features with\nlow task bias. To this end, we propose (1) data augmentation with dual-level\nmixups in both task and domain to fill the absence of target task-of-interest\ndata, (2) an extension of domain adversarial learning to learn domain-invariant\nfeatures with less task bias, and (3) a new dual-level contrastive learning\nmethod that enhances domain-invariance and less task biasedness of features.\nExperimental results show that our proposal achieves good performance on\nseveral benchmarks.\n","authors":["Yu Zhe","Jun Sakuma"],"pdf_url":"https://arxiv.org/pdf/2406.18996v1.pdf","comment":"Accepted by IEEE conference on Artificial intelligence 2024"},{"id":"http://arxiv.org/abs/2406.18992v1","updated":"2024-06-27T08:33:35Z","published":"2024-06-27T08:33:35Z","title":"Semi-supervised Concept Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) have garnered increasing attention due to\ntheir ability to provide concept-based explanations for black-box deep learning\nmodels while achieving high final prediction accuracy using human-like\nconcepts. However, the training of current CBMs heavily relies on the accuracy\nand richness of annotated concepts in the dataset. These concept labels are\ntypically provided by experts, which can be costly and require significant\nresources and effort. Additionally, concept saliency maps frequently misalign\nwith input saliency maps, causing concept predictions to correspond to\nirrelevant input features - an issue related to annotation alignment. To\naddress these limitations, we propose a new framework called SSCBM\n(Semi-supervised Concept Bottleneck Model). Our SSCBM is suitable for practical\nsituations where annotated data is scarce. By leveraging joint training on both\nlabeled and unlabeled data and aligning the unlabeled data at the concept\nlevel, we effectively solve these issues. We proposed a strategy to generate\npseudo labels and an alignment loss. Experiments demonstrate that our SSCBM is\nboth effective and efficient. With only 20% labeled data, we achieved 93.19%\n(96.39% in a fully supervised setting) concept accuracy and 75.51% (79.82% in a\nfully supervised setting) prediction accuracy.\n","authors":["Lijie Hu","Tianhao Huang","Huanyi Xie","Chenyang Ren","Zhengyu Hu","Lu Yu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18992v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2403.02311v3","updated":"2024-06-27T08:21:51Z","published":"2024-03-04T18:47:56Z","title":"Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications\n  to Cardiac MRI Segmentation","summary":"  Deep learning (DL)-based methods have achieved state-of-the-art performance\nfor many medical image segmentation tasks. Nevertheless, recent studies show\nthat deep neural networks (DNNs) can be miscalibrated and overconfident,\nleading to \"silent failures\" that are risky for clinical applications. Bayesian\nDL provides an intuitive approach to DL failure detection, based on posterior\nprobability estimation. However, the posterior is intractable for large medical\nimage segmentation DNNs. To tackle this challenge, we propose a Bayesian\nlearning framework using Hamiltonian Monte Carlo (HMC), tempered by cold\nposterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC\ncomputation, we further propose a cyclical annealing strategy, capturing both\nlocal and global geometries of the posterior distribution, enabling highly\nefficient Bayesian DNN training with the same computational budget as training\na single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along\nwith the segmentation uncertainty. We evaluate the proposed HMC-CP extensively\non cardiac magnetic resonance image (MRI) segmentation, using in-domain\nsteady-state free precession (SSFP) cine images as well as out-of-domain\ndatasets of quantitative T1 and T2 mapping. Our results show that the proposed\nmethod improves both segmentation accuracy and uncertainty estimation for in-\nand out-of-domain data, compared with well-established baseline methods such as\nMonte Carlo Dropout and Deep Ensembles. Additionally, we establish a conceptual\nlink between HMC and the commonly known stochastic gradient descent (SGD) and\nprovide general insight into the uncertainty of DL. This uncertainty is\nimplicitly encoded in the training dynamics but often overlooked. With reliable\nuncertainty estimation, our method provides a promising direction toward\ntrustworthy DL in clinical applications.\n","authors":["Yidong Zhao","Joao Tourais","Iain Pierce","Christian Nitsche","Thomas A. Treibel","Sebastian Weingärtner","Artur M. Schweidtmann","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2403.02311v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:011"},{"id":"http://arxiv.org/abs/2311.15937v2","updated":"2024-06-27T08:21:16Z","published":"2023-11-27T15:46:19Z","title":"Optimal Transport Aggregation for Visual Place Recognition","summary":"  The task of Visual Place Recognition (VPR) aims to match a query image\nagainst references from an extensive database of images from different places,\nrelying solely on visual cues. State-of-the-art pipelines focus on the\naggregation of features extracted from a deep backbone, in order to form a\nglobal descriptor for each image. In this context, we introduce SALAD (Sinkhorn\nAlgorithm for Locally Aggregated Descriptors), which reformulates NetVLAD's\nsoft-assignment of local features to clusters as an optimal transport problem.\nIn SALAD, we consider both feature-to-cluster and cluster-to-feature relations\nand we also introduce a 'dustbin' cluster, designed to selectively discard\nfeatures deemed non-informative, enhancing the overall descriptor quality.\nAdditionally, we leverage and fine-tune DINOv2 as a backbone, which provides\nenhanced description power for the local features, and dramatically reduces the\nrequired training time. As a result, our single-stage method not only surpasses\nsingle-stage baselines in public VPR datasets, but also surpasses two-stage\nmethods that add a re-ranking with significantly higher cost. Code and models\nare available at https://github.com/serizba/salad.\n","authors":["Sergio Izquierdo","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2311.15937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18977v1","updated":"2024-06-27T08:13:33Z","published":"2024-06-27T08:13:33Z","title":"RoboUniView: Visual-Language Model with Unified View Representation for\n  Robotic Manipulaiton","summary":"  Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a\nnovel paradigm, aiming to enhance the model's ability to generalize to new\nobjects and instructions. However, due to variations in camera specifications\nand mounting positions, existing methods exhibit significant performance\ndisparities across different robotic platforms. To address this challenge, we\npropose RoboUniView in this paper, an innovative approach that decouples visual\nfeature extraction from action learning. We first learn a unified view\nrepresentation from multi-perspective views by pre-training on readily\naccessible data, and then derive actions from this unified view representation\nto control robotic manipulation. This unified view representation more\naccurately mirrors the physical world and is not constrained by the robotic\nplatform's camera parameters. Thanks to this methodology, we achieve\nstate-of-the-art performance on the demanding CALVIN benchmark, enhancing the\nsuccess rate in the $D \\to D$ setting from 88.7% to 96.2%, and in the $ABC \\to\nD$ setting from 82.4% to 94.2%. Moreover, our model exhibits outstanding\nadaptability and flexibility: it maintains high performance under unseen camera\nparameters, can utilize multiple datasets with varying camera parameters, and\nis capable of joint cross-task learning across datasets. Code is provided for\nre-implementation. https://github.com/liufanfanlff/RoboUniview\n","authors":["Fanfan Liu","Feng Yan","Liming Zheng","Chengjian Feng","Yiyang Huang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.18977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09327v2","updated":"2024-06-27T08:09:27Z","published":"2024-06-13T17:06:15Z","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans","summary":"  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n","authors":["Stefan P. Hein","Manuel Schultheiss","Andrei Gafita","Raphael Zaum","Farid Yagubbayli","Robert Tauber","Isabel Rauscher","Matthias Eiber","Franz Pfeiffer","Wolfgang A. Weber"],"pdf_url":"https://arxiv.org/pdf/2406.09327v2.pdf","comment":"25 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.18967v1","updated":"2024-06-27T07:59:25Z","published":"2024-06-27T07:59:25Z","title":"Structural Attention: Rethinking Transformer for Unpaired Medical Image\n  Synthesis","summary":"  Unpaired medical image synthesis aims to provide complementary information\nfor an accurate clinical diagnostics, and address challenges in obtaining\naligned multi-modal medical scans. Transformer-based models excel in imaging\ntranslation tasks thanks to their ability to capture long-range dependencies.\nAlthough effective in supervised training settings, their performance falters\nin unpaired image synthesis, particularly in synthesizing structural details.\nThis paper empirically demonstrates that, lacking strong inductive biases,\nTransformer can converge to non-optimal solutions in the absence of paired\ndata. To address this, we introduce UNet Structured Transformer (UNest), a\nnovel architecture incorporating structural inductive biases for unpaired\nmedical image synthesis. We leverage the foundational Segment-Anything Model to\nprecisely extract the foreground structure and perform structural attention\nwithin the main anatomy. This guides the model to learn key anatomical regions,\nthus improving structural synthesis under the lack of supervision in unpaired\ntraining. Evaluated on two public datasets, spanning three modalities, i.e.,\nMR, CT, and PET, UNest improves recent methods by up to 19.30% across six\nmedical image synthesis tasks. Our code is released at\nhttps://github.com/HieuPhan33/MICCAI2024-UNest.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Bowen Zhang","Yuankai Qi","Zhibin Liao","Antonios Perperidis","Son Lam Phung","Johan W. Verjans","Minh-Son To"],"pdf_url":"https://arxiv.org/pdf/2406.18967v1.pdf","comment":"MICCAI2024 - Early Accept Top 11%"},{"id":"http://arxiv.org/abs/2406.18958v1","updated":"2024-06-27T07:40:59Z","published":"2024-06-27T07:40:59Z","title":"AnyControl: Create Your Artwork with Versatile Control on Text-to-Image\n  Generation","summary":"  The field of text-to-image (T2I) generation has made significant progress in\nrecent years, largely driven by advancements in diffusion models. Linguistic\ncontrol enables effective content creation, but struggles with fine-grained\ncontrol over image generation. This challenge has been explored, to a great\nextent, by incorporating additional user-supplied spatial conditions, such as\ndepth maps and edge maps, into pre-trained T2I models through extra encoding.\nHowever, multi-control image synthesis still faces several challenges.\nSpecifically, current approaches are limited in handling free combinations of\ndiverse input control signals, overlook the complex relationships among\nmultiple spatial conditions, and often fail to maintain semantic alignment with\nprovided textual prompts. This can lead to suboptimal user experiences. To\naddress these challenges, we propose AnyControl, a multi-control image\nsynthesis framework that supports arbitrary combinations of diverse control\nsignals. AnyControl develops a novel Multi-Control Encoder that extracts a\nunified multi-modal embedding to guide the generation process. This approach\nenables a holistic understanding of user inputs, and produces high-quality,\nfaithful results under versatile control signals, as demonstrated by extensive\nquantitative and qualitative evaluations. Our project page is available in\n\\url{https://any-control.github.io}.\n","authors":["Yanan Sun","Yanchen Liu","Yinhao Tang","Wenjie Pei","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17858v2","updated":"2024-06-27T07:39:05Z","published":"2024-06-25T18:02:11Z","title":"Depth-Driven Geometric Prompt Learning for Laparoscopic Liver Landmark\n  Detection","summary":"  Laparoscopic liver surgery poses a complex intraoperative dynamic environment\nfor surgeons, where remains a significant challenge to distinguish critical or\neven hidden structures inside the liver. Liver anatomical landmarks, e.g.,\nridge and ligament, serve as important markers for 2D-3D alignment, which can\nsignificantly enhance the spatial perception of surgeons for precise surgery.\nTo facilitate the detection of laparoscopic liver landmarks, we collect a novel\ndataset called L3D, which comprises 1,152 frames with elaborated landmark\nannotations from surgical videos of 39 patients across two medical sites. For\nbenchmarking purposes, 12 mainstream detection methods are selected and\ncomprehensively evaluated on L3D. Further, we propose a depth-driven geometric\nprompt learning network, namely D2GPLand. Specifically, we design a Depth-aware\nPrompt Embedding (DPE) module that is guided by self-supervised prompts and\ngenerates semantically relevant geometric information with the benefit of\nglobal depth cues extracted from SAM-based features. Additionally, a\nSemantic-specific Geometric Augmentation (SGA) scheme is introduced to\nefficiently merge RGB-D spatial and geometric information through reverse\nanatomic perception. The experimental results indicate that D2GPLand obtains\nstate-of-the-art performance on L3D, with 63.52% DICE and 48.68% IoU scores.\nTogether with 2D-3D fusion technology, our method can directly provide the\nsurgeon with intuitive guidance information in laparoscopic scenarios.\n","authors":["Jialun Pei","Ruize Cui","Yaoqian Li","Weixin Si","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.17858v2.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.18950v1","updated":"2024-06-27T07:30:54Z","published":"2024-06-27T07:30:54Z","title":"MMR-Mamba: Multi-Contrast MRI Reconstruction with Mamba and\n  Spatial-Frequency Information Fusion","summary":"  Multi-contrast MRI acceleration has become prevalent in MR imaging, enabling\nthe reconstruction of high-quality MR images from under-sampled k-space data of\nthe target modality, using guidance from a fully-sampled auxiliary modality.\nThe main crux lies in efficiently and comprehensively integrating complementary\ninformation from the auxiliary modality. Existing methods either suffer from\nquadratic computational complexity or fail to capture long-range correlated\nfeatures comprehensively. In this work, we propose MMR-Mamba, a novel framework\nthat achieves comprehensive integration of multi-contrast features through\nMamba and spatial-frequency information fusion. Firstly, we design the\n\\textit{Target modality-guided Cross Mamba} (TCM) module in the spatial domain,\nwhich maximally restores the target modality information by selectively\nabsorbing useful information from the auxiliary modality. Secondly, leveraging\nglobal properties of the Fourier domain, we introduce the \\textit{Selective\nFrequency Fusion} (SFF) module to efficiently integrate global information in\nthe frequency domain and recover high-frequency signals for the reconstruction\nof structure details. Additionally, we present the \\textit{Adaptive\nSpatial-Frequency Fusion} (ASFF) module, which enhances fused features by\nsupplementing less informative features from one domain with corresponding\nfeatures from the other domain. These innovative strategies ensure efficient\nfeature fusion across spatial and frequency domains, avoiding the introduction\nof redundant information and facilitating the reconstruction of high-quality\ntarget images. Extensive experiments on the BraTS and fastMRI knee datasets\ndemonstrate the superiority of the proposed MMR-Mamba over state-of-the-art MRI\nreconstruction methods.\n","authors":["Jing Zou","Lanqing Liu","Qi Chen","Shujun Wang","Xiaohan Xing","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.18950v1.pdf","comment":"10 pages, 5 figure"},{"id":"http://arxiv.org/abs/2310.02792v2","updated":"2024-06-27T07:29:09Z","published":"2023-10-04T13:11:20Z","title":"Continuous 3D Myocardial Motion Tracking via Echocardiography","summary":"  Myocardial motion tracking stands as an essential clinical tool in the\nprevention and detection of cardiovascular diseases (CVDs), the foremost cause\nof death globally. However, current techniques suffer from incomplete and\ninaccurate motion estimation of the myocardium in both spatial and temporal\ndimensions, hindering the early identification of myocardial dysfunction. To\naddress these challenges, this paper introduces the Neural Cardiac Motion Field\n(NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model\nthe 3D structure and the comprehensive 6D forward/backward motion of the heart.\nThis method surpasses pixel-wise limitations by offering the capability to\ncontinuously query the precise shape and motion of the myocardium at any\nspecific point throughout the cardiac cycle, enhancing the detailed analysis of\ncardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF\noperates without the need for paired datasets, and its optimization is\nself-supervised through the physics knowledge priors in both space and time\ndimensions, ensuring compatibility with both 2D and 3D echocardiogram video\ninputs. Experimental validations across three representative datasets support\nthe robustness and innovative nature of the NeuralCMF, marking significant\nadvantages over existing state-of-the-art methods in cardiac imaging and motion\ntracking.\n","authors":["Chengkang Shen","Hao Zhu","You Zhou","Yu Liu","Si Yi","Lili Dong","Weipeng Zhao","David J. Brady","Xun Cao","Zhan Ma","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.02792v2.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.18944v1","updated":"2024-06-27T07:14:14Z","published":"2024-06-27T07:14:14Z","title":"Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models","summary":"  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with only a few\nimages. However, recent studies find that these models are vulnerable to minor\nadversarial perturbation, and the fine-tuning performance is largely degraded\non corrupted datasets. Such characteristics are further exploited to craft\nprotective perturbation on sensitive images like portraits that prevent\nunauthorized generation. In response, diffusion-based purification methods have\nbeen proposed to remove these perturbations and retain generation performance.\nHowever, existing works lack detailed analysis of the fundamental shortcut\nlearning vulnerability of personalized diffusion models and also turn to\nover-purifying the images cause information loss. In this paper, we take a\ncloser look at the fine-tuning process of personalized diffusion models through\nthe lens of shortcut learning and propose a hypothesis that could explain the\nunderlying manipulation mechanisms of existing perturbation methods.\nSpecifically, we find that the perturbed images are greatly shifted from their\noriginal paired prompt in the CLIP-based latent space. As a result, training\nwith this mismatched image-prompt pair creates a construction that causes the\nmodels to dump their out-of-distribution noisy patterns to the identifier, thus\ncausing serious performance degradation. Based on this observation, we propose\na systematic approach to retain the training performance with purification that\nrealigns the latent image and its semantic meaning and also introduces\ncontrastive learning with a negative token to decouple the learning of wanted\nclean identity and the unwanted noisy pattern, that shows strong potential\ncapacity against further adaptive perturbation.\n","authors":["Yixin Liu","Ruoxi Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.18941v1","updated":"2024-06-27T07:13:09Z","published":"2024-06-27T07:13:09Z","title":"CLIP3D-AD: Extending CLIP for 3D Few-Shot Anomaly Detection with\n  Multi-View Images Generation","summary":"  Few-shot anomaly detection methods can effectively address data collecting\ndifficulty in industrial scenarios. Compared to 2D few-shot anomaly detection\n(2D-FSAD), 3D few-shot anomaly detection (3D-FSAD) is still an unexplored but\nessential task. In this paper, we propose CLIP3D-AD, an efficient 3D-FSAD\nmethod extended on CLIP. We successfully transfer strong generalization ability\nof CLIP into 3D-FSAD. Specifically, we synthesize anomalous images on given\nnormal images as sample pairs to adapt CLIP for 3D anomaly classification and\nsegmentation. For classification, we introduce an image adapter and a text\nadapter to fine-tune global visual features and text features. Meanwhile, we\npropose a coarse-to-fine decoder to fuse and facilitate intermediate\nmulti-layer visual representations of CLIP. To benefit from geometry\ninformation of point cloud and eliminate modality and data discrepancy when\nprocessed by CLIP, we project and render point cloud to multi-view normal and\nanomalous images. Then we design multi-view fusion module to fuse features of\nmulti-view images extracted by CLIP which are used to facilitate visual\nrepresentations for further enhancing vision-language correlation. Extensive\nexperiments demonstrate that our method has a competitive performance of 3D\nfew-shot anomaly classification and segmentation on MVTec-3D AD dataset.\n","authors":["Zuo Zuo","Jiahao Dong","Yao Wu","Yanyun Qu","Zongze Wu"],"pdf_url":"https://arxiv.org/pdf/2406.18941v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.13146v2","updated":"2024-06-27T07:02:48Z","published":"2024-04-19T19:24:20Z","title":"DeepFake-O-Meter v2.0: An Open Platform for DeepFake Detection","summary":"  Deepfakes, as AI-generated media, have increasingly threatened media\nintegrity and personal privacy with realistic yet fake digital content. In this\nwork, we introduce an open-source and user-friendly online platform,\nDeepFake-O-Meter v2.0, that integrates state-of-the-art methods for detecting\nDeepfake images, videos, and audio. Built upon DeepFake-O-Meter v1.0, we have\nmade significant upgrades and improvements in platform architecture design,\nincluding user interaction, detector integration, job balancing, and security\nmanagement. The platform aims to offer everyday users a convenient service for\nanalyzing DeepFake media using multiple state-of-the-art detection algorithms.\nIt ensures secure and private delivery of the analysis results. Furthermore, it\nserves as an evaluation and benchmarking platform for researchers in digital\nmedia forensics to compare the performance of multiple algorithms on the same\ninput. We have also conducted detailed usage analysis based on the collected\ndata to gain deeper insights into our platform's statistics. This involves\nanalyzing two-month trends in user activity and evaluating the processing\nefficiency of each detector.\n","authors":["Yan Ju","Chengzhe Sun","Shan Jia","Shuwei Hou","Zhaofeng Si","Soumyya Kanti Datta","Lipeng Ke","Riky Zhou","Anita Nikolich","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2404.13146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18927v1","updated":"2024-06-27T06:38:56Z","published":"2024-06-27T06:38:56Z","title":"RoFIR: Robust Fisheye Image Rectification Framework Impervious to\n  Optical Center Deviation","summary":"  Fisheye images are categorized fisheye into central and deviated based on the\noptical center position. Existing rectification methods are limited to central\nfisheye images, while this paper proposes a novel method that extends to\ndeviated fisheye image rectification. The challenge lies in the variant global\ndistortion distribution pattern caused by the random optical center position.\nTo address this challenge, we propose a distortion vector map (DVM) that\nmeasures the degree and direction of local distortion. By learning the DVM, the\nmodel can independently identify local distortions at each pixel without\nrelying on global distortion patterns. The model adopts a pre-training and\nfine-tuning training paradigm. In the pre-training stage, it predicts the\ndistortion vector map and perceives the local distortion features of each\npixel. In the fine-tuning stage, it predicts a pixel-wise flow map for deviated\nfisheye image rectification. We also propose a data augmentation method mixing\ncentral, deviated, and distorted-free images. Such data augmentation promotes\nthe model performance in rectifying both central and deviated fisheye images,\ncompared with models trained on single-type fisheye images. Extensive\nexperiments demonstrate the effectiveness and superiority of the proposed\nmethod.\n","authors":["Zhaokang Liao","Hao Feng","Shaokai Liu","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2406.18927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18925v1","updated":"2024-06-27T06:32:56Z","published":"2024-06-27T06:32:56Z","title":"Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding","summary":"  Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding?\n  We collect and release VisArgs, an annotated corpus designed to make explicit\nthe (usually implicit) structures underlying visual arguments. VisArgs includes\n1,611 images accompanied by three types of textual annotations: 5,112 visual\npremises (with region annotations), 5,574 commonsense premises, and reasoning\ntrees connecting them to a broader argument. We propose three tasks over\nVisArgs to probe machine capacity for visual argument understanding:\nlocalization of premises, identification of premises, and deduction of\nconclusions. Experiments demonstrate that 1) machines cannot fully identify the\nrelevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy\nof only 78.5%, whereas humans reached 98.0%. All models showed a performance\ndrop, with an average decrease in accuracy of 19.5%, when the comparison set\nwas changed from objects outside the image to irrelevant objects within the\nimage. Furthermore, 2) this limitation is the greatest factor impacting their\nperformance in understanding visual arguments. Most models improved the most\nwhen given relevant visual premises as additional inputs, compared to other\ninputs, for deducing the conclusion of the visual argument.\n","authors":["Jiwan Chung","Sungjae Lee","Minseo Kim","Seungju Han","Ashkan Yousefpour","Jack Hessel","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18925v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.02956v3","updated":"2024-06-27T06:30:01Z","published":"2024-02-05T12:34:03Z","title":"AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a\n  Single High-Resolution Image","summary":"  The process of estimating and counting tree density using only a single\naerial or satellite image is a difficult task in the fields of photogrammetry\nand remote sensing. However, it plays a crucial role in the management of\nforests. The huge variety of trees in varied topography severely hinders tree\ncounting models to perform well. The purpose of this paper is to propose a\nframework that is learnt from the source domain with sufficient labeled trees\nand is adapted to the target domain with only a limited number of labeled\ntrees. Our method, termed as AdaTreeFormer, contains one shared encoder with a\nhierarchical feature extraction scheme to extract robust features from the\nsource and target domains. It also consists of three subnets: two for\nextracting self-domain attention maps from source and target domains\nrespectively and one for extracting cross-domain attention maps. For the\nlatter, an attention-to-adapt mechanism is introduced to distill relevant\ninformation from different domains while generating tree density maps; a\nhierarchical cross-domain feature alignment scheme is proposed that\nprogressively aligns the features from the source and target domains. We also\nadopt adversarial learning into the framework to further reduce the gap between\nsource and target domains. Our AdaTreeFormer is evaluated on six designed\ndomain adaptation tasks using three tree counting datasets, \\ie Jiangsu,\nYosemite, and London. Experimental results show that AdaTreeFormer\nsignificantly surpasses the state of the art, \\eg in the cross domain from the\nYosemite to Jiangsu dataset, it achieves a reduction of 15.9 points in terms of\nthe absolute counting errors and an increase of 10.8\\% in the accuracy of the\ndetected trees' locations. The codes and datasets are available at\nhttps://github.com/HAAClassic/AdaTreeFormer.\n","authors":["Hamed Amini Amirkolaee","Miaojing Shi","Lianghua He","Mark Mulligan"],"pdf_url":"https://arxiv.org/pdf/2402.02956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18070v2","updated":"2024-06-27T06:26:12Z","published":"2024-06-26T05:01:37Z","title":"EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation","summary":"  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n","authors":["Baoqi Pei","Guo Chen","Jilan Xu","Yuping He","Yicheng Liu","Kanghua Pan","Yifei Huang","Yali Wang","Tong Lu","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2406.18070v2.pdf","comment":"Champion solutions in the EgoVis CVPR 2024 workshop"},{"id":"http://arxiv.org/abs/2406.18919v1","updated":"2024-06-27T06:22:02Z","published":"2024-06-27T06:22:02Z","title":"Classification of Carotid Plaque with Jellyfish Sign Through\n  Convolutional and Recurrent Neural Networks Utilizing Plaque Surface Edges","summary":"  In carotid arteries, plaque can develop as localized elevated lesions. The\nJellyfish sign, marked by fluctuating plaque surfaces with blood flow\npulsation, is a dynamic characteristic of these plaques that has recently\nattracted attention. Detecting this sign is vital, as it is often associated\nwith cerebral infarction. This paper proposes an ultrasound video-based\nclassification method for the Jellyfish sign, using deep neural networks. The\nproposed method first preprocesses carotid ultrasound videos to separate the\nmovement of the vascular wall from plaque movements. These preprocessed videos\nare then combined with plaque surface information and fed into a deep learning\nmodel comprising convolutional and recurrent neural networks, enabling the\nefficient classification of the Jellyfish sign. The proposed method was\nverified using ultrasound video images from 200 patients. Ablation studies\ndemonstrated the effectiveness of each component of the proposed method.\n","authors":["Takeshi Yoshidomi","Shinji Kume","Hiroaki Aizawa","Akira Furui"],"pdf_url":"https://arxiv.org/pdf/2406.18919v1.pdf","comment":"4 pages, 3 figures, accepted at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2403.06138v2","updated":"2024-06-27T06:16:30Z","published":"2024-03-10T08:56:02Z","title":"BSDA: Bayesian Random Semantic Data Augmentation for Medical Image\n  Classification","summary":"  Data augmentation is a crucial regularization technique for deep neural\nnetworks, particularly in medical image classification. Mainstream data\naugmentation (DA) methods are usually applied at the image level. Due to the\nspecificity and diversity of medical imaging, expertise is often required to\ndesign effective DA strategies, and improper augmentation operations can\ndegrade model performance. Although automatic augmentation methods exist, they\nare computationally intensive. Semantic data augmentation can implemented by\ntranslating features in feature space. However, over-translation may violate\nthe image label. To address these issues, we propose \\emph{Bayesian Random\nSemantic Data Augmentation} (BSDA), a computationally efficient and\nhandcraft-free feature-level DA method. BSDA uses variational Bayesian to\nestimate the distribution of the augmentable magnitudes, and then a sample from\nthis distribution is added to the original features to perform semantic data\naugmentation. We performed experiments on nine 2D and five 3D medical image\ndatasets. Experimental results show that BSDA outperforms current DA methods.\nAdditionally, BSDA can be easily assembled into CNNs or Transformers as a\nplug-and-play module, improving the network's performance. The code is\navailable online at \\url{https://github.com/YaoyaoZhu19/BSDA}.\n","authors":["Yaoyao Zhu","Xiuding Cai","Xueyao Wang","Xiaoqing Chen","Yu Yao","Zhongliang Fu"],"pdf_url":"https://arxiv.org/pdf/2403.06138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18915v1","updated":"2024-06-27T06:12:01Z","published":"2024-06-27T06:12:01Z","title":"Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models","summary":"  Large-scale endeavors like RT-1 and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 5 real-world and 12 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser and\nCode-As-Policies. We believe \\methodLong\\ can be the scalable method for both\ngenerating data for robotics and solving novel tasks in a zero-shot setting.\n","authors":["Jiafei Duan","Wentao Yuan","Wilbert Pumacay","Yi Ru Wang","Kiana Ehsani","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.18915v1.pdf","comment":"Project page: https://robo-point.github.io/"},{"id":"http://arxiv.org/abs/2406.18908v1","updated":"2024-06-27T05:48:26Z","published":"2024-06-27T05:48:26Z","title":"A Universal Railway Obstacle Detection System based on Semi-supervised\n  Segmentation And Optical Flow","summary":"  Detecting obstacles in railway scenarios is both crucial and challenging due\nto the wide range of obstacle categories and varying ambient conditions such as\nweather and light. Given the impossibility of encompassing all obstacle\ncategories during the training stage, we address this out-of-distribution (OOD)\nissue with a semi-supervised segmentation approach guided by optical flow\nclues. We reformulate the task as a binary segmentation problem instead of the\ntraditional object detection approach. To mitigate data shortages, we generate\nhighly realistic synthetic images using Segment Anything (SAM) and YOLO,\neliminating the need for manual annotation to produce abundant pixel-level\nannotations. Additionally, we leverage optical flow as prior knowledge to train\nthe model effectively. Several experiments are conducted, demonstrating the\nfeasibility and effectiveness of our approach.\n","authors":["Qiushi Guo"],"pdf_url":"https://arxiv.org/pdf/2406.18908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02027v2","updated":"2024-06-27T05:47:55Z","published":"2024-06-04T07:06:06Z","title":"Inference Attacks: A Taxonomy, Survey, and Promising Directions","summary":"  The prosperity of machine learning has also brought people's concerns about\ndata privacy. Among them, inference attacks can implement privacy breaches in\nvarious MLaaS scenarios and model training/prediction phases. Specifically,\ninference attacks can perform privacy inference on undisclosed target training\nsets based on outputs of the target model, including but not limited to\nstatistics, membership, semantics, data representation, etc. For instance,\ninfer whether the target data has the characteristics of AIDS. In addition, the\nrapid development of the machine learning community in recent years, especially\nthe surge of model types and application scenarios, has further stimulated the\ninference attacks' research. Thus, studying inference attacks and analyzing\nthem in depth is urgent and significant. However, there is still a gap in the\nsystematic discussion of inference attacks from taxonomy, global perspective,\nattack, and defense perspectives. This survey provides an in-depth and\ncomprehensive inference of attacks and corresponding countermeasures in\nML-as-a-service based on taxonomy and the latest researches. Without\ncompromising researchers' intuition, we first propose the 3MP taxonomy based on\nthe community research status, trying to normalize the confusing naming system\nof inference attacks. Also, we analyze the pros and cons of each type of\ninference attack, their workflow, countermeasure, and how they interact with\nother attacks. In the end, we point out several promising directions for\nresearchers from a more comprehensive and novel perspective.\n","authors":["Feng Wu","Lei Cui","Shaowen Yao","Shui Yu"],"pdf_url":"https://arxiv.org/pdf/2406.02027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06911v2","updated":"2024-06-27T05:39:46Z","published":"2024-06-11T03:09:37Z","title":"AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising","summary":"  Diffusion models have garnered significant interest from the community for\ntheir great generative ability across various applications. However, their\ntypical multi-step sequential-denoising nature gives rise to high cumulative\nlatency, thereby precluding the possibilities of parallel computation. To\naddress this, we introduce AsyncDiff, a universal and plug-and-play\nacceleration scheme that enables model parallelism across multiple devices. Our\napproach divides the cumbersome noise prediction model into multiple\ncomponents, assigning each to a different device. To break the dependency chain\nbetween these components, it transforms the conventional sequential denoising\ninto an asynchronous process by exploiting the high similarity between hidden\nstates in consecutive diffusion steps. Consequently, each component is\nfacilitated to compute in parallel on separate devices. The proposed strategy\nsignificantly reduces inference latency while minimally impacting the\ngenerative quality. Specifically, for the Stable Diffusion v2.1, AsyncDiff\nachieves a 2.7x speedup with negligible degradation and a 4.0x speedup with\nonly a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our\nexperiments also demonstrate that AsyncDiff can be readily applied to video\ndiffusion models with encouraging performances. The code is available at\nhttps://github.com/czg1225/AsyncDiff.\n","authors":["Zigeng Chen","Xinyin Ma","Gongfan Fang","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06911v2.pdf","comment":"Work in progress. Project Page:\n  https://czg1225.github.io/asyncdiff_page/"},{"id":"http://arxiv.org/abs/2406.18901v1","updated":"2024-06-27T05:28:44Z","published":"2024-06-27T05:28:44Z","title":"Autoencoder based approach for the mitigation of spurious correlations","summary":"  Deep neural networks (DNNs) have exhibited remarkable performance across\nvarious tasks, yet their susceptibility to spurious correlations poses a\nsignificant challenge for out-of-distribution (OOD) generalization. Spurious\ncorrelations refer to erroneous associations in data that do not reflect true\nunderlying relationships but are instead artifacts of dataset characteristics\nor biases. These correlations can lead DNNs to learn patterns that are not\nrobust across diverse datasets or real-world scenarios, hampering their ability\nto generalize beyond training data. In this paper, we propose an\nautoencoder-based approach to analyze the nature of spurious correlations that\nexist in the Global Wheat Head Detection (GWHD) 2021 dataset. We then use\ninpainting followed by Weighted Boxes Fusion (WBF) to achieve a 2% increase in\nthe Average Domain Accuracy (ADA) over the YOLOv5 baseline and consistently\nshow that our approach has the ability to suppress some of the spurious\ncorrelations in the GWHD 2021 dataset. The key advantage of our approach is\nthat it is more suitable in scenarios where there is limited scope to adapt or\nfine-tune the trained model in unseen test environments.\n","authors":["Srinitish Srinivasan","Karthik Seemakurthy"],"pdf_url":"https://arxiv.org/pdf/2406.18901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18898v1","updated":"2024-06-27T05:26:38Z","published":"2024-06-27T05:26:38Z","title":"360 in the Wild: Dataset for Depth Prediction and View Synthesis","summary":"  The large abundance of perspective camera datasets facilitated the emergence\nof novel learning-based strategies for various tasks, such as camera\nlocalization, single image depth estimation, or view synthesis. However,\npanoramic or omnidirectional image datasets, including essential information,\nsuch as pose and depth, are mostly made with synthetic scenes. In this work, we\nintroduce a large scale 360$^{\\circ}$ videos dataset in the wild. This dataset\nhas been carefully scraped from the Internet and has been captured from various\nlocations worldwide. Hence, this dataset exhibits very diversified environments\n(e.g., indoor and outdoor) and contexts (e.g., with and without moving\nobjects). Each of the 25K images constituting our dataset is provided with its\nrespective camera's pose and depth map. We illustrate the relevance of our\ndataset for two main tasks, namely, single image depth estimation and view\nsynthesis.\n","authors":["Kibaek Park","Francois Rameau","Jaesik Park","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2406.18898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04857v2","updated":"2024-06-27T05:11:39Z","published":"2024-02-07T13:54:56Z","title":"Advancing Video Anomaly Detection: A Concise Review and a New Dataset","summary":"  Video Anomaly Detection (VAD) finds widespread applications in security\nsurveillance, traffic monitoring, industrial monitoring, and healthcare.\nDespite extensive research efforts, there remains a lack of concise reviews\nthat provide insightful guidance for researchers. Such reviews would serve as\nquick references to grasp current challenges, research trends, and future\ndirections. In this paper, we present such a review, examining models and\ndatasets from various perspectives. We emphasize the critical relationship\nbetween model and dataset, where the quality and diversity of datasets\nprofoundly influence model performance, and dataset development adapts to the\nevolving needs of emerging approaches. Our review identifies practical issues,\nincluding the absence of comprehensive datasets with diverse scenarios. To\naddress this, we introduce a new dataset, Multi-Scenario Anomaly Detection\n(MSAD), comprising 14 distinct scenarios captured from various camera views.\nOur dataset has diverse motion patterns and challenging variations, such as\ndifferent lighting and weather conditions, providing a robust foundation for\ntraining superior models. We conduct an in-depth analysis of recent\nrepresentative models using MSAD and highlight its potential in addressing the\nchallenges of detecting anomalies across diverse and evolving surveillance\nscenarios. Our dataset is available here.\n","authors":["Liyun Zhu","Lei Wang","Arjun Raj","Tom Gedeon","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04857v2.pdf","comment":"Research report"},{"id":"http://arxiv.org/abs/2406.18893v1","updated":"2024-06-27T05:08:46Z","published":"2024-06-27T05:08:46Z","title":"AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image\n  Models","summary":"  We consider the problem of customizing text-to-image diffusion models with\nuser-supplied reference images. Given new prompts, the existing methods can\ncapture the key concept from the reference images but fail to align the\ngenerated image with the prompt. In this work, we seek to address this key\nissue by proposing new methods that can easily be used in conjunction with\nexisting customization methods that optimize the embeddings/weights at various\nintermediate stages of the text encoding process.\n  The first contribution of this paper is a dissection of the various stages of\nthe text encoding process leading up to the conditioning vector for\ntext-to-image models. We take a holistic view of existing customization methods\nand notice that key and value outputs from this process differs substantially\nfrom their corresponding baseline (non-customized) models (e.g., baseline\nstable diffusion). While this difference does not impact the concept being\ncustomized, it leads to other parts of the generated image not being aligned\nwith the prompt (see first row in Fig 1). Further, we also observe that these\nkeys and values allow independent control various aspects of the final\ngeneration, enabling semantic manipulation of the output. Taken together, the\nfeatures spanning these keys and values, serve as the basis for our next\ncontribution where we fix the aforementioned issues with existing methods. We\npropose a new post-processing algorithm, \\textbf{AlignIT}, that infuses the\nkeys and values for the concept of interest while ensuring the keys and values\nfor all other tokens in the input prompt are unchanged.\n  Our proposed method can be plugged in directly to existing customization\nmethods, leading to a substantial performance improvement in the alignment of\nthe final result with the input prompt while retaining the customization\nquality.\n","authors":["Aishwarya Agarwal","Srikrishna Karanam","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.18893v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.16777v2","updated":"2024-06-27T04:23:30Z","published":"2023-10-25T17:10:37Z","title":"MixerFlow: MLP-Mixer meets Normalising Flows","summary":"  Normalising flows are generative models that transform a complex density into\na simpler density through the use of bijective transformations enabling both\ndensity estimation and data generation from a single model. %However, the\nrequirement for bijectivity imposes the use of specialised architectures. In\nthe context of image modelling, the predominant choice has been the Glow-based\narchitecture, whereas alternative architectures remain largely unexplored in\nthe research community. In this work, we propose a novel architecture called\nMixerFlow, based on the MLP-Mixer architecture, further unifying the generative\nand discriminative modelling architectures. MixerFlow offers an efficient\nmechanism for weight sharing for flow-based models. Our results demonstrate\ncomparative or superior density estimation on image datasets and good scaling\nas the image resolution increases, making MixerFlow a simple yet powerful\nalternative to the Glow-based architectures. We also show that MixerFlow\nprovides more informative embeddings than Glow-based architectures and can\nintegrate many structured transformations such as splines or Kolmogorov-Arnold\nNetworks.\n","authors":["Eshant English","Matthias Kirchler","Christoph Lippert"],"pdf_url":"https://arxiv.org/pdf/2310.16777v2.pdf","comment":"Alternative title: MixerFlow for Image Modelling; Accepted at\n  ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2406.16562v2","updated":"2024-06-27T03:57:05Z","published":"2024-06-24T11:56:15Z","title":"EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models","summary":"  The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive datasets. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to\nalign closely with human evaluative judgments, resulting in a robust evaluation\nmodel. Our comprehensive tests across 24 text-to-image generation models\ndemonstrate that EvalAlign not only provides superior metric stability but also\naligns more closely with human preferences than existing metrics, confirming\nits effectiveness and utility in model assessment.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Mengping Yang","Cheng Zhang","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2406.16562v2.pdf","comment":"Github Repository: https://github.com/SAIS-FUXI/EvalAlign"},{"id":"http://arxiv.org/abs/2406.15182v2","updated":"2024-06-27T03:54:50Z","published":"2024-06-21T14:27:02Z","title":"DiffExplainer: Unveiling Black Box Models Via Counterfactual Generation","summary":"  In the field of medical imaging, particularly in tasks related to early\ndisease detection and prognosis, understanding the reasoning behind AI model\npredictions is imperative for assessing their reliability. Conventional\nexplanation methods encounter challenges in identifying decisive features in\nmedical image classifications, especially when discriminative features are\nsubtle or not immediately evident. To address this limitation, we propose an\nagent model capable of generating counterfactual images that prompt different\ndecisions when plugged into a black box model. By employing this agent model,\nwe can uncover influential image patterns that impact the black model's final\npredictions. Through our methodology, we efficiently identify features that\ninfluence decisions of the deep black box. We validated our approach in the\nrigorous domain of medical prognosis tasks, showcasing its efficacy and\npotential to enhance the reliability of deep learning models in medical image\nclassification compared to existing interpretation methods. The code will be\npublicly available at https://github.com/ayanglab/DiffExplainer.\n","authors":["Yingying Fang","Shuang Wu","Zihao Jin","Caiwen Xu","Shiyi Wang","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.15182v2.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.18868v1","updated":"2024-06-27T03:48:57Z","published":"2024-06-27T03:48:57Z","title":"Advancing Cross-domain Discriminability in Continual Learning of\n  Vison-Language Models","summary":"  Continual learning (CL) with Vision-Language Models (VLMs) has overcome the\nconstraints of traditional CL, which only focuses on previously encountered\nclasses. During the CL of VLMs, we need not only to prevent the catastrophic\nforgetting on incrementally learned knowledge but also to preserve the\nzero-shot ability of VLMs. However, existing methods require additional\nreference datasets to maintain such zero-shot ability and rely on\ndomain-identity hints to classify images across different domains. In this\nstudy, we propose Regression-based Analytic Incremental Learning (RAIL), which\nutilizes a recursive ridge regression-based adapter to learn from a sequence of\ndomains in a non-forgetting manner and decouple the cross-domain correlations\nby projecting features to a higher-dimensional space. Cooperating with a\ntraining-free fusion module, RAIL absolutely preserves the VLM's zero-shot\nability on unseen domains without any reference data. Additionally, we\nintroduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In\nthis setting, a CL learner is required to incrementally learn from multiple\ndomains and classify test images from both seen and unseen domains without any\ndomain-identity hint. We theoretically prove RAIL's absolute memorization on\nincrementally learned domains. Experiment results affirm RAIL's\nstate-of-the-art performance in both X-TAIL and existing Multi-domain\nTask-Incremental Learning settings. The code will be released upon acceptance.\n","authors":["Yicheng Xu","Yuxin Chen","Jiahao Nie","Yusong Wang","Huiping Zhuang","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.18868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02021v2","updated":"2024-06-27T03:46:35Z","published":"2024-02-03T04:27:26Z","title":"Transfer Learning in ECG Diagnosis: Is It Effective?","summary":"  The adoption of deep learning in ECG diagnosis is often hindered by the\nscarcity of large, well-labeled datasets in real-world scenarios, leading to\nthe use of transfer learning to leverage features learned from larger datasets.\nYet the prevailing assumption that transfer learning consistently outperforms\ntraining from scratch has never been systematically validated. In this study,\nwe conduct the first extensive empirical study on the effectiveness of transfer\nlearning in multi-label ECG classification, by investigating comparing the\nfine-tuning performance with that of training from scratch, covering a variety\nof ECG datasets and deep neural networks. We confirm that fine-tuning is the\npreferable choice for small downstream datasets; however, when the dataset is\nsufficiently large, training from scratch can achieve comparable performance,\nalbeit requiring a longer training time to catch up. Furthermore, we find that\ntransfer learning exhibits better compatibility with convolutional neural\nnetworks than with recurrent neural networks, which are the two most prevalent\narchitectures for time-series ECG applications. Our results underscore the\nimportance of transfer learning in ECG diagnosis, yet depending on the amount\nof available data, researchers may opt not to use it, considering the\nnon-negligible cost associated with pre-training.\n","authors":["Cuong V. Nguyen","Cuong D. Do"],"pdf_url":"https://arxiv.org/pdf/2402.02021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10595v3","updated":"2024-06-27T03:30:16Z","published":"2024-04-16T14:20:55Z","title":"Automated Evaluation of Large Vision-Language Models on Self-driving\n  Corner Cases","summary":"  Large Vision-Language Models (LVLMs) have received widespread attention in\nadvancing the interpretable self-driving. Existing evaluations of LVLMs\nprimarily focus on the multi-faceted capabilities in natural circumstances,\nlacking automated and quantifiable assessment for self-driving, let alone the\nsevere road corner cases. In this paper, we propose CODA-LM, the very first\nbenchmark for the automatic evaluation of LVLMs for self-driving corner cases.\nWe adopt a hierarchical data structure to prompt powerful LVLMs to analyze\ncomplex driving scenes and generate high-quality pre-annotation for human\nannotators, and for LVLM evaluation, we show that using the text-only large\nlanguage models (LLMs) as judges reveals even better alignment with human\npreferences than the LVLM judges. Moreover, with CODA-LM, we build CODA-VLM, a\nnew driving LVLM surpassing all the open-sourced counterparts on CODA-LM. Our\nCODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on\nthe regional perception task. We hope CODA-LM can become the catalyst to\npromote interpretable self-driving empowered by LVLMs.\n","authors":["Kai Chen","Yanze Li","Wenhua Zhang","Yanxin Liu","Pengxiang Li","Ruiyuan Gao","Lanqing Hong","Meng Tian","Xinhai Zhao","Zhenguo Li","Dit-Yan Yeung","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2404.10595v3.pdf","comment":"Project Page: https://coda-dataset.github.io/coda-lm/"},{"id":"http://arxiv.org/abs/2406.17117v2","updated":"2024-06-27T03:25:19Z","published":"2024-06-24T20:11:46Z","title":"Speeding Up Image Classifiers with Little Companions","summary":"  Scaling up neural networks has been a key recipe to the success of large\nlanguage and vision models. However, in practice, up-scaled models can be\ndisproportionately costly in terms of computations, providing only marginal\nimprovements in performance; for example, EfficientViT-L3-384 achieves <2%\nimprovement on ImageNet-1K accuracy over the base L1-224 model, while requiring\n$14\\times$ more multiply-accumulate operations (MACs). In this paper, we\ninvestigate scaling properties of popular families of neural networks for image\nclassification, and find that scaled-up models mostly help with \"difficult\"\nsamples. Decomposing the samples by difficulty, we develop a simple\nmodel-agnostic two-pass Little-Big algorithm that first uses a light-weight\n\"little\" model to make predictions of all samples, and only passes the\ndifficult ones for the \"big\" model to solve. Good little companion achieve\ndrastic MACs reduction for a wide variety of model families and scales. Without\nloss of accuracy or modification of existing models, our Little-Big models\nachieve MACs reductions of 76% for EfficientViT-L3-384, 81% for\nEfficientNet-B7-600, 71% for DeiT3-L-384 on ImageNet-1K. Little-Big also speeds\nup the InternImage-G-512 model by 62% while achieving 90% ImageNet-1K top-1\naccuracy, serving both as a strong baseline and as a simple practical method\nfor large model compression.\n","authors":["Yang Liu","Kowshik Thopalli","Jayaraman Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2406.17117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18864v1","updated":"2024-06-27T03:23:47Z","published":"2024-06-27T03:23:47Z","title":"Learning Modality Knowledge Alignment for Cross-Modality Transfer","summary":"  Cross-modality transfer aims to leverage large pretrained models to complete\ntasks that may not belong to the modality of pretraining data. Existing works\nachieve certain success in extending classical finetuning to cross-modal\nscenarios, yet we still lack understanding about the influence of modality gap\non the transfer. In this work, a series of experiments focusing on the source\nrepresentation quality during transfer are conducted, revealing the connection\nbetween larger modality gap and lesser knowledge reuse which means ineffective\ntransfer. We then formalize the gap as the knowledge misalignment between\nmodalities using conditional distribution P(Y|X). Towards this problem, we\npresent Modality kNowledge Alignment (MoNA), a meta-learning approach that\nlearns target data transformation to reduce the modality knowledge discrepancy\nahead of the transfer. Experiments show that out method enables better reuse of\nsource modality knowledge in cross-modality transfer, which leads to\nimprovements upon existing finetuning methods.\n","authors":["Wenxuan Ma","Shuang Li","Lincan Cai","Jingxuan Kang"],"pdf_url":"https://arxiv.org/pdf/2406.18864v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2403.13338v2","updated":"2024-06-27T03:04:26Z","published":"2024-03-20T06:46:01Z","title":"Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion\n  Prediction with T1-MRI-based Brain Network","summary":"  Prediction the conversion to early-stage dementia is critical for mitigating\nits progression but remains challenging due to subtle cognitive impairments and\nstructural brain changes. Traditional T1-weighted magnetic resonance imaging\n(T1-MRI) research focus on identifying brain atrophy regions but often fails to\naddress the intricate connectivity between them. This limitation underscores\nthe necessity of focuing on inter-regional connectivity for a comprehensive\nunderstand of the brain's complex network. Moreover, there is a pressing demand\nfor methods that adaptively preserve and extract critical information,\nparticularly specialized subgraph mining techniques for brain networks. These\nare essential for developing high-quality feature representations that reveal\ncritical spatial impacts of structural brain changes and its topology. In this\npaper, we propose Brain-SubGNN, a novel graph representation network to mine\nand enhance critical subgraphs based on T1-MRI. This network provides a\nsubgraph-level interpretation, enhancing interpretability and insights for\ngraph analysis. The process begins by extracting node features and a\ncorrelation matrix between nodes to construct a task-oriented brain network.\nBrain-SubGNN then adaptively identifies and enhances critical subgraphs,\ncapturing both loop and neighbor subgraphs. This method reflects the loop\ntopology and local changes, indicative of long-range connections, and maintains\nlocal and global brain attributes. Extensive experiments validate the\neffectiveness and advantages of Brain-SubGNN, demonstrating its potential as a\npowerful tool for understanding and diagnosing early-stage dementia. Source\ncode is available at https://github.com/Leng-10/Brain-SubGNN.\n","authors":["Yilin Leng","Wenju Cui","Bai Chen","Xi Jiang","Shuangqing Chen","Jian Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.13338v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2403.08002v5","updated":"2024-06-27T02:51:29Z","published":"2024-03-12T18:12:02Z","title":"Towards a clinically accessible radiology foundation model: open-access\n  and lightweight, with automated evaluation","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world clinics. Frontier general-domain models such as GPT-4V still\nhave significant performance gaps in multimodal biomedical applications. More\nimportantly, less-acknowledged pragmatic issues, including accessibility, model\ncost, and tedious manual evaluation make it hard for clinicians to use\nstate-of-the-art large models directly on private patient data. Here, we\nexplore training open-source small multimodal models (SMMs) to bridge\ncompetency gaps for unmet clinical needs in radiology. To maximize data\nefficiency, we adopt a modular approach by incorporating state-of-the-art\npre-trained models for image and text modalities, and focusing on training a\nlightweight adapter to ground each modality to the text embedding space, as\nexemplified by LLaVA-Med. For training, we assemble a large dataset of over 697\nthousand radiology image-text pairs. For evaluation, we propose CheXprompt, a\nGPT-4-based metric for factuality evaluation, and demonstrate its parity with\nexpert evaluation. For best practice, we conduct a systematic ablation study on\nvarious choices in data engineering and multimodal training. The resulting\nLlaVA-Rad (7B) model attains state-of-the-art results on standard radiology\ntasks such as report generation and cross-modal retrieval, even outperforming\nmuch larger models such as GPT-4V and Med-PaLM M (84B). The inference of\nLlaVA-Rad is fast and can be performed on a single V100 GPU in private\nsettings, offering a promising state-of-the-art tool for real-world clinical\napplications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Akshay Chaudhari","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08466v2","updated":"2024-06-27T02:45:49Z","published":"2024-02-13T13:48:54Z","title":"Taking Training Seriously: Human Guidance and Management-Based\n  Regulation of Artificial Intelligence","summary":"  Fervent calls for more robust governance of the harms associated with\nartificial intelligence (AI) are leading to the adoption around the world of\nwhat regulatory scholars have called a management-based approach to regulation.\nRecent initiatives in the United States and Europe, as well as the adoption of\nmajor self-regulatory standards by the International Organization for\nStandardization, share in common a core management-based paradigm. These\nmanagement-based initiatives seek to motivate an increase in human oversight of\nhow AI tools are trained and developed. Refinements and systematization of\nhuman-guided training techniques will thus be needed to fit within this\nemerging era of management-based regulatory paradigm. If taken seriously,\nhuman-guided training can alleviate some of the technical and ethical pressures\non AI, boosting AI performance with human intuition as well as better\naddressing the needs for fairness and effective explainability. In this paper,\nwe discuss the connection between the emerging management-based regulatory\nframeworks governing AI and the need for human oversight during training. We\nbroadly cover some of the technical components involved in human-guided\ntraining and then argue that the kinds of high-stakes use cases for AI that\nappear of most concern to regulators should lean more on human-guided training\nthan on data-only training. We hope to foster a discussion between legal\nscholars and computer scientists involving how to govern a domain of technology\nthat is vast, heterogenous, and dynamic in its applications and risks.\n","authors":["Cary Coglianese","Colton R. Crum"],"pdf_url":"https://arxiv.org/pdf/2402.08466v2.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.18849v1","updated":"2024-06-27T02:40:35Z","published":"2024-06-27T02:40:35Z","title":"Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception\n  Ability of LVLMs","summary":"  Currently many benchmarks have been proposed to evaluate the perception\nability of the Large Vision-Language Models (LVLMs). However, most benchmarks\nconduct questions by selecting images from existing datasets, resulting in the\npotential data leakage. Besides, these benchmarks merely focus on evaluating\nLVLMs on the realistic style images and clean scenarios, leaving the\nmulti-stylized images and noisy scenarios unexplored. In response to these\nchallenges, we propose a dynamic and scalable benchmark named Dysca for\nevaluating LVLMs by leveraging synthesis images. Specifically, we leverage\nStable Diffusion and design a rule-based method to dynamically generate novel\nimages, questions and the corresponding answers. We consider 51 kinds of image\nstyles and evaluate the perception capability in 20 subtasks. Moreover, we\nconduct evaluations under 4 scenarios (i.e., Clean, Corruption, Print Attacking\nand Adversarial Attacking) and 3 question types (i.e., Multi-choices,\nTrue-or-false and Free-form). Thanks to the generative paradigm, Dysca serves\nas a scalable benchmark for easily adding new subtasks and scenarios. A total\nof 8 advanced open-source LVLMs with 10 checkpoints are evaluated on Dysca,\nrevealing the drawbacks of current LVLMs. The benchmark is released in\n\\url{https://github.com/Benchmark-Dysca/Dysca}.\n","authors":["Jie Zhang","Zhongqi Wang","Mengqi Lei","Zheng Yuan","Bei Yan","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18845v1","updated":"2024-06-27T02:32:46Z","published":"2024-06-27T02:32:46Z","title":"Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion\n  Approach for Event Stream Recognition","summary":"  Existing event stream-based pattern recognition models usually represent the\nevent stream as the point cloud, voxel, image, etc., and design various deep\nneural networks to learn their features. Although considerable results can be\nachieved in simple cases, however, the model performance may be limited by\nmonotonous modality expressions, sub-optimal fusion, and readout mechanisms. In\nthis paper, we propose a novel dual-stream framework for event stream-based\npattern recognition via differentiated fusion, termed EFV++. It models two\ncommon event representations simultaneously, i.e., event images and event\nvoxels. The spatial and three-dimensional stereo information can be learned\nseparately by utilizing Transformer and Graph Neural Network (GNN). We believe\nthe features of each representation still contain both efficient and redundant\nfeatures and a sub-optimal solution may be obtained if we directly fuse them\nwithout differentiation. Thus, we divide each feature into three levels and\nretain high-quality features, blend medium-quality features, and exchange\nlow-quality features. The enhanced dual features will be fed into the fusion\nTransformer together with bottleneck features. In addition, we introduce a\nnovel hybrid interaction readout mechanism to enhance the diversity of features\nas final representations. Extensive experiments demonstrate that our proposed\nframework achieves state-of-the-art performance on multiple widely used event\nstream-based classification datasets. Specifically, we achieve new\nstate-of-the-art performance on the Bullying10k dataset, i.e., $90.51\\%$, which\nexceeds the second place by $+2.21\\%$. The source code of this paper has been\nreleased on\n\\url{https://github.com/Event-AHU/EFV_event_classification/tree/EFVpp}.\n","authors":["Lan Chen","Dong Li","Xiao Wang","Pengpeng Shao","Wei Zhang","Yaowei Wang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2406.18845v1.pdf","comment":"In Peer Review, Journal Extension of PRCV 2023"},{"id":"http://arxiv.org/abs/2406.18844v1","updated":"2024-06-27T02:31:03Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but raises\nsecurity risks through potential backdoor attacks due to their openness.\nPrevious backdoor studies focus on enclosed scenarios with consistent training\nand testing instructions, neglecting the practical domain gaps that could\naffect attack effectiveness. This paper empirically examines the\ngeneralizability of backdoor attacks during the instruction tuning of LVLMs for\nthe first time, revealing certain limitations of most backdoor strategies in\npractical scenarios. We quantitatively evaluate the generalizability of six\ntypical backdoor attacks on image caption benchmarks across multiple LVLMs,\nconsidering both visual and textual domain offsets. Our findings indicate that\nattack generalizability is positively correlated with the backdoor trigger's\nirrelevance to specific images/models and the preferential correlation of the\ntrigger pattern. Additionally, we modify existing backdoor attacks based on the\nabove key observations, demonstrating significant improvements in cross-domain\nscenario generalizability (+86% attack success rate). Notably, even without\naccess to the instruction datasets, a multimodal instruction set can be\nsuccessfully poisoned with a very low poisoning rate (0.2%), achieving an\nattack success rate of over 97%. This paper underscores that even simple\ntraditional backdoor strategies pose a serious threat to LVLMs, necessitating\nmore attention and in-depth research.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Ee-Chien Chang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.11580v2","updated":"2024-06-27T02:25:31Z","published":"2023-12-18T10:55:11Z","title":"PlaNet-S: Automatic Semantic Segmentation of Placenta","summary":"  [Purpose] To develop a fully automated semantic placenta segmentation model\nthat integrates the U-Net and SegNeXt architectures through ensemble learning.\n[Methods] A total of 218 pregnant women with suspected placental anomalies who\nunderwent magnetic resonance imaging (MRI) were enrolled, yielding 1090\nannotated images for developing a deep learning model for placental\nsegmentation. The images were standardized and divided into training and test\nsets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an\nensemble framework, was assessed using Intersection over Union (IoU) and\ncounting connected components (CCC) against the U-Net model. [Results] PlaNet-S\nhad significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-\n0.010) (p<0.01). The CCC for PlaNet-S was significantly higher than that for\nU-Net (p<0.01), matching the ground truth in 86.0\\% and 56.7\\% of the cases,\nrespectively. [Conclusion]PlaNet-S performed better than the traditional U-Net\nin placental segmentation tasks. This model addresses the challenges of\ntime-consuming physician-assisted manual segmentation and offers the potential\nfor diverse applications in placental imaging analyses.\n","authors":["Shinnosuke Yamamoto","Isso Saito","Eichi Takaya","Ayaka Harigai","Tomomi Sato","Tomoya Kobayashi","Kei Takase","Takuya Ueda"],"pdf_url":"https://arxiv.org/pdf/2312.11580v2.pdf","comment":"11 pages, 5 figures, Shinnosuke Yamamoto and Isso Saito equally\n  contributed to this work. In the original submission, there was a\n  typographical error in the reported standard deviation for the Intersection\n  over Union (IoU) values of the PlaNet-S model. The standard deviation was\n  incorrectly listed as 0.01 instead of the correct value of 0.1. This has been\n  corrected in the revised version"},{"id":"http://arxiv.org/abs/2406.17297v2","updated":"2024-06-27T02:19:51Z","published":"2024-06-25T05:58:34Z","title":"Towards Open-set Camera 3D Object Detection","summary":"  Traditional camera 3D object detectors are typically trained to recognize a\npredefined set of known object classes. In real-world scenarios, these\ndetectors may encounter unknown objects outside the training categories and\nfail to identify them correctly. To address this gap, we present OS-Det3D\n(Open-set Camera 3D Object Detection), a two-stage training framework enhancing\nthe ability of camera 3D detectors to identify both known and unknown objects.\nThe framework involves our proposed 3D Object Discovery Network (ODN3D), which\nis specifically trained using geometric cues such as the location and scale of\n3D boxes to discover general 3D objects. ODN3D is trained in a class-agnostic\nmanner, and the provided 3D object region proposals inherently come with data\nnoise. To boost accuracy in identifying unknown objects, we introduce a Joint\nObjectness Selection (JOS) module. JOS selects the pseudo ground truth for\nunknown objects from the 3D object region proposals of ODN3D by combining the\nODN3D objectness and camera feature attention objectness. Experiments on the\nnuScenes and KITTI datasets demonstrate the effectiveness of our framework in\nenabling camera 3D detectors to successfully identify unknown objects while\nalso improving their performance on known objects.\n","authors":["Zhuolin He","Xinrun Li","Heng Gao","Jiachen Tang","Shoumeng Qiu","Wenfu Wang","Lvjian Lu","Xuchong Qiu","Xiangyang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2406.17297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17770v2","updated":"2024-06-27T02:12:28Z","published":"2024-06-25T17:55:11Z","title":"MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning","summary":"  Multi-modal large language models (MLLMs) have made significant strides in\nvarious visual understanding tasks. However, the majority of these models are\nconstrained to process low-resolution images, which limits their effectiveness\nin perception tasks that necessitate detailed visual information. In our study,\nwe present MG-LLaVA, an innovative MLLM that enhances the model's visual\nprocessing capabilities by incorporating a multi-granularity vision flow, which\nincludes low-resolution, high-resolution, and object-centric features. We\npropose the integration of an additional high-resolution visual encoder to\ncapture fine-grained details, which are then fused with base visual features\nthrough a Conv-Gate fusion network. To further refine the model's object\nrecognition abilities, we incorporate object-level features derived from\nbounding boxes identified by offline detectors. Being trained solely on\npublicly available multimodal data through instruction tuning, MG-LLaVA\ndemonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide\nvariety of language encoders, ranging from 3.8B to 34B, to evaluate the model's\nperformance comprehensively. Extensive evaluations across multiple benchmarks\ndemonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter\nsizes, showcasing its remarkable efficacy. The code will be available at\nhttps://github.com/PhoenixZ810/MG-LLaVA.\n","authors":["Xiangyu Zhao","Xiangtai Li","Haodong Duan","Haian Huang","Yining Li","Kai Chen","Hua Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18360v2","updated":"2024-06-27T02:11:44Z","published":"2024-06-26T14:00:21Z","title":"XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis","summary":"  Thoroughly testing autonomy systems is crucial in the pursuit of safe\nautonomous driving vehicles. It necessitates creating safety-critical scenarios\nthat go beyond what can be safely collected from real-world data, as many of\nthese scenarios occur infrequently on public roads. However, the evaluation of\nmost existing NVS methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground truth images using\nmetrics. Unfortunately, this evaluation protocol falls short of meeting the\nactual requirements in closed-loop simulations. Specifically, the true\napplication demands the capability to render novel views that extend beyond the\noriginal trajectory (such as cross-lane views), which are challenging to\ncapture in the real world. To address this, this paper presents a novel driving\nview synthesis dataset and benchmark specifically designed for autonomous\ndriving simulations. This dataset is unique as it includes testing images\ncaptured by deviating from the training trajectory by 1-4 meters. It comprises\nsix sequences encompassing various time and weather conditions. Each sequence\ncontains 450 training images, 150 testing images, and their corresponding\ncamera poses and intrinsic parameters. Leveraging this novel dataset, we\nestablish the first realistic benchmark for evaluating existing NVS approaches\nunder front-only and multi-camera settings. The experimental findings\nunderscore the significant gap that exists in current approaches, revealing\ntheir inadequate ability to fulfill the demanding prerequisites of cross-lane\nor closed-loop simulation. Our dataset is released publicly at the project\npage: https://3d-aigc.github.io/XLD/.\n","authors":["Hao Li","Ming Yuan","Yan Zhang","Chenming Wu","Chen Zhao","Chunyu Song","Haocheng Feng","Errui Ding","Dingwen Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18360v2.pdf","comment":"project page: https://3d-aigc.github.io/XLD/"},{"id":"http://arxiv.org/abs/2406.18837v1","updated":"2024-06-27T02:11:33Z","published":"2024-06-27T02:11:33Z","title":"Dense Monocular Motion Segmentation Using Optical Flow and Pseudo Depth\n  Map: A Zero-Shot Approach","summary":"  Motion segmentation from a single moving camera presents a significant\nchallenge in the field of computer vision. This challenge is compounded by the\nunknown camera movements and the lack of depth information of the scene. While\ndeep learning has shown impressive capabilities in addressing these issues,\nsupervised models require extensive training on massive annotated datasets, and\nunsupervised models also require training on large volumes of unannotated data,\npresenting significant barriers for both. In contrast, traditional methods\nbased on optical flow do not require training data, however, they often fail to\ncapture object-level information, leading to over-segmentation or\nunder-segmentation. In addition, they also struggle in complex scenes with\nsubstantial depth variations and non-rigid motion, due to the overreliance of\noptical flow. To overcome these challenges, we propose an innovative hybrid\napproach that leverages the advantages of both deep learning methods and\ntraditional optical flow based methods to perform dense motion segmentation\nwithout requiring any training. Our method initiates by automatically\ngenerating object proposals for each frame using foundation models. These\nproposals are then clustered into distinct motion groups using both optical\nflow and relative depth maps as motion cues. The integration of depth maps\nderived from state-of-the-art monocular depth estimation models significantly\nenhances the motion cues provided by optical flow, particularly in handling\nmotion parallax issues. Our method is evaluated on the DAVIS-Moving and\nYTVOS-Moving datasets, and the results demonstrate that our method outperforms\nthe best unsupervised method and closely matches with the state-of-theart\nsupervised methods.\n","authors":["Yuxiang Huang","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2406.18837v1.pdf","comment":"For the offical publication, see https://crv.pubpub.org/pub/iunjzl55"},{"id":"http://arxiv.org/abs/2406.18836v1","updated":"2024-06-27T02:10:30Z","published":"2024-06-27T02:10:30Z","title":"Zero-shot Composed Image Retrieval Considering Query-target Relationship\n  Leveraging Masked Image-text Pairs","summary":"  This paper proposes a novel zero-shot composed image retrieval (CIR) method\nconsidering the query-target relationship by masked image-text pairs. The\nobjective of CIR is to retrieve the target image using a query image and a\nquery text. Existing methods use a textual inversion network to convert the\nquery image into a pseudo word to compose the image and text and use a\npre-trained visual-language model to realize the retrieval. However, they do\nnot consider the query-target relationship to train the textual inversion\nnetwork to acquire information for retrieval. In this paper, we propose a novel\nzero-shot CIR method that is trained end-to-end using masked image-text pairs.\nBy exploiting the abundant image-text pairs that are convenient to obtain with\na masking strategy for learning the query-target relationship, it is expected\nthat accurate zero-shot CIR using a retrieval-focused textual inversion network\ncan be realized. Experimental results show the effectiveness of the proposed\nmethod.\n","authors":["Huaying Zhang","Rintaro Yanagi","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2406.18836v1.pdf","comment":"Accepted as a conference paper in IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2406.18817v1","updated":"2024-06-27T01:16:44Z","published":"2024-06-27T01:16:44Z","title":"Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised\n  Clustering Analysis","summary":"  This paper presents a novel non-rigid point set registration method that is\ninspired by unsupervised clustering analysis. Unlike previous approaches that\ntreat the source and target point sets as separate entities, we develop a\nholistic framework where they are formulated as clustering centroids and\nclustering members, separately. We then adopt Tikhonov regularization with an\n$\\ell_1$-induced Laplacian kernel instead of the commonly used Gaussian kernel\nto ensure smooth and more robust displacement fields. Our formulation delivers\nclosed-form solutions, theoretical guarantees, independence from dimensions,\nand the ability to handle large deformations. Subsequently, we introduce a\nclustering-improved Nystr\\\"om method to effectively reduce the computational\ncomplexity and storage of the Gram matrix to linear, while providing a rigorous\nbound for the low-rank approximation. Our method achieves high accuracy results\nacross various scenarios and surpasses competitors by a significant margin,\nparticularly on shapes with substantial deformations. Additionally, we\ndemonstrate the versatility of our method in challenging tasks such as shape\ntransfer and medical registration.\n","authors":["Mingyang Zhao","Jingen Jiang","Lei Ma","Shiqing Xin","Gaofeng Meng","Dong-Ming Yan"],"pdf_url":"https://arxiv.org/pdf/2406.18817v1.pdf","comment":"[CVPR 2024 Highlight] Project and code at:\n  https://github.com/zikai1/CVPR24_PointSetReg"},{"id":"http://arxiv.org/abs/2406.18809v1","updated":"2024-06-27T00:54:11Z","published":"2024-06-27T00:54:11Z","title":"Divide, Ensemble and Conquer: The Last Mile on Unsupervised Domain\n  Adaptation for On-Board Semantic Segmentation","summary":"  The last mile of unsupervised domain adaptation (UDA) for semantic\nsegmentation is the challenge of solving the syn-to-real domain gap. Recent UDA\nmethods have progressed significantly, yet they often rely on strategies\ncustomized for synthetic single-source datasets (e.g., GTA5), which limits\ntheir generalisation to multi-source datasets. Conversely, synthetic\nmulti-source datasets hold promise for advancing the last mile of UDA but\nremain underutilized in current research. Thus, we propose DEC, a flexible UDA\nframework for multi-source datasets. Following a divide-and-conquer strategy,\nDEC simplifies the task by categorizing semantic classes, training models for\neach category, and fusing their outputs by an ensemble model trained\nexclusively on synthetic datasets to obtain the final segmentation mask. DEC\ncan integrate with existing UDA methods, achieving state-of-the-art performance\non Cityscapes, BDD100K, and Mapillary Vistas, significantly narrowing the\nsyn-to-real domain gap.\n","authors":["Tao Lian","Jose L. Gómez","Antonio M. López"],"pdf_url":"https://arxiv.org/pdf/2406.18809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12070v2","updated":"2024-06-27T00:45:18Z","published":"2023-11-19T19:44:44Z","title":"FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled\n  Diffusion Model","summary":"  Diffusion models have demonstrated significant potential in producing\nhigh-quality images in medical image translation to aid disease diagnosis,\nlocalization, and treatment. Nevertheless, current diffusion models have\nlimited success in achieving faithful image translations that can accurately\npreserve the anatomical structures of medical images, especially for unpaired\ndatasets. The preservation of structural and anatomical details is essential to\nreliable medical diagnosis and treatment planning, as structural mismatches can\nlead to disease misidentification and treatment errors. In this study, we\nintroduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT\nconversion. FDDM first obtains the anatomical information of the CT image from\nthe MR image through an initial conversion module. This anatomical information\nthen guides a subsequent diffusion model to generate high-quality CT images.\nOur diffusion model uses a dual-path reverse diffusion process for\nlow-frequency and high-frequency information, achieving a better balance\nbetween image quality and anatomical accuracy. We extensively evaluated FDDM\nusing public datasets for brain MR-to-CT and pelvis MR-to-CT translations,\ndemonstrating its superior performance to other GAN-based, VAE-based, and\ndiffusion-based models. The evaluation metrics included Frechet Inception\nDistance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity\nIndex Measure (SSIM). FDDM achieved the best scores on all metrics for both\ndatasets, particularly excelling in FID, with scores of 25.9 for brain data and\n29.2 for pelvis data, significantly outperforming other methods. These results\ndemonstrate that FDDM can generate high-quality target domain images while\nmaintaining the accuracy of translated anatomical structures.\n","authors":["Yunxiang Li","Hua-Chieh Shao","Xiaoxue Qian","You Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2404.11819v2","updated":"2024-06-27T23:16:58Z","published":"2024-04-18T00:41:32Z","title":"Utilizing Adversarial Examples for Bias Mitigation and Accuracy\n  Enhancement","summary":"  We propose a novel approach to mitigate biases in computer vision models by\nutilizing counterfactual generation and fine-tuning. While counterfactuals have\nbeen used to analyze and address biases in DNN models, the counterfactuals\nthemselves are often generated from biased generative models, which can\nintroduce additional biases or spurious correlations. To address this issue, we\npropose using adversarial images, that is images that deceive a deep neural\nnetwork but not humans, as counterfactuals for fair model training. Our\napproach leverages a curriculum learning framework combined with a fine-grained\nadversarial loss to fine-tune the model using adversarial examples. By\nincorporating adversarial images into the training data, we aim to prevent\nbiases from propagating through the pipeline. We validate our approach through\nboth qualitative and quantitative assessments, demonstrating improved bias\nmitigation and accuracy compared to existing methods. Qualitatively, our\nresults indicate that post-training, the decisions made by the model are less\ndependent on the sensitive attribute and our model better disentangles the\nrelationship between sensitive attributes and classification variables.\n","authors":["Pushkar Shukla","Dhruv Srikanth","Lee Cohen","Matthew Turk"],"pdf_url":"https://arxiv.org/pdf/2404.11819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19568v1","updated":"2024-06-27T23:03:58Z","published":"2024-06-27T23:03:58Z","title":"What Matters in Detecting AI-Generated Videos like Sora?","summary":"  Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/\n","authors":["Chirui Chang","Zhengzhe Liu","Xiaoyang Lyu","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.19568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19557v1","updated":"2024-06-27T22:17:49Z","published":"2024-06-27T22:17:49Z","title":"Robustness Testing of Black-Box Models Against CT Degradation Through\n  Test-Time Augmentation","summary":"  Deep learning models for medical image segmentation and object detection are\nbecoming increasingly available as clinical products. However, as details are\nrarely provided about the training data, models may unexpectedly fail when\ncases differ from those in the training distribution. An approach allowing\npotential users to independently test the robustness of a model, treating it as\na black box and using only a few cases from their own site, is key for\nadoption. To address this, a method to test the robustness of these models\nagainst CT image quality variation is presented. In this work we present this\nframework by demonstrating that given the same training data, the model\narchitecture and data pre processing greatly affect the robustness of several\nfrequently used segmentation and object detection methods to simulated CT\nimaging artifacts and degradation. Our framework also addresses the concern\nabout the sustainability of deep learning models in clinical use, by\nconsidering future shifts in image quality due to scanner deterioration or\nimaging protocol changes which are not reflected in a limited local test\ndataset.\n","authors":["Jack Highton","Quok Zong Chong","Samuel Finestone","Arian Beqiri","Julia A. Schnabel","Kanwal K. Bhatia"],"pdf_url":"https://arxiv.org/pdf/2406.19557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19540v1","updated":"2024-06-27T21:34:51Z","published":"2024-06-27T21:34:51Z","title":"Weighted Circle Fusion: Ensembling Circle Representation from Different\n  Object Detection Results","summary":"  Recently, the use of circle representation has emerged as a method to improve\nthe identification of spherical objects (such as glomeruli, cells, and nuclei)\nin medical imaging studies. In traditional bounding box-based object detection,\ncombining results from multiple models improves accuracy, especially when\nreal-time processing isn't crucial. Unfortunately, this widely adopted strategy\nis not readily available for combining circle representations. In this paper,\nwe propose Weighted Circle Fusion (WCF), a simple approach for merging\npredictions from various circle detection models. Our method leverages\nconfidence scores associated with each proposed bounding circle to generate\naveraged circles. Our method undergoes thorough evaluation on a proprietary\ndataset for glomerular detection in object detection within whole slide imaging\n(WSI). The findings reveal a performance gain of 5 %, respectively, compared to\nexisting ensemble methods. Furthermore, the Weighted Circle Fusion technique\nnot only improves the precision of object detection in medical images but also\nnotably decreases false detections, presenting a promising direction for future\nresearch and application in pathological image analysis.\n","authors":["Jialin Yue","Tianyuan Yao","Ruining Deng","Quan Liu","Juming Xiong","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2406.19540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19520v1","updated":"2024-06-27T20:41:49Z","published":"2024-06-27T20:41:49Z","title":"Comparative Analysis Of Color Models For Human Perception And Visual\n  Color Difference","summary":"  Color is integral to human experience, influencing emotions, decisions, and\nperceptions. This paper presents a comparative analysis of various color\nmodels' alignment with human visual perception. The study evaluates color\nmodels such as RGB, HSV, HSL, XYZ, CIELAB, and CIELUV to assess their\neffectiveness in accurately representing how humans perceive color. We evaluate\neach model based on its ability to accurately reflect visual color differences\nand dominant palette extraction compatible with the human eye. In image\nprocessing, accurate assessment of color difference is essential for\napplications ranging from digital design to quality control. Current color\ndifference metrics do not always match how people see colors, causing issues in\naccurately judging subtle differences. Understanding how different color models\nalign with human visual perception is crucial for various applications in image\nprocessing, digital media, and design.\n","authors":["Aruzhan Burambekova","Pakizar Shamoi"],"pdf_url":"https://arxiv.org/pdf/2406.19520v1.pdf","comment":"The paper has been submitted to EJMCA journal for consideration.\n  Current version is a preprint"},{"id":"http://arxiv.org/abs/2406.15727v2","updated":"2024-06-27T20:13:34Z","published":"2024-06-22T04:32:50Z","title":"Semi-supervised variational autoencoder for cell feature extraction in\n  multiplexed immunofluorescence images","summary":"  Advancements in digital imaging technologies have sparked increased interest\nin using multiplexed immunofluorescence (mIF) images to visualise and identify\nthe interactions between specific immunophenotypes with the tumour\nmicroenvironment at the cellular level. Current state-of-the-art multiplexed\nimmunofluorescence image analysis pipelines depend on cell feature\nrepresentations characterised by morphological and stain intensity-based\nmetrics generated using simple statistical and machine learning-based tools.\nHowever, these methods are not capable of generating complex representations of\ncells. We propose a deep learning-based cell feature extraction model using a\nvariational autoencoder with supervision using a latent subspace to extract\ncell features in mIF images. We perform cell phenotype classification using a\ncohort of more than 44,000 multiplexed immunofluorescence cell image patches\nextracted across 1,093 tissue microarray cores of breast cancer patients, to\ndemonstrate the success of our model against current and alternative methods.\n","authors":["Piumi Sandarenu","Julia Chen","Iveta Slapetova","Lois Browne","Peter H. Graham","Alexander Swarbrick","Ewan K. A. Millar","Yang Song","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2406.15727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10916v2","updated":"2024-06-27T19:43:52Z","published":"2024-03-16T12:44:08Z","title":"FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation","summary":"  Fish stock assessment often involves manual fish counting by taxonomy\nspecialists, which is both time-consuming and costly. We propose FishNet, an\nautomated computer vision system for both taxonomic classification and fish\nsize estimation from images captured with a low-cost digital camera. The system\nfirst performs object detection and segmentation using a Mask R-CNN to identify\nindividual fish from images containing multiple fish, possibly consisting of\ndifferent species. Then each fish species is classified and the length is\npredicted using separate machine learning models. To develop the model, we use\na dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different\nspecies and ranging in length from 10cm to 250cm, with additional annotations\nand quality control methods used to curate high-quality training data. On\nheld-out test data sets, our system achieves a 92% intersection over union on\nthe fish segmentation task, a 89% top-1 classification accuracy on single fish\nspecies classification, and a 2.3cm mean absolute error on the fish length\nestimation task.\n","authors":["Moseli Mots'oehli","Anton Nikolaev","Wawan B. IGede","John Lynham","Peter J. Mous","Peter Sadowski"],"pdf_url":"https://arxiv.org/pdf/2403.10916v2.pdf","comment":"IEEE COINS 2024"},{"id":"http://arxiv.org/abs/2406.19498v1","updated":"2024-06-27T19:27:37Z","published":"2024-06-27T19:27:37Z","title":"Stereo Vision Based Robot for Remote Monitoring with VR Support","summary":"  The machine vision systems have been playing a significant role in visual\nmonitoring systems. With the help of stereovision and machine learning, it will\nbe able to mimic human-like visual system and behaviour towards the\nenvironment. In this paper, we present a stereo vision based 3-DOF robot which\nwill be used to monitor places from remote using cloud server and internet\ndevices. The 3-DOF robot will transmit human-like head movements, i.e., yaw,\npitch, roll and produce 3D stereoscopic video and stream it in Real-time. This\nvideo stream is sent to the user through any generic internet devices with VR\nbox support, i.e., smartphones giving the user a First-person real-time 3D\nexperience and transfers the head motion of the user to the robot also in\nReal-time. The robot will also be able to track moving objects and faces as a\ntarget using deep neural networks which enables it to be a standalone\nmonitoring robot. The user will be able to choose specific subjects to monitor\nin a space. The stereovision enables us to track the depth information of\ndifferent objects detected and will be used to track human interest objects\nwith its distances and sent to the cloud. A full working prototype is developed\nwhich showcases the capabilities of a monitoring system based on stereo vision,\nrobotics, and machine learning.\n","authors":["Mohamed Fazil M. S.","Arockia Selvakumar A.","Daniel Schilberg"],"pdf_url":"https://arxiv.org/pdf/2406.19498v1.pdf","comment":"6 Pages, 10 Figures"},{"id":"http://arxiv.org/abs/2406.19492v1","updated":"2024-06-27T19:16:57Z","published":"2024-06-27T19:16:57Z","title":"High-resolution segmentations of the hypothalamus and its subregions for\n  training of segmentation models","summary":"  Segmentation of brain structures on magnetic resonance imaging (MRI) is a\nhighly relevant neuroimaging topic, as it is a prerequisite for different\nanalyses such as volumetry or shape analysis. Automated segmentation\nfacilitates the study of brain structures in larger cohorts when compared with\nmanual segmentation, which is time-consuming. However, the development of most\nautomated methods relies on large and manually annotated datasets, which limits\nthe generalizability of these methods. Recently, new techniques using synthetic\nimages have emerged, reducing the need for manual annotation. Here we provide\nHELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built\nfrom publicly available ultra-high resolution ex vivo MRI from 10 whole\nhemispheres, which can be used to develop segmentation methods using synthetic\ndata. The label maps are obtained with a combination of manual labels for the\nhypothalamic regions and automated segmentations for the rest of the brain, and\nmirrored to simulate entire brains. We also provide the pre-processed ex vivo\nscans, as this dataset can support future projects to include other structures\nafter these are manually segmented.\n","authors":["Livia Rodrigues","Martina Bocchetta","Oula Puonti","Douglas Greve","Ana Carolina Londe","Marcondes França","Simone Appenzeller","Leticia Rittner","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2406.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16222v2","updated":"2024-06-27T19:02:24Z","published":"2024-04-24T21:49:59Z","title":"Step Differences in Instructional Video","summary":"  Comparing a user video to a reference how-to video is a key requirement for\nAR/VR technology delivering personalized assistance tailored to the user's\nprogress. However, current approaches for language-based assistance can only\nanswer questions about a single video. We propose an approach that first\nautomatically generates large amounts of visual instruction tuning data\ninvolving pairs of videos from HowTo100M by leveraging existing step\nannotations and accompanying narrations, and then trains a video-conditioned\nlanguage model to jointly reason across multiple raw videos. Our model achieves\nstate-of-the-art performance at identifying differences between video pairs and\nranking videos based on the severity of these differences, and shows promising\nability to perform general reasoning over multiple videos. Project page:\nhttps://github.com/facebookresearch/stepdiff\n","authors":["Tushar Nagarajan","Lorenzo Torresani"],"pdf_url":"https://arxiv.org/pdf/2404.16222v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19485v1","updated":"2024-06-27T18:58:41Z","published":"2024-06-27T18:58:41Z","title":"GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for\n  Carotid Artery Segmentation","summary":"  Atherosclerosis is a chronic, progressive disease that primarily affects the\narterial walls. It is one of the major causes of cardiovascular disease.\nMagnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial\ninsights into vascular disease diagnosis by clearly visualizing vascular\nstructures. However, the complex anatomy of the neck poses challenges in\ndistinguishing the carotid artery (CA) from surrounding structures, especially\nwith changes like atherosclerosis. In order to address these issues, we propose\nGAPNet, which is a consisting of a novel geometric prior deduced from.\n","authors":["Lin Zhang","Chenggang Lu","Xin-yang Shi","Caifeng Shan","Jiong Zhang","Da Chen","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2406.19485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19464v1","updated":"2024-06-27T18:06:38Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. These information can surprisingly ease the\nlearning of contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments, by learning from diverse in-the-wild human\ndemonstrations. Project website: https://mani-wav.github.io/\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19461v1","updated":"2024-06-27T18:03:06Z","published":"2024-06-27T18:03:06Z","title":"Efficient and Distributed Large-Scale 3D Map Registration using\n  Tomographic Features","summary":"  A robust, resource-efficient, distributed, and minimally parameterized 3D map\nmatching and merging algorithm is proposed. The suggested algorithm utilizes\ntomographic features from 2D projections of horizontal cross-sections of\ngravity-aligned local maps, and matches these projection slices at all possible\nheight differences, enabling the estimation of four degrees of freedom in an\nefficient and parallelizable manner. The advocated algorithm improves\nstate-of-the-art feature extraction and registration pipelines by an order of\nmagnitude in memory use and execution time. Experimental studies are offered to\ninvestigate the efficiency of this 3D map merging scheme.\n","authors":["Halil Utku Unlu","Anthony Tzes","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2406.19461v1.pdf","comment":"Submitted to Elsevier Journal: Robotics and Autonomous Systems (RAS)"},{"id":"http://arxiv.org/abs/2310.06389v3","updated":"2024-06-27T18:02:06Z","published":"2023-10-10T07:52:30Z","title":"Learning Stackable and Skippable LEGO Bricks for Efficient,\n  Reconfigurable, and Variable-Resolution Diffusion Modeling","summary":"  Diffusion models excel at generating photo-realistic images but come with\nsignificant computational costs in both training and sampling. While various\ntechniques address these computational challenges, a less-explored issue is\ndesigning an efficient and adaptable network backbone for iterative refinement.\nCurrent options like U-Net and Vision Transformer often rely on\nresource-intensive deep networks and lack the flexibility needed for generating\nimages at variable resolutions or with a smaller network than used in training.\nThis study introduces LEGO bricks, which seamlessly integrate Local-feature\nEnrichment and Global-content Orchestration. These bricks can be stacked to\ncreate a test-time reconfigurable diffusion backbone, allowing selective\nskipping of bricks to reduce sampling costs and generate higher-resolution\nimages than the training data. LEGO bricks enrich local regions with an MLP and\ntransform them using a Transformer block while maintaining a consistent\nfull-resolution image across all bricks. Experimental results demonstrate that\nLEGO bricks enhance training efficiency, expedite convergence, and facilitate\nvariable-resolution image generation while maintaining strong generative\nperformance. Moreover, LEGO significantly reduces sampling time compared to\nother methods, establishing it as a valuable enhancement for diffusion models.\nOur code and project page are available at\nhttps://jegzheng.github.io/LEGODiffusion.\n","authors":["Huangjie Zheng","Zhendong Wang","Jianbo Yuan","Guanghan Ning","Pengcheng He","Quanzeng You","Hongxia Yang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06389v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19435v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"A Sanity Check for AI-generated Image Detection","summary":"  With the rapid development of generative models, discerning AI-generated\ncontent has evoked increasing attention from both industry and academia. In\nthis paper, we conduct a sanity check on \"whether the task of AI-generated\nimage detection has been solved\". To start with, we present Chameleon dataset,\nconsisting AIgenerated images that are genuinely challenging for human\nperception. To quantify the generalization of existing methods, we evaluate 9\noff-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,\nalmost all models classify AI-generated images as real ones. Later, we propose\nAIDE (AI-generated Image DEtector with Hybrid Features), which leverages\nmultiple experts to simultaneously extract visual artifacts and noise patterns.\nSpecifically, to capture the high-level semantics, we utilize CLIP to compute\nthe visual embedding. This effectively enables the model to discern\nAI-generated images based on semantics or contextual information; Secondly, we\nselect the highest frequency patches and the lowest frequency patches in the\nimage, and compute the low-level patchwise features, aiming to detect\nAI-generated images by low-level artifacts, for example, noise pattern,\nanti-aliasing, etc. While evaluating on existing benchmarks, for example,\nAIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to\nstate-of-the-art methods, and on our proposed challenging Chameleon benchmarks,\nit also achieves the promising results, despite this problem for detecting\nAI-generated images is far from being solved. The dataset, codes, and pre-train\nmodels will be published at https://github.com/shilinyan99/AIDE.\n","authors":["Shilin Yan","Ouxiang Li","Jiayin Cai","Yanbin Hao","Xiaolong Jiang","Yao Hu","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.19435v1.pdf","comment":"Project page: https://shilinyan99.github.io/AIDE Code:\n  https://github.com/shilinyan99/AIDE"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2406.19283v1","updated":"2024-06-27T15:55:53Z","published":"2024-06-27T15:55:53Z","title":"PhysioLLM: Supporting Personalized Health Insights with Wearables and\n  Large Language Models","summary":"  We present PhysioLLM, an interactive system that leverages large language\nmodels (LLMs) to provide personalized health understanding and exploration by\nintegrating physiological data from wearables with contextual information.\nUnlike commercial health apps for wearables, our system offers a comprehensive\nstatistical analysis component that discovers correlations and trends in user\ndata, allowing users to ask questions in natural language and receive generated\npersonalized insights, and guides them to develop actionable goals. As a case\nstudy, we focus on improving sleep quality, given its measurability through\nphysiological data and its importance to general well-being. Through a user\nstudy with 24 Fitbit watch users, we demonstrate that PhysioLLM outperforms\nboth the Fitbit App alone and a generic LLM chatbot in facilitating a deeper,\npersonalized understanding of health data and supporting actionable steps\ntoward personal health goals.\n","authors":["Cathy Mengying Fang","Valdemar Danry","Nathan Whitmore","Andria Bao","Andrew Hutchison","Cayden Pierce","Pattie Maes"],"pdf_url":"https://arxiv.org/pdf/2406.19283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07683v3","updated":"2024-06-27T15:54:58Z","published":"2023-09-14T12:58:30Z","title":"Assessing the nature of large language models: A caution against\n  anthropocentrism","summary":"  Generative AI models garnered a large amount of public attention and\nspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion\ncamps exist: one excited about possibilities these models offer for fundamental\nchanges to human tasks, and another highly concerned about power these models\nseem to have. To address these concerns, we assessed several LLMs, primarily\nGPT 3.5, using standard, normed, and validated cognitive and personality\nmeasures. For this seedling project, we developed a battery of tests that\nallowed us to estimate the boundaries of some of these models capabilities, how\nstable those capabilities are over a short period of time, and how they compare\nto humans. Our results indicate that LLMs are unlikely to have developed\nsentience, although its ability to respond to personality inventories is\ninteresting. GPT3.5 did display large variability in both cognitive and\npersonality measures over repeated observations, which is not expected if it\nhad a human-like personality. Variability notwithstanding, LLMs display what in\na human would be considered poor mental health, including low self-esteem,\nmarked dissociation from reality, and in some cases narcissism and psychopathy,\ndespite upbeat and helpful responses.\n","authors":["Ann Speed"],"pdf_url":"https://arxiv.org/pdf/2309.07683v3.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19226v1","updated":"2024-06-27T14:51:07Z","published":"2024-06-27T14:51:07Z","title":"Simulating Classroom Education with LLM-Empowered Agents","summary":"  Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.\n","authors":["Zheyuan Zhang","Daniel Zhang-Li","Jifan Yu","Linlu Gong","Jinchang Zhou","Zhiyuan Liu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07474v2","updated":"2024-06-27T13:17:58Z","published":"2024-05-13T05:23:48Z","title":"Integrating Intent Understanding and Optimal Behavior Planning for\n  Behavior Tree Generation from Human Instructions","summary":"  Robots executing tasks following human instructions in domestic or industrial\nenvironments essentially require both adaptability and reliability. Behavior\nTree (BT) emerges as an appropriate control architecture for these scenarios\ndue to its modularity and reactivity. Existing BT generation methods, however,\neither do not involve interpreting natural language or cannot theoretically\nguarantee the BTs' success. This paper proposes a two-stage framework for BT\ngeneration, which first employs large language models (LLMs) to interpret goals\nfrom high-level instructions, then constructs an efficient goal-specific BT\nthrough the Optimal Behavior Tree Expansion Algorithm (OBTEA). We represent\ngoals as well-formed formulas in first-order logic, effectively bridging intent\nunderstanding and optimal behavior planning. Experiments in the service robot\nvalidate the proficiency of LLMs in producing grammatically correct and\naccurately interpreted goals, demonstrate OBTEA's superiority over the baseline\nBT Expansion algorithm in various metrics, and finally confirm the practical\ndeployability of our framework. The project website is\nhttps://dids-ei.github.io/Project/LLM-OBTEA/.\n","authors":["Xinglin Chen","Yishuai Cai","Yunxin Mao","Minglong Li","Wenjing Yang","Weixia Xu","Ji Wang"],"pdf_url":"https://arxiv.org/pdf/2405.07474v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19054v1","updated":"2024-06-27T10:01:56Z","published":"2024-06-27T10:01:56Z","title":"A look under the hood of the Interactive Deep Learning Enterprise\n  (No-IDLE)","summary":"  This DFKI technical report presents the anatomy of the No-IDLE prototype\nsystem (funded by the German Federal Ministry of Education and Research) that\nprovides not only basic and fundamental research in interactive machine\nlearning, but also reveals deeper insights into users' behaviours, needs, and\ngoals. Machine learning and deep learning should become accessible to millions\nof end users. No-IDLE's goals and scienfific challenges centre around the\ndesire to increase the reach of interactive deep learning solutions for\nnon-experts in machine learning. One of the key innovations described in this\ntechnical report is a methodology for interactive machine learning combined\nwith multimodal interaction which will become central when we start interacting\nwith semi-intelligent machines in the upcoming area of neural networks and\nlarge language models.\n","authors":["Daniel Sonntag","Michael Barz","Thiago Gouvêa"],"pdf_url":"https://arxiv.org/pdf/2406.19054v1.pdf","comment":"DFKI Technical Report"},{"id":"http://arxiv.org/abs/2403.07122v2","updated":"2024-06-27T08:43:26Z","published":"2024-03-11T19:31:36Z","title":"Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars\n  and Virtual Others in Virtual Reality","summary":"  Virtual humans play a pivotal role in social virtual environments, shaping\nusers' VR experiences. The diversity in available options and users'\npreferences can result in a heterogeneous mix of appearances among a group of\nvirtual humans. The resulting variety in higher-order anthropomorphic and\nrealistic cues introduces multiple (in)congruencies, eventually impacting the\nplausibility of the experience. In this work, we consider the impact of\n(in)congruencies in the realism of a group of virtual humans, including\nco-located others and one's self-avatar. In a 2 x 3 mixed design, participants\nembodied either (1) a personalized realistic or (2) a customized stylized\nself-avatar across three consecutive VR exposures in which they were\naccompanied by a group of virtual others being either (1) all realistic, (2)\nall stylized, or (3) mixed. Our results indicate groups of virtual others of\nhigher realism, i.e., potentially more congruent with participants' real-world\nexperiences and expectations, were considered more human-like, increasing the\nfeeling of co-presence and the impression of interaction possibilities.\n(In)congruencies concerning the homogeneity of the group did not cause\nconsiderable effects. Furthermore, our results indicate that a self-avatar's\ncongruence with the participant's real-world experiences concerning their own\nphysical body yielded notable benefits for virtual body ownership and\nself-identification for realistic personalized avatars. Notably, the\nincongruence between a stylized self-avatar and a group of realistic virtual\nothers resulted in diminished ratings of self-location and self-identification.\nWe conclude on the implications of our findings and discuss our results within\ncurrent theories of VR experiences, considering (in)congruent visual cues and\ntheir impact on the perception of virtual others, self-representation, and\nspatial presence.\n","authors":["David Mal","Nina Döllinger","Erik Wolf","Stephan Wenninger","Mario Botsch","Carolin Wienrich","Marc Erich Latoschik"],"pdf_url":"https://arxiv.org/pdf/2403.07122v2.pdf","comment":"Provisionally accepted - Original research article in Frontiers in\n  Virtual Reality, Section Virtual Reality and Human Behavior"},{"id":"http://arxiv.org/abs/2406.19575v1","updated":"2024-06-27T23:27:35Z","published":"2024-06-27T23:27:35Z","title":"AR-PPF: Advanced Resolution-Based Pixel Preemption Data Filtering for\n  Efficient Time-Series Data Analysis","summary":"  With the advent of automation, many manufacturing industries have\ntransitioned to data-centric methodologies, giving rise to an unprecedented\ninflux of data during the manufacturing process. This data has become\ninstrumental in analyzing the quality of manufacturing process and equipment.\nEngineers and data analysts, in particular, require extensive time-series data\nfor seasonal cycle analysis. However, due to computational resource\nconstraints, they are often limited to querying short-term data multiple times\nor resorting to the use of summarized data in which key patterns may be\noverlooked. This study proposes a novel solution to overcome these limitations;\nthe advanced resolution-based pixel preemption data filtering (AR-PPF)\nalgorithm. This technology allows for efficient visualization of time-series\ncharts over long periods while significantly reducing the time required to\nretrieve data. We also demonstrates how this approach not only enhances the\nefficiency of data analysis but also ensures that key feature is not lost,\nthereby providing a more accurate and comprehensive understanding of the data.\n","authors":["Taewoong Kim","Kukjin Choi","Sungjun Kim"],"pdf_url":"https://arxiv.org/pdf/2406.19575v1.pdf","comment":"7pages, preprint, '24 Samsung Best Paper Awards"},{"id":"http://arxiv.org/abs/2406.19546v1","updated":"2024-06-27T21:55:31Z","published":"2024-06-27T21:55:31Z","title":"Understanding Modality Preferences in Search Clarification","summary":"  This study is the first attempt to explore the impact of clarification\nquestion modality on user preference in search engines. We introduce the\nmulti-modal search clarification dataset, MIMICS-MM, containing clarification\nquestions with associated expert-collected and model-generated images. We\nanalyse user preferences over different clarification modes of text, image, and\ncombination of both through crowdsourcing by taking into account image and text\nquality, clarity, and relevance. Our findings demonstrate that users generally\nprefer multi-modal clarification over uni-modal approaches. We explore the use\nof automated image generation techniques and compare the quality, relevance,\nand user preference of model-generated images with human-collected ones. The\nstudy reveals that text-to-image generation models, such as Stable Diffusion,\ncan effectively generate multi-modal clarification questions. By investigating\nmulti-modal clarification, this research establishes a foundation for future\nadvancements in search systems.\n","authors":["Leila Tavakoli","Giovanni Castiglia","Federica Calo","Yashar Deldjoo","Hamed Zamani","Johanne R. Trippas"],"pdf_url":"https://arxiv.org/pdf/2406.19546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19528v1","updated":"2024-06-27T21:03:56Z","published":"2024-06-27T21:03:56Z","title":"Using Large Language Models to Assist Video Content Analysis: An\n  Exploratory Study of Short Videos on Depression","summary":"  Despite the growing interest in leveraging Large Language Models (LLMs) for\ncontent analysis, current studies have primarily focused on text-based content.\nIn the present work, we explored the potential of LLMs in assisting video\ncontent analysis by conducting a case study that followed a new workflow of\nLLM-assisted multimodal content analysis. The workflow encompasses codebook\ndesign, prompt engineering, LLM processing, and human evaluation. We\nstrategically crafted annotation prompts to get LLM Annotations in structured\nform and explanation prompts to generate LLM Explanations for a better\nunderstanding of LLM reasoning and transparency. To test LLM's video annotation\ncapabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos\nabout depression. We compared the LLM Annotations with those of two human\ncoders and found that LLM has higher accuracy in object and activity\nAnnotations than emotion and genre Annotations. Moreover, we identified the\npotential and limitations of LLM's capabilities in annotating videos. Based on\nthe findings, we explore opportunities and challenges for future research and\nimprovements to the workflow. We also discuss ethical concerns surrounding\nfuture studies based on LLM-assisted video analysis.\n","authors":["Jiaying Liu","Yunlong Wang","Yao Lyu","Yiheng Su","Shuo Niu","Xuhai \"Orson\" Xu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19528v1.pdf","comment":"6 pages, 2 figures, under review in CSCW 24"},{"id":"http://arxiv.org/abs/2406.19512v1","updated":"2024-06-27T20:18:18Z","published":"2024-06-27T20:18:18Z","title":"Captioning Visualizations with Large Language Models (CVLLM): A Tutorial","summary":"  Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.\n","authors":["Giuseppe Carenini","Jordon Johnson","Ali Salamatian"],"pdf_url":"https://arxiv.org/pdf/2406.19512v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2307.08564v2","updated":"2024-06-27T17:18:10Z","published":"2023-07-17T15:31:58Z","title":"Shaping New Norms for AI","summary":"  As Artificial Intelligence (AI) becomes increasingly integrated into our\nlives, the need for new norms is urgent. However, AI evolves at a much faster\npace than the characteristic time of norm formation, posing an unprecedented\nchallenge to our societies. This paper examines possible criticalities of the\nprocesses of norm formation surrounding AI. Thus, it focuses on how new norms\ncan be established, rather than on what these norms should be. It distinguishes\ndifferent scenarios based on the centralisation or decentralisation of the norm\nformation process, analysing the cases where new norms are shaped by formal\nauthorities, informal institutions, or emerge spontaneously in a bottom-up\nfashion. On the latter point, the paper reports a conversation with ChatGPT in\nwhich the LLM discusses some of the emerging norms it has observed. Far from\nseeking exhaustiveness, this article aims to offer readers interpretive tools\nto understand society's response to the growing pervasiveness of AI. An outlook\non how AI could influence the formation of future social norms emphasises the\nimportance for open societies to anchor their formal deliberation process in an\nopen, inclusive, and transparent public discourse.\n","authors":["Andrea Baronchelli"],"pdf_url":"https://arxiv.org/pdf/2307.08564v2.pdf","comment":null}]},"2024-06-28T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.20099v1","updated":"2024-06-28T17:59:51Z","published":"2024-06-28T17:59:51Z","title":"Odd-One-Out: Anomaly Detection by Comparing with Neighbors","summary":"  This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances within a\nscene. Unlike the traditional AD benchmarks, in our setting, anomalies in this\ncontext are scene-specific, defined by the regular instances that make up the\nmajority. Since object instances are often partly visible from a single\nviewpoint, our setting provides multiple views of each scene as input. To\nprovide a testbed for future research in this task, we introduce two\nbenchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates\n3D object-centric representations for each instance and detects the anomalous\nones through a cross-examination between the instances. We rigorously analyze\nour method quantitatively and qualitatively in the presented benchmarks.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.20099v1.pdf","comment":"Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD"},{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20092v1","updated":"2024-06-28T17:57:14Z","published":"2024-06-28T17:57:14Z","title":"LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context\n  Compression","summary":"  While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in large multi-modal models (LMMs) has remained a largely overlooked\narea. In this work, we present the study on the analysis of redundancy\nconcerning visual tokens and efficient training within these models. Our\ninitial experiments show that eliminating up to 70% of visual tokens at the\ntesting stage by simply average pooling only leads to a minimal 3% reduction in\nvisual question answering accuracy on the GQA benchmark, indicating significant\nredundancy in visual context. Addressing this, we introduce Visual Context\nCompressor, which reduces the number of visual tokens during training to\nenhance training efficiency without sacrificing performance. To minimize\ninformation loss caused by the compression on visual tokens while maintaining\ntraining efficiency, we develop LLaVolta as a lite training scheme. LLaVolta\nincorporates stage-wise visual context compression to progressively compress\nthe visual tokens from heavily to lightly, and finally no compression at the\nend of training, yielding no loss of information when testing. Extensive\nexperiments demonstrate that our approach enhances the performance of MLLMs in\nboth image-language and video-language understanding, while also significantly\ncutting training costs. Code is available at\nhttps://github.com/Beckschen/LLaVolta\n","authors":["Jieneng Chen","Luoxin Ye","Ju He","Zhao-Yang Wang","Daniel Khashabi","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2406.20092v1.pdf","comment":"Code is available at https://github.com/Beckschen/LLaVolta"},{"id":"http://arxiv.org/abs/2406.20085v1","updated":"2024-06-28T17:53:18Z","published":"2024-06-28T17:53:18Z","title":"Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language","summary":"  Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.\n","authors":["Yicheng Chen","Xiangtai Li","Yining Li","Yanhong Zeng","Jianzong Wu","Xiangyu Zhao","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.20085v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.20083v1","updated":"2024-06-28T17:51:10Z","published":"2024-06-28T17:51:10Z","title":"PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful\n  Navigators","summary":"  We present PoliFormer (Policy Transformer), an RGB-only indoor navigation\nagent trained end-to-end with reinforcement learning at scale that generalizes\nto the real-world without adaptation despite being trained purely in\nsimulation. PoliFormer uses a foundational vision transformer encoder with a\ncausal transformer decoder enabling long-term memory and reasoning. It is\ntrained for hundreds of millions of interactions across diverse environments,\nleveraging parallelized, multi-machine rollouts for efficient training with\nhigh throughput. PoliFormer is a masterful navigator, producing\nstate-of-the-art results across two distinct embodiments, the LoCoBot and\nStretch RE-1 robots, and four navigation benchmarks. It breaks through the\nplateaus of previous work, achieving an unprecedented 85.5% success rate in\nobject goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.\nPoliFormer can also be trivially extended to a variety of downstream\napplications such as object tracking, multi-object navigation, and\nopen-vocabulary navigation with no finetuning.\n","authors":["Kuo-Hao Zeng","Zichen Zhang","Kiana Ehsani","Rose Hendrix","Jordi Salvador","Alvaro Herrasti","Ross Girshick","Aniruddha Kembhavi","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2406.20083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20081v1","updated":"2024-06-28T17:47:32Z","published":"2024-06-28T17:47:32Z","title":"Segment Anything without Supervision","summary":"  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.\n","authors":["XuDong Wang","Jingfeng Yang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2406.20081v1.pdf","comment":"Code: https://github.com/frank-xwang/UnSAM"},{"id":"http://arxiv.org/abs/2406.20078v1","updated":"2024-06-28T17:42:08Z","published":"2024-06-28T17:42:08Z","title":"GM-DF: Generalized Multi-Scenario Deepfake Detection","summary":"  Existing face forgery detection usually follows the paradigm of training\nmodels in a single domain, which leads to limited generalization capacity when\nunseen scenarios and unknown attacks occur. In this paper, we elaborately\ninvestigate the generalization capacity of deepfake detection models when\njointly trained on multiple face forgery detection datasets. We first find a\nrapid degradation of detection accuracy when models are directly trained on\ncombined datasets due to the discrepancy across collection scenarios and\ngeneration methods. To address the above issue, a Generalized Multi-Scenario\nDeepfake Detection framework (GM-DF) is proposed to serve multiple real-world\nscenarios by a unified model. First, we propose a hybrid expert modeling\napproach for domain-specific real/forgery feature extraction. Besides, as for\nthe commonality representation, we use CLIP to extract the common features for\nbetter aligning visual and textual features across domains. Meanwhile, we\nintroduce a masked image reconstruction mechanism to force models to capture\nrich forged details. Finally, we supervise the models via a domain-aware\nmeta-learning strategy to further enhance their generalization capacities.\nSpecifically, we design a novel domain alignment loss to strongly align the\ndistributions of the meta-test domains and meta-train domains. Thus, the\nupdated models are able to represent both specific and common real/forgery\nfeatures across multiple datasets. In consideration of the lack of study of\nmulti-dataset training, we establish a new benchmark leveraging multi-source\ndata to fairly evaluate the models' generalization capacity on unseen\nscenarios. Both qualitative and quantitative experiments on five datasets\nconducted on traditional protocols as well as the proposed benchmark\ndemonstrate the effectiveness of our approach.\n","authors":["Yingxin Lai","Zitong Yu","Jing Yang","Bin Li","Xiangui Kang","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20077v1","updated":"2024-06-28T17:39:38Z","published":"2024-06-28T17:39:38Z","title":"HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model","summary":"  We introduce HouseCrafter, a novel approach that can lift a floorplan into a\ncomplete large 3D indoor scene (e.g., a house). Our key insight is to adapt a\n2D diffusion model, which is trained on web-scale images, to generate\nconsistent multi-view color (RGB) and depth (D) images across different\nlocations of the scene. Specifically, the RGB-D images are generated\nautoregressively in a batch-wise manner along sampled locations based on the\nfloorplan, where previously generated images are used as condition to the\ndiffusion model to produce images at nearby locations. The global floorplan and\nattention design in the diffusion model ensures the consistency of the\ngenerated images, from which a 3D scene can be reconstructed. Through extensive\nevaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate\nhigh-quality house-scale 3D scenes. Ablation studies also validate the\neffectiveness of different design choices. We will release our code and model\nweights. Project page: https://neu-vi.github.io/houseCrafter/\n","authors":["Hieu T. Nguyen","Yiwen Chen","Vikram Voleti","Varun Jampani","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.20077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20076v1","updated":"2024-06-28T17:38:18Z","published":"2024-06-28T17:38:18Z","title":"EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model","summary":"  Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.\n","authors":["Yuxuan Zhang","Tianheng Cheng","Rui Hu","ei Liu","Heng Liu","Longjin Ran","Xiaoxin Chen","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20076v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.20066v1","updated":"2024-06-28T17:22:33Z","published":"2024-06-28T17:22:33Z","title":"ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for\n  High-Quality Radiance Fields Reconstruction","summary":"  NeRF-based methods reconstruct 3D scenes by building a radiance field with\nimplicit or explicit representations. While NeRF-based methods can perform\nnovel view synthesis (NVS) at arbitrary scale, the performance in\nhigh-resolution novel view synthesis (HRNVS) with low-resolution (LR)\noptimization often results in oversmoothing. On the other hand, single-image\nsuper-resolution (SR) aims to enhance LR images to HR counterparts but lacks\nmulti-view consistency. To address these challenges, we propose Arbitrary-Scale\nSuper-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel\nview synthesis (SRNVS). We propose an attention-based VoxelGridSR model to\ndirectly perform 3D super-resolution (SR) on the optimized volume. Our model is\ntrained on diverse scenes to ensure generalizability. For unseen scenes trained\nwith LR views, we then can directly apply our VoxelGridSR to further refine the\nvolume and achieve multi-view consistent SR. We demonstrate quantitative and\nqualitatively that the proposed method achieves significant performance in\nSRNVS.\n","authors":["Ding-Jiun Huang","Zi-Ting Chou","Yu-Chiang Frank Wang","Cheng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.20066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07015v4","updated":"2024-06-28T17:14:13Z","published":"2023-05-11T17:55:25Z","title":"Exploiting Diffusion Prior for Real-World Image Super-Resolution","summary":"  We present a novel approach to leverage prior knowledge encapsulated in\npre-trained text-to-image diffusion models for blind super-resolution (SR).\nSpecifically, by employing our time-aware encoder, we can achieve promising\nrestoration results without altering the pre-trained synthesis model, thereby\npreserving the generative prior and minimizing training cost. To remedy the\nloss of fidelity caused by the inherent stochasticity of diffusion models, we\nemploy a controllable feature wrapping module that allows users to balance\nquality and fidelity by simply adjusting a scalar value during the inference\nprocess. Moreover, we develop a progressive aggregation sampling strategy to\novercome the fixed-size constraints of pre-trained diffusion models, enabling\nadaptation to resolutions of any size. A comprehensive evaluation of our method\nusing both synthetic and real-world benchmarks demonstrates its superiority\nover current state-of-the-art approaches. Code and models are available at\nhttps://github.com/IceClear/StableSR.\n","authors":["Jianyi Wang","Zongsheng Yue","Shangchen Zhou","Kelvin C. K. Chan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2305.07015v4.pdf","comment":"Accepted by IJCV'2024. Some Figs are compressed due to size limits.\n  Uncompressed ver.:\n  https://github.com/IceClear/StableSR/releases/download/UncompressedPDF/StableSR_IJCV_Uncompressed.pdf.\n  Project page: https://iceclear.github.io/projects/stablesr/"},{"id":"http://arxiv.org/abs/2406.20055v1","updated":"2024-06-28T17:07:11Z","published":"2024-06-28T17:07:11Z","title":"SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,\noffering efficient training and rendering speeds, making it suitable for\nreal-time applications.However, current methods require highly controlled\nenvironments (no moving people or wind-blown elements, and consistent lighting)\nto meet the inter-view consistency assumption of 3DGS. This makes\nreconstruction of real-world captures problematic. We present SpotlessSplats,\nan approach that leverages pre-trained and general-purpose features coupled\nwith robust optimization to effectively ignore transient distractors. Our\nmethod achieves state-of-the-art reconstruction quality both visually and\nquantitatively, on casual captures.\n","authors":["Sara Sabour","Lily Goli","George Kopanas","Mark Matthews","Dmitry Lagun","Leonidas Guibas","Alec Jacobson","David J. Fleet","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2406.20055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20042v1","updated":"2024-06-28T16:40:57Z","published":"2024-06-28T16:40:57Z","title":"HAITCH: A Framework for Distortion and Motion Correction in Fetal\n  Multi-Shell Diffusion-Weighted MRI","summary":"  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the\nmicrostructure of the rapidly-developing fetal brain. However, fetal motion\nduring scans and its interaction with magnetic field inhomogeneities result in\nartifacts and data scattering across spatial and angular domains. The effects\nof those artifacts are more pronounced in high-angular resolution fetal dMRI,\nwhere signal-to-noise ratio is very low. Those effects lead to biased estimates\nand compromise the consistency and reliability of dMRI analysis. This work\npresents HAITCH, the first and the only publicly available tool to correct and\nreconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers\nseveral technical advances that include a blip-reversed dual-echo acquisition\nfor dynamic distortion correction, advanced motion correction for model-free\nand robust reconstruction, optimized multi-shell design for enhanced\ninformation capture and increased tolerance to motion, and outlier detection\nfor improved reconstruction fidelity. The framework is open-source, flexible,\nand can be used to process any type of fetal dMRI data including single-echo or\nsingle-shell acquisitions, but is most effective when used with multi-shell\nmulti-echo fetal dMRI data that cannot be processed with any of the existing\ntools. Validation experiments on real fetal dMRI scans demonstrate significant\nimprovements and accurate correction across diverse fetal ages and motion\nlevels. HAITCH successfully removes artifacts and reconstructs high-fidelity\nfetal dMRI data suitable for advanced diffusion modeling, including fiber\norientation distribution function estimation. These advancements pave the way\nfor more reliable analysis of the fetal brain microstructure and tractography\nunder challenging imaging conditions.\n","authors":["Haykel Snoussi","Davood Karimi","Onur Afacan","Mustafa Utkur","Ali Gholipour"],"pdf_url":"https://arxiv.org/pdf/2406.20042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15180v2","updated":"2024-06-28T16:40:39Z","published":"2023-07-27T20:19:11Z","title":"EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical\n  Guarantees","summary":"  The popularity of text-based CAPTCHA as a security mechanism to protect\nwebsites from automated bots has prompted researches in CAPTCHA solvers, with\nthe aim of understanding its failure cases and subsequently making CAPTCHAs\nmore secure. Recently proposed solvers, built on advances in deep learning, are\nable to crack even the very challenging CAPTCHAs with high accuracy. However,\nthese solvers often perform poorly on out-of-distribution samples that contain\nvisual features different from those in the training set. Furthermore, they\nlack the ability to detect and avoid such samples, making them susceptible to\nbeing locked out by defense systems after a certain number of failed attempts.\nIn this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep\nensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it\nharder to be detected. We prove novel theoretical bounds on the effectiveness\nof our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.\nOur experiments show that the proposed approaches perform well when cracking\nCAPTCHA datasets that contain both in-distribution and out-of-distribution\nsamples.\n","authors":["Duc C. Hoang","Behzad Ousat","Amin Kharraz","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2307.15180v2.pdf","comment":"A previous version of this paper was presented at the Epistemic\n  Uncertainty - E-pi UAI 2023 Workshop"},{"id":"http://arxiv.org/abs/2406.20024v1","updated":"2024-06-28T16:13:55Z","published":"2024-06-28T16:13:55Z","title":"eMoE-Tracker: Environmental MoE-based Transformer for Robust\n  Event-guided Object Tracking","summary":"  The unique complementarity of frame-based and event cameras for high frame\nrate object tracking has recently inspired some research attempts to develop\nmulti-modal fusion approaches. However, these methods directly fuse both\nmodalities and thus ignore the environmental attributes, e.g., motion blur,\nillumination variance, occlusion, scale variation, etc. Meanwhile, no\ninteraction between search and template features makes distinguishing target\nobjects and backgrounds difficult. As a result, performance degradation is\ninduced especially in challenging conditions. This paper proposes a novel and\neffective Transformer-based event-guided tracking framework, called\neMoE-Tracker, which achieves new SOTA performance under various conditions. Our\nkey idea is to disentangle the environment into several learnable attributes to\ndynamically learn the attribute-specific features for better interaction and\ndiscriminability between the target information and background. To achieve the\ngoal, we first propose an environmental Mix-of-Experts (eMoE) module that is\nbuilt upon the environmental Attributes Disentanglement to learn\nattribute-specific features and environmental Attributes Gating to assemble the\nattribute-specific features by the learnable attribute scores dynamically. The\neMoE module is a subtle router that fine-tunes the transformer backbone more\nefficiently. We then introduce a contrastive relation modeling (CRM) module to\nimprove interaction and discriminability between the target information and\nbackground. Extensive experiments on diverse event-based benchmark datasets\nshowcase the superior performance of our eMoE-Tracker compared to the prior\narts.\n","authors":["Yucheng Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20024v1.pdf","comment":"RGB-event single object tracking"},{"id":"http://arxiv.org/abs/2406.20005v1","updated":"2024-06-28T15:44:55Z","published":"2024-06-28T15:44:55Z","title":"Malaria Cell Detection Using Deep Neural Networks","summary":"  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n","authors":["Saurabh Sawant","Anurag Singh"],"pdf_url":"https://arxiv.org/pdf/2406.20005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v3","updated":"2024-06-28T15:42:12Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v3.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2406.19997v1","updated":"2024-06-28T15:32:59Z","published":"2024-06-28T15:32:59Z","title":"Wavelets Are All You Need for Autoregressive Image Generation","summary":"  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n","authors":["Wael Mattar","Idan Levy","Nir Sharon","Shai Dekel"],"pdf_url":"https://arxiv.org/pdf/2406.19997v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19973v1","updated":"2024-06-28T15:01:23Z","published":"2024-06-28T15:01:23Z","title":"STLLaVA-Med: Self-Training Large Language and Vision Assistant for\n  Medical","summary":"  Large Vision-Language Models (LVLMs) have shown significant potential in\nassisting medical diagnosis by leveraging extensive biomedical datasets.\nHowever, the advancement of medical image understanding and reasoning\ncritically depends on building high-quality visual instruction data, which is\ncostly and labor-intensive to obtain, particularly in the medical domain. To\nmitigate this data-starving issue, we introduce Self-Training Large Language\nand Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed\nto train a policy model (an LVLM) capable of auto-generating medical visual\ninstruction data to improve data efficiency, guided through Direct Preference\nOptimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,\nGPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning\nprocess on the auto-generated data, encouraging the policy model to align\nefficiently with human preferences. We validate the efficacy and data\nefficiency of STLLaVA-Med across three major medical Visual Question Answering\n(VQA) benchmarks, demonstrating competitive zero-shot performance with the\nutilization of only 9% of the medical data.\n","authors":["Guohao Sun","Can Qin","Huazhu Fu","Linwei Wang","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2406.19973v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05779v3","updated":"2024-06-28T14:53:06Z","published":"2024-06-09T13:25:02Z","title":"Learning to utilize image second-order derivative information for crisp\n  edge detection","summary":"  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) the lack of\nprior knowledge regarding image edges, and (2) the issue of imbalanced pixel\ndistribution. We propose a second-order derivative-based multi-scale contextual\nenhancement module (SDMCM) to help the model locate true edge pixels accurately\nby introducing the edge prior knowledge. We also construct a hybrid focal loss\nfunction (HFL) to alleviate the imbalanced distribution issue. In addition, we\nemploy the conditionally parameterized convolution (CondConv) to develop a\nnovel boundary refinement module (BRM), which can further refine the final\noutput edge maps. In the end, we propose a U-shape network named LUS-Net which\nis based on the SDMCM and BRM for crisp edge detection. We perform extensive\nexperiments on three standard benchmarks, and the experiment results illustrate\nthat our method can predict crisp and clean edge maps and achieves\nstate-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2\ndataset (ODS=0.768), and BIPED dataset (ODS=0.903).\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Yimeng Fan","Mingyang Li","Wenlin Li"],"pdf_url":"https://arxiv.org/pdf/2406.05779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17032v2","updated":"2024-06-28T14:34:00Z","published":"2024-06-24T18:00:11Z","title":"DWARF: Disease-weighted network for attention map refinement","summary":"  The interpretability of deep learning is crucial for evaluating the\nreliability of medical imaging models and reducing the risks of inaccurate\npatient recommendations. This study addresses the \"human out of the loop\" and\n\"trustworthiness\" issues in medical image analysis by integrating medical\nprofessionals into the interpretability process. We propose a disease-weighted\nattention map refinement network (DWARF) that leverages expert feedback to\nenhance model relevance and accuracy. Our method employs cyclic training to\niteratively improve diagnostic performance, generating precise and\ninterpretable feature maps. Experimental results demonstrate significant\nimprovements in interpretability and diagnostic accuracy across multiple\nmedical imaging datasets. This approach fosters effective collaboration between\nAI systems and healthcare professionals, ultimately aiming to improve patient\noutcomes\n","authors":["Haozhe Luo","Aurélie Pahud de Mortanges","Oana Inel","Abraham Bernstein","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2406.17032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19943v1","updated":"2024-06-28T14:22:30Z","published":"2024-06-28T14:22:30Z","title":"Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches","summary":"  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n","authors":["Andjela Dimitrijevic","Vincent Noblet","Benjamin De Leener"],"pdf_url":"https://arxiv.org/pdf/2406.19943v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2406.19941v1","updated":"2024-06-28T14:17:16Z","published":"2024-06-28T14:17:16Z","title":"GRACE: Graph-Regularized Attentive Convolutional Entanglement with\n  Laplacian Smoothing for Robust DeepFake Video Detection","summary":"  As DeepFake video manipulation techniques escalate, posing profound threats,\nthe urgent need to develop efficient detection strategies is underscored.\nHowever, one particular issue lies with facial images being mis-detected, often\noriginating from degraded videos or adversarial attacks, leading to unexpected\ntemporal artifacts that can undermine the efficacy of DeepFake video detection\ntechniques. This paper introduces a novel method for robust DeepFake video\ndetection, harnessing the power of the proposed Graph-Regularized Attentive\nConvolutional Entanglement (GRACE) based on the graph convolutional network\nwith graph Laplacian to address the aforementioned challenges. First,\nconventional Convolution Neural Networks are deployed to perform spatiotemporal\nfeatures for the entire video. Then, the spatial and temporal features are\nmutually entangled by constructing a graph with sparse constraint, enforcing\nessential features of valid face images in the noisy face sequences remaining,\nthus augmenting stability and performance for DeepFake video detection.\nFurthermore, the Graph Laplacian prior is proposed in the graph convolutional\nnetwork to remove the noise pattern in the feature space to further improve the\nperformance. Comprehensive experiments are conducted to illustrate that our\nproposed method delivers state-of-the-art performance in DeepFake video\ndetection under noisy face sequences. The source code is available at\nhttps://github.com/ming053l/GRACE.\n","authors":["Chih-Chung Hsu","Shao-Ning Chen","Mei-Hsuan Wu","Yi-Fang Wang","Chia-Ming Lee","Yi-Shiuan Chou"],"pdf_url":"https://arxiv.org/pdf/2406.19941v1.pdf","comment":"Submitted to TPAMI 2024"},{"id":"http://arxiv.org/abs/2404.00548v2","updated":"2024-06-28T14:13:18Z","published":"2024-03-31T03:30:37Z","title":"Modeling State Shifting via Local-Global Distillation for Event-Frame\n  Gaze Tracking","summary":"  This paper tackles the problem of passive gaze estimation using both event\nand frame data. Considering the inherently different physiological structures,\nit is intractable to accurately estimate gaze purely based on a given state.\nThus, we reformulate gaze estimation as the quantification of the state\nshifting from the current state to several prior registered anchor states.\nSpecifically, we propose a two-stage learning-based gaze estimation framework\nthat divides the whole gaze estimation process into a coarse-to-fine approach\ninvolving anchor state selection and final gaze location. Moreover, to improve\nthe generalization ability, instead of learning a large gaze estimation network\ndirectly, we align a group of local experts with a student network, where a\nnovel denoising distillation algorithm is introduced to utilize denoising\ndiffusion techniques to iteratively remove inherent noise in event data.\nExtensive experiments demonstrate the effectiveness of the proposed method,\nwhich surpasses state-of-the-art methods by a large margin of 15$\\%$. The code\nwill be publicly available at\nhttps://github.com/jdjdli/Denoise_distill_EF_gazetracker.\n","authors":["Jiading Li","Zhiyu Zhu","Jinhui Hou","Junhui Hou","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2404.00548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00592v2","updated":"2024-06-28T14:02:06Z","published":"2023-12-01T13:56:28Z","title":"Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)","summary":"  Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.\n","authors":["Emma Cramer","Jonas Reiher","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2312.00592v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19922v1","updated":"2024-06-28T13:51:59Z","published":"2024-06-28T13:51:59Z","title":"Parallax-tolerant Image Stitching via Segmentation-guided\n  Multi-homography Warping","summary":"  Large parallax between images is an intractable issue in image stitching.\nVarious warping-based methods are proposed to address it, yet the results are\nunsatisfactory. In this paper, we propose a novel image stitching method using\nmulti-homography warping guided by image segmentation. Specifically, we\nleverage the Segment Anything Model to segment the target image into numerous\ncontents and partition the feature points into multiple subsets via the\nenergy-based multi-homography fitting algorithm. The multiple subsets of\nfeature points are used to calculate the corresponding multiple homographies.\nFor each segmented content in the overlapping region, we select its\nbest-fitting homography with the lowest photometric error. For each segmented\ncontent in the non-overlapping region, we calculate a weighted combination of\nthe linearized homographies. Finally, the target image is warped via the\nbest-fitting homographies to align with the reference image, and the final\npanorama is generated via linear blending. Comprehensive experimental results\non the public datasets demonstrate that our method provides the best alignment\naccuracy by a large margin, compared with the state-of-the-art methods. The\nsource code is available at https://github.com/tlliao/multi-homo-warp.\n","authors":["Tianli Liao","Ce Wang","Lei Li","Guangen Liu","Nan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19922v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.19905v1","updated":"2024-06-28T13:20:17Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in the study of\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthus they employ a router to predict the routing for each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization direction of tokens. This can lead to severe optimization\nconflicts between different tokens within an expert. To address this problem,\nthis paper proposes a novel method based on token-level gradient analysis.\nSpecifically, we first use token-level gradients to identify conflicting tokens\nin experts. Then, we add a specialized loss tailored to eliminate conflicts\namong tokens within each expert. Our method can serve as a plug-in for diverse\nLarge Vision-Language Models, and extensive experimental results demonstrate\nthe effectiveness of our method. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Sheng","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19899v1","updated":"2024-06-28T13:07:43Z","published":"2024-06-28T13:07:43Z","title":"On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images","summary":"  The count of mitotic figures (MFs) observed in hematoxylin and eosin\n(H&E)-stained slides is an important prognostic marker as it is a measure for\ntumor cell proliferation. However, the identification of MFs has a known low\ninter-rater agreement. Deep learning algorithms can standardize this task, but\nthey require large amounts of annotated data for training and validation.\nFurthermore, label noise introduced during the annotation process may impede\nthe algorithm's performance. Unlike H&E, the mitosis-specific antibody\nphospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides\nstained against PHH3 leads to higher agreement among raters and has therefore\nrecently been used as a ground truth for the annotation of MFs in H&E. However,\nas PHH3 facilitates the recognition of cells indistinguishable from H&E stain\nalone, the use of this ground truth could potentially introduce noise into the\nH&E-related dataset, impacting model performance. This study analyzes the\nimpact of PHH3-assisted MF annotation on inter-rater reliability and object\nlevel agreement through an extensive multi-rater experiment. We found that the\nannotators' object-level agreement increased when using PHH3-assisted labeling.\nSubsequently, MF detectors were evaluated on the resulting datasets to\ninvestigate the influence of PHH3-assisted labeling on the models' performance.\nAdditionally, a novel dual-stain MF detector was developed to investigate the\ninterpretation-shift of PHH3-assisted labels used in H&E, which clearly\noutperformed single-stain detectors. However, the PHH3-assisted labels did not\nhave a positive effect on solely H&E-based models. The high performance of our\ndual-input detector reveals an information mismatch between the H&E and\nPHH3-stained images as the cause of this effect.\n","authors":["Jonathan Ganz","Christian Marzahl","Jonas Ammeling","Barbara Richter","Chloé Puget","Daniela Denk","Elena A. Demeter","Flaviu A. Tabaran","Gabriel Wasinger","Karoline Lipnik","Marco Tecilla","Matthew J. Valentine","Michael J. Dark","Niklas Abele","Pompei Bolfa","Ramona Erber","Robert Klopfleisch","Sophie Merz","Taryn A. Donovan","Samir Jabari","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2406.19899v1.pdf","comment":"10 pages, 5 figures, 1 Table"},{"id":"http://arxiv.org/abs/2406.19875v1","updated":"2024-06-28T12:35:01Z","published":"2024-06-28T12:35:01Z","title":"InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in\n  Very Long Video Understanding","summary":"  Understanding long videos, ranging from tens of minutes to several hours,\npresents unique challenges in video comprehension. Despite the increasing\nimportance of long-form video content, existing benchmarks primarily focus on\nshorter clips. To address this gap, we introduce InfiniBench a comprehensive\nbenchmark for very long video understanding which presents 1)The longest video\nduration, averaging 76.34 minutes; 2) The largest number of question-answer\npairs, 108.2K; 3) Diversity in questions that examine nine different skills and\ninclude both multiple-choice questions and open-ended questions; 4)\nHumancentric, as the video sources come from movies and daily TV shows, with\nspecific human-level question designs such as Movie Spoiler Questions that\nrequire critical thinking and comprehensive understanding. Using InfiniBench,\nwe comprehensively evaluate existing Large MultiModality Models (LMMs) on each\nskill, including the commercial model Gemini 1.5 Flash and the open-source\nmodels. The evaluation shows significant challenges in our benchmark.Our\nresults show that the best AI models such Gemini struggles to perform well with\n42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark\nwill stimulate the LMMs community towards long video and human-level\nunderstanding. Our benchmark can be accessed at\nhttps://vision-cair.github.io/InfiniBench/\n","authors":["Kirolos Ataallah","Chenhui Gou","Eslam Abdelrahman","Khushbu Pahwa","Jian Ding","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2406.19875v1.pdf","comment":"16 page ,17 figures"},{"id":"http://arxiv.org/abs/2406.11252v2","updated":"2024-06-28T11:59:01Z","published":"2024-06-17T06:28:58Z","title":"Mining Open Semantics from CLIP: A Relation Transition Perspective for\n  Few-Shot Learning","summary":"  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive\nzero-shot capability. The key to improve the adaptation of CLIP to downstream\ntask with few exemplars lies in how to effectively model and transfer the\nuseful knowledge embedded in CLIP. Previous work mines the knowledge typically\nbased on the limited visual samples and close-set semantics (i.e., within\ntarget category set of downstream task). However, the aligned CLIP image/text\nencoders contain abundant relationships between visual features and almost\ninfinite open semantics, which may benefit the few-shot learning but remains\nunexplored. In this paper, we propose to mine open semantics as anchors to\nperform a relation transition from image-anchor relationship to image-target\nrelationship to make predictions. Specifically, we adopt a transformer module\nwhich takes the visual feature as \"Query\", the text features of the anchors as\n\"Key\" and the similarity matrix between the text features of anchor and target\nclasses as \"Value\". In this way, the output of such a transformer module\nrepresents the relationship between the image and target categories, i.e., the\nclassification predictions. To avoid manually selecting the open semantics, we\nmake the [CLASS] token of input text embedding learnable. We conduct extensive\nexperiments on eleven representative classification datasets. The results show\nthat our method performs favorably against previous state-of-the-arts\nconsidering few-shot classification settings.\n","authors":["Cilin Yan","Haochen Wang","Xiaolong Jiang","Yao Hu","Xu Tang","Guoliang Kang","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2406.11252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19852v1","updated":"2024-06-28T11:49:59Z","published":"2024-06-28T11:49:59Z","title":"FootBots: A Transformer-based Architecture for Motion Prediction in\n  Soccer","summary":"  Motion prediction in soccer involves capturing complex dynamics from player\nand ball interactions. We present FootBots, an encoder-decoder\ntransformer-based architecture addressing motion prediction and conditioned\nmotion prediction through equivariance properties. FootBots captures temporal\nand social dynamics using set attention blocks and multi-attention block\ndecoder. Our evaluation utilizes two datasets: a real soccer dataset and a\ntailored synthetic one. Insights from the synthetic dataset highlight the\neffectiveness of FootBots' social attention mechanism and the significance of\nconditioned motion prediction. Empirical results on real soccer data\ndemonstrate that FootBots outperforms baselines in motion prediction and excels\nin conditioned tasks, such as predicting the players based on the ball\nposition, predicting the offensive (defensive) team based on the ball and the\ndefensive (offensive) team, and predicting the ball position based on all\nplayers. Our evaluation connects quantitative and qualitative findings.\nhttps://youtu.be/9kaEkfzG3L8\n","authors":["Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2406.19852v1.pdf","comment":"Published as a conference paper at IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2406.19844v1","updated":"2024-06-28T11:35:35Z","published":"2024-06-28T11:35:35Z","title":"StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object\n  Tracking and Trajectory Prediction","summary":"  3D multi-object tracking and trajectory prediction are two crucial modules in\nautonomous driving systems. Generally, the two tasks are handled separately in\ntraditional paradigms and a few methods have started to explore modeling these\ntwo tasks in a joint manner recently. However, these approaches suffer from the\nlimitations of single-frame training and inconsistent coordinate\nrepresentations between tracking and prediction tasks. In this paper, we\npropose a streaming and unified framework for joint 3D Multi-Object Tracking\nand trajectory Prediction (StreamMOTP) to address the above challenges.\nFirstly, we construct the model in a streaming manner and exploit a memory bank\nto preserve and leverage the long-term latent features for tracked objects more\neffectively. Secondly, a relative spatio-temporal positional encoding strategy\nis introduced to bridge the gap of coordinate representations between the two\ntasks and maintain the pose-invariance for trajectory prediction. Thirdly, we\nfurther improve the quality and consistency of predicted trajectories with a\ndual-stream predictor. We conduct extensive experiments on popular nuSences\ndataset and the experimental results demonstrate the effectiveness and\nsuperiority of StreamMOTP, which outperforms previous methods significantly on\nboth tasks. Furthermore, we also prove that the proposed framework has great\npotential and advantages in actual applications of autonomous driving.\n","authors":["Jiaheng Zhuang","Guoan Wang","Siyu Zhang","Xiyang Wang","Hangning Zhou","Ziyao Xu","Chi Zhang","Zhiheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03511v3","updated":"2024-06-28T11:23:11Z","published":"2023-12-06T14:13:38Z","title":"Kandinsky 3.0 Technical Report","summary":"  We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3\n","authors":["Vladimir Arkhipkin","Andrei Filatov","Viacheslav Vasilev","Anastasia Maltseva","Said Azizov","Igor Pavlov","Julia Agafonova","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2312.03511v3.pdf","comment":"Project page: https://ai-forever.github.io/Kandinsky-3"},{"id":"http://arxiv.org/abs/2406.19833v1","updated":"2024-06-28T11:11:24Z","published":"2024-06-28T11:11:24Z","title":"LightStereo: Channel Boost Is All Your Need for Efficient 2D Cost\n  Aggregation","summary":"  We present LightStereo, a cutting-edge stereo-matching network crafted to\naccelerate the matching process. Departing from conventional methodologies that\nrely on aggregating computationally intensive 4D costs, LightStereo adopts the\n3D cost volume as a lightweight alternative. While similar approaches have been\nexplored previously, our breakthrough lies in enhancing performance through a\ndedicated focus on the channel dimension of the 3D cost volume, where the\ndistribution of matching costs is encapsulated. Our exhaustive exploration has\nyielded plenty of strategies to amplify the capacity of the pivotal dimension,\nensuring both precision and efficiency. We compare the proposed LightStereo\nwith existing state-of-the-art methods across various benchmarks, which\ndemonstrate its superior performance in speed, accuracy, and resource\nutilization. LightStereo achieves a competitive EPE metric in the SceneFlow\ndatasets while demanding a minimum of only 22 GFLOPs, with an inference time of\njust 17 ms. Our comprehensive analysis reveals the effect of 2D cost\naggregation for stereo matching, paving the way for real-world applications of\nefficient stereo systems. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Dujun Nie","Wenzhao Zheng","Youmin Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19833v1.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/OpenStereo}"},{"id":"http://arxiv.org/abs/2406.19815v1","updated":"2024-06-28T10:45:37Z","published":"2024-06-28T10:45:37Z","title":"Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based\n  on Multi-dimensional Features","summary":"  Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.\n","authors":["Feng Liu","Qing Xu","Qijian Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19814v1","updated":"2024-06-28T10:45:25Z","published":"2024-06-28T10:45:25Z","title":"Extract More from Less: Efficient Fine-Grained Visual Recognition in\n  Low-Data Regimes","summary":"  The emerging task of fine-grained image classification in low-data regimes\nassumes the presence of low inter-class variance and large intra-class\nvariation along with a highly limited amount of training samples per class.\nHowever, traditional ways of separately dealing with fine-grained\ncategorisation and extremely scarce data may be inefficient under both these\nharsh conditions presented together. In this paper, we present a novel\nframework, called AD-Net, aiming to enhance deep neural network performance on\nthis challenge by leveraging the power of Augmentation and Distillation\ntechniques. Specifically, our approach is designed to refine learned features\nthrough self-distillation on augmented samples, mitigating harmful overfitting.\nWe conduct comprehensive experiments on popular fine-grained image\nclassification benchmarks where our AD-Net demonstrates consistent improvement\nover traditional fine-tuning and state-of-the-art low-data techniques.\nRemarkably, with the smallest data available, our framework shows an\noutstanding relative accuracy increase of up to 45 % compared to standard\nResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise\nthat our approach is practically architecture-independent and adds zero extra\ncost at inference time. Additionally, we provide an extensive study on the\nimpact of every framework's component, highlighting the importance of each in\nachieving optimal performance. Source code and trained models are publicly\navailable at github.com/demidovd98/fgic_lowd.\n","authors":["Dmitry Demidov","Abduragim Shtanchaev","Mihail Mihaylov","Mohammad Almansoori"],"pdf_url":"https://arxiv.org/pdf/2406.19814v1.pdf","comment":"Main paper and Appendices"},{"id":"http://arxiv.org/abs/2406.19811v1","updated":"2024-06-28T10:39:36Z","published":"2024-06-28T10:39:36Z","title":"EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D\n  Gaussian Splatting","summary":"  Human activities are inherently complex, and even simple household tasks\ninvolve numerous object interactions. To better understand these activities and\nbehaviors, it is crucial to model their dynamic interactions with the\nenvironment. The recent availability of affordable head-mounted cameras and\negocentric data offers a more accessible and efficient means to understand\ndynamic human-object interactions in 3D environments. However, most existing\nmethods for human activity modeling either focus on reconstructing 3D models of\nhand-object or human-scene interactions or on mapping 3D scenes, neglecting\ndynamic interactions with objects. The few existing solutions often require\ninputs from multiple sources, including multi-camera setups, depth-sensing\ncameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the\nfirst method capable of simultaneously reconstructing 3D scenes and dynamically\ntracking 3D object motion from RGB egocentric input alone. We leverage the\nuniquely discrete nature of Gaussian Splatting and segment dynamic interactions\nfrom the background. Our approach employs a clip-level online learning pipeline\nthat leverages the dynamic nature of human activities, allowing us to\nreconstruct the temporal evolution of the scene in chronological order and\ntrack rigid object motion. Additionally, our method automatically segments\nobject and background Gaussians, providing 3D representations for both static\nscenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic\nGaussian methods in challenging in-the-wild videos and we also qualitatively\ndemonstrate the high quality of the reconstructed models.\n","authors":["Daiwei Zhang","Gengyan Li","Jiajie Li","Mickaël Bressieux","Otmar Hilliges","Marc Pollefeys","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19796v1","updated":"2024-06-28T10:05:58Z","published":"2024-06-28T10:05:58Z","title":"Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting","summary":"  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n","authors":["Wei Li","Jingyang Zhang","Pheng-Ann Heng","Lixu Gu"],"pdf_url":"https://arxiv.org/pdf/2406.19796v1.pdf","comment":"Accepted by MICCAI24"},{"id":"http://arxiv.org/abs/2404.09666v2","updated":"2024-06-28T09:25:25Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Mattias P. Heinrich","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15613v2","updated":"2024-06-28T09:22:38Z","published":"2024-05-24T14:58:51Z","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach","summary":"  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.\n","authors":["Huy V. Vo","Vasil Khalidov","Timothée Darcet","Théo Moutakanni","Nikita Smetanin","Marc Szafraniec","Hugo Touvron","Camille Couprie","Maxime Oquab","Armand Joulin","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2405.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19336v2","updated":"2024-06-28T09:20:01Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v2.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2304.10839v4","updated":"2024-06-28T09:01:03Z","published":"2023-04-21T09:30:22Z","title":"Cross-domain Denoising for Low-dose Multi-frame Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test to\nassist in diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data. However, the real-world scenario differs significantly\nfrom the simulation domain, especially when using the multi-slice spiral\nscanner geometry. This paper proposes a two-stage method for the commercially\navailable multi-slice spiral CT scanners that better exploits the complete\nreconstruction pipeline for LDCT denoising across different domains. Our\napproach makes good use of the high redundancy of multi-slice projections and\nthe volumetric reconstructions while leveraging the over-smoothing problem in\nconventional cascaded frameworks caused by aggressive denoising. The dedicated\ndesign also provides a more explicit interpretation of the data flow. Extensive\nexperiments on various datasets showed that the proposed method could remove up\nto 70\\% of noise without compromised spatial resolution, and subjective\nevaluations by two experienced radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19756v1","updated":"2024-06-28T08:54:44Z","published":"2024-06-28T08:54:44Z","title":"Structure-aware World Model for Probe Guidance via Large-scale\n  Self-supervised Pre-train","summary":"  The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.\n","authors":["Haojun Jiang","Meng Li","Zhenguo Sun","Ning Jia","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19756v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.19749v1","updated":"2024-06-28T08:48:14Z","published":"2024-06-28T08:48:14Z","title":"SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction\n  Network for Vessel Segmentation","summary":"  Automatic vessel segmentation is paramount for developing next-generation\ninterventional navigation systems. However, current approaches suffer from\nsuboptimal segmentation performances due to significant challenges in\nintraoperative images (i.e., low signal-to-noise ratio, small or slender\nvessels, and strong interference). In this paper, a novel spatial-frequency\nlearning and topological channel interaction network (SPIRONet) is proposed to\naddress the above issues. Specifically, dual encoders are utilized to\ncomprehensively capture local spatial and global frequency vessel features.\nThen, a cross-attention fusion module is introduced to effectively fuse spatial\nand frequency features, thereby enhancing feature discriminability.\nFurthermore, a topological channel interaction module is designed to filter out\ntask-irrelevant responses based on graph neural networks. Extensive\nexperimental results on several challenging datasets (CADSA, CAXF, DCA1, and\nXCAD) demonstrate state-of-the-art performances of our method. Moreover, the\ninference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing\nclinical real-time requirements (6~12FPS). These promising outcomes indicate\nSPIRONet's potential for integration into vascular interventional navigation\nsystems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Zhen-Qiu Feng","Mei-Jiang Gui","Hao Li","Tian-Yu Xiang","Bo-Xian Yao","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2406.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2406.19726v1","updated":"2024-06-28T08:16:54Z","published":"2024-06-28T08:16:54Z","title":"EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans","summary":"  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n","authors":["Nicola Garau","Giulia Martinelli","Niccolò Bisagno","Denis Tomè","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2406.19726v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.01469v2","updated":"2024-06-28T08:13:48Z","published":"2023-09-04T09:26:04Z","title":"Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework","summary":"  Fibre ropes with the latest technology have emerged as an appealing\nalternative to steel ropes for offshore industries due to their lightweight and\nhigh tensile strength. At the same time, frequent inspection of these ropes is\nessential to ensure the proper functioning and safety of the entire system. The\ndevelopment of deep learning (DL) models in condition monitoring (CM)\napplications offers a simpler and more effective approach for defect detection\nin synthetic fibre ropes (SFRs). The present paper investigates the performance\nof Detectron2, a state-of-the-art library for defect detection and instance\nsegmentation. Detectron2 with Mask R-CNN architecture is used for segmenting\ndefects in SFRs. Mask R-CNN with various backbone configurations has been\ntrained and tested on an experimentally obtained dataset comprising 1,803\nhigh-dimensional images containing seven damage classes (placking high,\nplacking medium, placking low, compression, core out, chafing, and normal\nrespectively) for SFRs. By leveraging the capabilities of Detectron2, this\nstudy aims to develop an automated and efficient method for detecting defects\nin SFRs, enhancing the inspection process, and ensuring the safety of the fibre\nropes.\n","authors":["Anju Rani","Daniel O. Arroyo","Petar Durdevic"],"pdf_url":"https://arxiv.org/pdf/2309.01469v2.pdf","comment":"12 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.18584v2","updated":"2024-06-28T07:34:25Z","published":"2024-06-06T06:22:06Z","title":"Assessment of Sentinel-2 spatial and temporal coverage based on the\n  scene classification layer","summary":"  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used\nthe data for diverse applications. The scene classification layer (SCL) inside\nthe S2 product provides rich information for training, such as filtering images\nwith high cloud coverage. However, there is more potential in this. We propose\na technique to assess the clean optical coverage of a region, expressed by a\nSITS and calculated with the S2-based SCL data. With a manual threshold and\nspecific labels in the SCL, the proposed technique assigns a percentage of\nspatial and temporal coverage across the time series and a high/low assessment.\nBy evaluating the AI4EO challenge for Enhanced Agriculture, we show that the\nassessment is correlated to the predictive results of ML models. The\nclassification results in a region with low spatial and temporal coverage is\nworse than in a region with high coverage. Finally, we applied the technique\nacross all continents of the global dataset LandCoverNet.\n","authors":["Cristhian Sanchez","Francisco Mena","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2406.18584v2.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2406.19703v1","updated":"2024-06-28T07:28:50Z","published":"2024-06-28T07:28:50Z","title":"Vision Transformer with Key-select Routing Attention for Single Image\n  Dehazing","summary":"  We present Ksformer, utilizing Multi-scale Key-select Routing Attention\n(MKRA) for intelligent selection of key areas through multi-channel,\nmulti-scale windows with a top-k operator, and Lightweight Frequency Processing\nModule (LFPM) to enhance high-frequency features, outperforming other dehazing\nmethods in tests.\n","authors":["Lihan Tong","Weijia Li","Qingxia Yang","Liyuan Chen","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19703v1.pdf","comment":"5 pages,4 figures,IEICE Trans. Information and Systems"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2406.19693v1","updated":"2024-06-28T07:09:06Z","published":"2024-06-28T07:09:06Z","title":"MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?","summary":"  It is fundamentally challenging for robots to serve as useful assistants in\nhuman environments because this requires addressing a spectrum of sub-problems\nacross robotics, including perception, language understanding, reasoning, and\nplanning. The recent advancements in Multimodal Large Language Models (MLLMs)\nhave demonstrated their exceptional abilities in solving complex mathematical\nproblems, mastering commonsense and abstract reasoning. This has led to the\nrecent utilization of MLLMs as the brain in robotic systems, enabling these\nmodels to conduct high-level planning prior to triggering low-level control\nactions for task execution. However, it remains uncertain whether existing\nMLLMs are reliable in serving the brain role of robots. In this study, we\nintroduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo)\nbenchmark, which tests the capability of MLLMs for robot applications.\nSpecifically, we identify four essential capabilities perception, task\nplanning, visual reasoning, and safety measurement that MLLMs must possess to\nqualify as the robot's central processing unit. We have developed several\nscenarios for each capability, resulting in a total of 14 metrics for\nevaluation. We present experimental results for various MLLMs, including both\ncommercial and open-source models, to assess the performance of existing\nsystems. Our findings indicate that no single model excels in all areas,\nsuggesting that current MLLMs are not yet trustworthy enough to serve as the\ncognitive core for robots. Our data can be found in\nhttps://mm-robobench.github.io/.\n","authors":["Jinming Li","Yichen Zhu","Zhiyuan Xu","Jindong Gu","Minjie Zhu","Xin Liu","Ning Liu","Yaxin Peng","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2406.19693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19690v1","updated":"2024-06-28T07:06:02Z","published":"2024-06-28T07:06:02Z","title":"Deep Fusion Model for Brain Tumor Classification Using Fine-Grained\n  Gradient Preservation","summary":"  Brain tumors are one of the most common diseases that lead to early death if\nnot diagnosed at an early stage. Traditional diagnostic approaches are\nextremely time-consuming and prone to errors. In this context, computer\nvision-based approaches have emerged as an effective tool for accurate brain\ntumor classification. While some of the existing solutions demonstrate\nnoteworthy accuracy, the models become infeasible to deploy in areas where\ncomputational resources are limited. This research addresses the need for\naccurate and fast classification of brain tumors with a priority of deploying\nthe model in technologically underdeveloped regions. The research presents a\nnovel architecture for precise brain tumor classification fusing pretrained\nResNet152V2 and modified VGG16 models. The proposed architecture undergoes a\ndiligent fine-tuning process that ensures fine gradients are preserved in deep\nneural networks, which are essential for effective brain tumor classification.\nThe proposed solution incorporates various image processing techniques to\nimprove image quality and achieves an astounding accuracy of 98.36% and 98.04%\nin Figshare and Kaggle datasets respectively. This architecture stands out for\nhaving a streamlined profile, with only 2.8 million trainable parameters. We\nhave leveraged 8-bit quantization to produce a model of size 73.881 MB,\nsignificantly reducing it from the previous size of 289.45 MB, ensuring smooth\ndeployment in edge devices even in resource-constrained areas. Additionally,\nthe use of Grad-CAM improves the interpretability of the model, offering\ninsightful information regarding its decision-making process. Owing to its high\ndiscriminative ability, this model can be a reliable option for accurate brain\ntumor classification.\n","authors":["Niful Islam","Mohaiminul Islam Bhuiyan","Jarin Tasnim Raya","Nur Shazwani Kamarudin","Khan Md Hasib","M. F. Mridha","Dewan Md. Farid"],"pdf_url":"https://arxiv.org/pdf/2406.19690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16462v2","updated":"2024-06-28T07:04:21Z","published":"2023-11-28T03:45:29Z","title":"Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information","summary":"  Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.\n","authors":["Jie Li","Zhixin Li","Zhi Liu","Pengyuan Zhou","Richang Hong","Qiyue Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2311.16462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.19070v2","updated":"2024-06-28T06:47:10Z","published":"2024-06-27T10:40:35Z","title":"FAGhead: Fully Animate Gaussian Head from Monocular Videos","summary":"  High-fidelity reconstruction of 3D human avatars has a wild application in\nvisual reality. In this paper, we introduce FAGhead, a method that enables\nfully controllable human portraits from monocular videos. We explicit the\ntraditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to\nreconstruct with complex expressions. Furthermore, we employ a novel\nPoint-based Learnable Representation Field (PLRF) with learnable Gaussian point\npositions to enhance reconstruction performance. Meanwhile, to effectively\nmanage the edges of avatars, we introduced the alpha rendering to supervise the\nalpha value of each pixel. Extensive experimental results on the open-source\ndatasets and our capturing datasets demonstrate that our approach is able to\ngenerate high-fidelity 3D head avatars and fully control the expression and\npose of the virtual avatars, which is outperforming than existing works.\n","authors":["Yixin Xuan","Xinyang Li","Gongxin Yao","Shiwei Zhou","Donghui Sun","Xiaoxin Chen","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2406.19070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18146v2","updated":"2024-06-28T06:43:39Z","published":"2024-06-26T07:56:17Z","title":"A Refer-and-Ground Multimodal Large Language Model for Biomedicine","summary":"  With the rapid development of multimodal large language models (MLLMs),\nespecially their capabilities in visual chat through refer and ground\nfunctionalities, their significance is increasingly recognized. However, the\nbiomedical field currently exhibits a substantial gap in this area, primarily\ndue to the absence of a dedicated refer and ground dataset for biomedical\nimages. To address this challenge, we devised the Med-GRIT-270k dataset. It\ncomprises 270k question-and-answer pairs and spans eight distinct medical\nimaging modalities. Most importantly, it is the first dedicated to the\nbiomedical domain and integrating refer and ground conversations. The key idea\nis to sample large-scale biomedical image-mask pairs from medical segmentation\ndatasets and generate instruction datasets from text using chatGPT.\nAdditionally, we introduce a Refer-and-Ground Multimodal Large Language Model\nfor Biomedicine (BiRD) by using this dataset and multi-task instruction\nlearning. Extensive experiments have corroborated the efficacy of the\nMed-GRIT-270k dataset and the multi-modal, fine-grained interactive\ncapabilities of the BiRD model. This holds significant reference value for the\nexploration and development of intelligent biomedical assistants.\n","authors":["Xiaoshuang Huang","Haifeng Huang","Lingdong Shen","Yehui Yang","Fangxin Shang","Junwei Liu","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18146v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19680v1","updated":"2024-06-28T06:40:53Z","published":"2024-06-28T06:40:53Z","title":"MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance","summary":"  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n","authors":["Yuang Zhang","Jiaxi Gu","Li-Wen Wang","Han Wang","Junqi Cheng","Yuefeng Zhu","Fangyuan Zou"],"pdf_url":"https://arxiv.org/pdf/2406.19680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19675v1","updated":"2024-06-28T06:25:21Z","published":"2024-06-28T06:25:21Z","title":"Deep Learning-based Depth Estimation Methods from Monocular Image and\n  Videos: A Comprehensive Survey","summary":"  Estimating depth from single RGB images and videos is of widespread interest\ndue to its applications in many areas, including autonomous driving, 3D\nreconstruction, digital entertainment, and robotics. More than 500 deep\nlearning-based papers have been published in the past 10 years, which indicates\nthe growing interest in the task. This paper presents a comprehensive survey of\nthe existing deep learning-based methods, the challenges they address, and how\nthey have evolved in their architecture and supervision methods. It provides a\ntaxonomy for classifying the current work based on their input and output\nmodalities, network architectures, and learning methods. It also discusses the\nmajor milestones in the history of monocular depth estimation, and different\npipelines, datasets, and evaluation metrics used in existing methods.\n","authors":["Uchitha Rajapaksha","Ferdous Sohel","Hamid Laga","Dean Diepeveen","Mohammed Bennamoun"],"pdf_url":"https://arxiv.org/pdf/2406.19675v1.pdf","comment":"46 pages, 10 figures, The paper has been accepted for publication in\n  ACM Computing Surveys 2024"},{"id":"http://arxiv.org/abs/2405.05164v2","updated":"2024-06-28T06:11:11Z","published":"2024-05-08T15:54:57Z","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion","summary":"  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n","authors":["Bing Zhu","Zixin He","Weiyi Xiong","Guanhua Ding","Jianan Liu","Tao Huang","Wei Chen","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2405.05164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19672v1","updated":"2024-06-28T06:06:01Z","published":"2024-06-28T06:06:01Z","title":"Beyond First-Order: A Multi-Scale Approach to Finger Knuckle Print\n  Biometrics","summary":"  Recently, finger knuckle prints (FKPs) have gained attention due to their\nrich textural patterns, positioning them as a promising biometric for identity\nrecognition. Prior FKP recognition methods predominantly leverage first-order\nfeature descriptors, which capture intricate texture details but fail to\naccount for structural information. Emerging research, however, indicates that\nsecond-order textures, which describe the curves and arcs of the textures,\nencompass this overlooked structural information. This paper introduces a novel\nFKP recognition approach, the Dual-Order Texture Competition Network (DOTCNet),\ndesigned to capture texture information in FKP images comprehensively. DOTCNet\nincorporates three dual-order texture competitive modules (DTCMs), each\ntargeting textures at different scales. Each DTCM employs a learnable texture\ndescriptor, specifically a learnable Gabor filter (LGF), to extract texture\nfeatures. By leveraging LGFs, the network extracts first and second order\ntextures to describe fine textures and structural features thoroughly.\nFurthermore, an attention mechanism enhances relevant features in the\nfirst-order features, thereby highlighting significant texture details. For\nsecond-order features, a competitive mechanism emphasizes structural\ninformation while reducing noise from higher-order features. Extensive\nexperimental results reveal that DOTCNet significantly outperforms several\nstandard algorithms on the publicly available PolyU-FKP dataset.\n","authors":["Chengrui Gao","Ziyuan Yang","Andrew Beng Jin Teoh","Min Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.19672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19364v2","updated":"2024-06-28T05:56:08Z","published":"2024-06-27T17:46:13Z","title":"SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues","summary":"  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.\n","authors":["Yuxin Xie","Tao Zhou","Yi Zhou","Geng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19364v2.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.18684v2","updated":"2024-06-28T05:43:41Z","published":"2024-06-26T18:42:22Z","title":"CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification","summary":"  In recent years, Joint Communication and Sensing (JC&S), has demonstrated\nsignificant success, particularly in utilizing sub-6 GHz frequencies with\ncommercial-off-the-shelf (COTS) Wi-Fi devices for applications such as\nlocalization, gesture recognition, and pose classification. Deep learning and\nthe existence of large public datasets has been pivotal in achieving such\nresults. However, at mmWave frequencies (30-300 GHz), which has shown potential\nfor more accurate sensing performance, there is a noticeable lack of research\nin the domain of COTS Wi-Fi sensing. Challenges such as limited research\nhardware, the absence of large datasets, limited functionality in COTS\nhardware, and the complexities of data collection present obstacles to a\ncomprehensive exploration of this field. In this work, we aim to address these\nchallenges by developing a method that can generate synthetic mmWave channel\nstate information (CSI) samples. In particular, we use a generative adversarial\nnetwork (GAN) on an existing dataset, to generate 30,000 additional CSI\nsamples. The augmented samples exhibit a remarkable degree of consistency with\nthe original data, as indicated by the notably high GAN-train and GAN-test\nscores. Furthermore, we integrate the augmented samples in training a pose\nclassification model. We observe that the augmented samples complement the real\ndata and improve the generalization of the classification model.\n","authors":["Nabeel Nisar Bhat","Rafael Berkvens","Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2406.18684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19668v1","updated":"2024-06-28T05:38:32Z","published":"2024-06-28T05:38:32Z","title":"PopAlign: Population-Level Alignment for Fair Text-to-Image Generation","summary":"  Text-to-image (T2I) models achieve high-fidelity generation through extensive\ntraining on large datasets. However, these models may unintentionally pick up\nundesirable biases of their training data, such as over-representation of\nparticular identities in gender or ethnicity neutral prompts. Existing\nalignment methods such as Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) fail to address this problem effectively\nbecause they operate on pairwise preferences consisting of individual samples,\nwhile the aforementioned biases can only be measured at a population level. For\nexample, a single sample for the prompt \"doctor\" could be male or female, but a\nmodel generating predominantly male doctors even with repeated sampling\nreflects a gender bias. To address this limitation, we introduce PopAlign, a\nnovel approach for population-level preference optimization, while standard\noptimization would prefer entire sets of samples over others. We further derive\na stochastic lower bound that directly optimizes for individual samples from\npreferred populations over others for scalable training. Using human evaluation\nand standard image quality and bias metrics, we show that PopAlign\nsignificantly mitigates the bias of pretrained T2I models while largely\npreserving the generation quality. Code is available at\nhttps://github.com/jacklishufan/PopAlignSDXL.\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.19668v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19666v1","updated":"2024-06-28T05:25:57Z","published":"2024-06-28T05:25:57Z","title":"CSAKD: Knowledge Distillation with Cross Self-Attention for\n  Hyperspectral and Multispectral Image Fusion","summary":"  Hyperspectral imaging, capturing detailed spectral information for each\npixel, is pivotal in diverse scientific and industrial applications. Yet, the\nacquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to\nbe addressed due to the hardware limitations of existing imaging systems. A\nprevalent workaround involves capturing both a high-resolution multispectral\nimage (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield\nthe desired HR-HSI. Although deep learning-based methods have shown promising\nin HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial\nmodel complexities hinder deployment on resource-constrained imaging devices.\nThis paper introduces a novel knowledge distillation (KD) framework for\nHR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the\nproposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency\nfor constructing Dual Two-Streamed (DTS) network structure, designed to extract\njoint and distinct features from LR-HSI and HR-MSI simultaneously. To fully\nexploit the spatial and spectral feature representations of LR-HSI and HR-MSI,\nwe propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse\nthose features to improve the spatial and spectral quality of the reconstructed\nHR-HSI. Finally, the proposed KD-based joint loss function is employed to\nco-train the teacher and student networks. Our experimental results demonstrate\nthat the student model not only achieves comparable or superior LR-HSI SR\nperformance but also significantly reduces the model-size and computational\nrequirements. This marks a substantial advancement over existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/ming053l/CSAKD.\n","authors":["Chih-Chung Hsu","Chih-Chien Ni","Chia-Ming Lee","Li-Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19666v1.pdf","comment":"Submitted to TIP 2024"},{"id":"http://arxiv.org/abs/2405.19769v2","updated":"2024-06-28T05:25:19Z","published":"2024-05-30T07:34:05Z","title":"All-In-One Medical Image Restoration via Task-Adaptive Routing","summary":"  Although single-task medical image restoration (MedIR) has witnessed\nremarkable success, the limited generalizability of these methods poses a\nsubstantial obstacle to wider application. In this paper, we focus on the task\nof all-in-one medical image restoration, aiming to address multiple distinct\nMedIR tasks with a single universal model. Nonetheless, due to significant\ndifferences between different MedIR tasks, training a universal model often\nencounters task interference issues, where different tasks with shared\nparameters may conflict with each other in the gradient update direction. This\ntask interference leads to deviation of the model update direction from the\noptimal path, thereby affecting the model's performance. To tackle this issue,\nwe propose a task-adaptive routing strategy, allowing conflicting tasks to\nselect different network paths in spatial and channel dimensions, thereby\nmitigating task interference. Experimental results demonstrate that our\nproposed \\textbf{A}ll-in-one \\textbf{M}edical \\textbf{I}mage\n\\textbf{R}estoration (\\textbf{AMIR}) network achieves state-of-the-art\nperformance in three MedIR tasks: MRI super-resolution, CT denoising, and PET\nsynthesis, both in single-task and all-in-one settings. The code and data will\nbe available at\n\\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.\n","authors":["Zhiwen Yang","Haowei Chen","Ziniu Qian","Yang Yi","Hui Zhang","Dan Zhao","Bingzheng Wei","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2405.19769v2.pdf","comment":"This article has been early accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19665v1","updated":"2024-06-28T05:22:39Z","published":"2024-06-28T05:22:39Z","title":"PM-VIS+: High-Performance Video Instance Segmentation without Video\n  Annotation","summary":"  Video instance segmentation requires detecting, segmenting, and tracking\nobjects in videos, typically relying on costly video annotations. This paper\nintroduces a method that eliminates video annotations by utilizing image\ndatasets. The PM-VIS algorithm is adapted to handle both bounding box and\ninstance-level pixel annotations dynamically. We introduce ImageNet-bbox to\nsupplement missing categories in video datasets and propose the PM-VIS+\nalgorithm to adjust supervision based on annotation types. To enhance accuracy,\nwe use pseudo masks and semi-supervised optimization techniques on unannotated\nvideo data. This method achieves high video instance segmentation performance\nwithout manual video annotations, offering a cost-effective solution and new\nperspectives for video instance segmentation applications. The code will be\navailable in https://github.com/ldknight/PM-VIS-plus\n","authors":["Zhangjing Yang","Dun Liu","Xin Wang","Zhe Li","Barathwaj Anandan","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2406.19665v1.pdf","comment":"MIPR 2024"},{"id":"http://arxiv.org/abs/2406.18844v2","updated":"2024-06-28T05:21:13Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but raises\nsecurity risks through potential backdoor attacks due to their openness.\nPrevious backdoor studies focus on enclosed scenarios with consistent training\nand testing instructions, neglecting the practical domain gaps that could\naffect attack effectiveness. This paper empirically examines the\ngeneralizability of backdoor attacks during the instruction tuning of LVLMs for\nthe first time, revealing certain limitations of most backdoor strategies in\npractical scenarios. We quantitatively evaluate the generalizability of six\ntypical backdoor attacks on image caption benchmarks across multiple LVLMs,\nconsidering both visual and textual domain offsets. Our findings indicate that\nattack generalizability is positively correlated with the backdoor trigger's\nirrelevance to specific images/models and the preferential correlation of the\ntrigger pattern. Additionally, we modify existing backdoor attacks based on the\nabove key observations, demonstrating significant improvements in cross-domain\nscenario generalizability (+86% attack success rate). Notably, even without\naccess to the instruction datasets, a multimodal instruction set can be\nsuccessfully poisoned with a very low poisoning rate (0.2%), achieving an\nattack success rate of over 97%. This paper underscores that even simple\ntraditional backdoor strategies pose a serious threat to LVLMs, necessitating\nmore attention and in-depth research.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Ee-Chien Chang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.19655v1","updated":"2024-06-28T04:49:57Z","published":"2024-06-28T04:49:57Z","title":"Basketball-SORT: An Association Method for Complex Multi-object\n  Occlusion Problems in Basketball Multi-object Tracking","summary":"  Recent deep learning-based object detection approaches have led to\nsignificant progress in multi-object tracking (MOT) algorithms. The current MOT\nmethods mainly focus on pedestrian or vehicle scenes, but basketball sports\nscenes are usually accompanied by three or more object occlusion problems with\nsimilar appearances and high-intensity complex motions, which we call complex\nmulti-object occlusion (CMOO). Here, we propose an online and robust MOT\napproach, named Basketball-SORT, which focuses on the CMOO problems in\nbasketball videos. To overcome the CMOO problem, instead of using the\nintersection-over-union-based (IoU-based) approach, we use the trajectories of\nneighboring frames based on the projected positions of the players. Our method\ndesigns the basketball game restriction (BGR) and reacquiring Long-Lost IDs\n(RLLI) based on the characteristics of basketball scenes, and we also solve the\nocclusion problem based on the player trajectories and appearance features.\nExperimental results show that our method achieves a Higher Order Tracking\nAccuracy (HOTA) score of 63.48$\\%$ on the basketball fixed video dataset and\noutperforms other recent popular approaches. Overall, our approach solved the\nCMOO problem more effectively than recent MOT algorithms.\n","authors":["Qingrui Hu","Atom Scott","Calvin Yeung","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2406.19655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19649v1","updated":"2024-06-28T04:38:12Z","published":"2024-06-28T04:38:12Z","title":"AstMatch: Adversarial Self-training Consistency Framework for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) has shown considerable potential in medical\nimage segmentation, primarily leveraging consistency regularization and\npseudo-labeling. However, many SSL approaches only pay attention to low-level\nconsistency and overlook the significance of pseudo-label reliability.\nTherefore, in this work, we propose an adversarial self-training consistency\nframework (AstMatch). Firstly, we design an adversarial consistency\nregularization (ACR) approach to enhance knowledge transfer and strengthen\nprediction consistency under varying perturbation intensities. Second, we apply\na feature matching loss for adversarial training to incorporate high-level\nconsistency regularization. Additionally, we present the pyramid channel\nattention (PCA) and efficient channel and spatial attention (ECSA) modules to\nimprove the discriminator's performance. Finally, we propose an adaptive\nself-training (AST) approach to ensure the pseudo-labels' quality. The proposed\nAstMatch has been extensively evaluated with cutting-edge SSL methods on three\npublic-available datasets. The experimental results under different labeled\nratios indicate that AstMatch outperforms other existing methods, achieving new\nstate-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/AstMatch.\n","authors":["Guanghao Zhu","Jing Zhang","Juanxiu Liu","Xiaohui Du","Ruqian Hao","Yong Liu","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19640v1","updated":"2024-06-28T04:10:21Z","published":"2024-06-28T04:10:21Z","title":"Efficient Event Stream Super-Resolution with Recursive Multi-Branch\n  Fusion","summary":"  Current Event Stream Super-Resolution (ESR) methods overlook the redundant\nand complementary information present in positive and negative events within\nthe event stream, employing a direct mixing approach for super-resolution,\nwhich may lead to detail loss and inefficiency. To address these issues, we\npropose an efficient Recursive Multi-Branch Information Fusion Network (RMFNet)\nthat separates positive and negative events for complementary information\nextraction, followed by mutual supplementation and refinement. Particularly, we\nintroduce Feature Fusion Modules (FFM) and Feature Exchange Modules (FEM). FFM\nis designed for the fusion of contextual information within neighboring event\nstreams, leveraging the coupling relationship between positive and negative\nevents to alleviate the misleading of noises in the respective branches. FEM\nefficiently promotes the fusion and exchange of information between positive\nand negative branches, enabling superior local information enhancement and\nglobal information complementation. Experimental results demonstrate that our\napproach achieves over 17% and 31% improvement on synthetic and real datasets,\naccompanied by a 2.3X acceleration. Furthermore, we evaluate our method on two\ndownstream event-driven applications, \\emph{i.e.}, object recognition and video\nreconstruction, achieving remarkable results that outperform existing methods.\nOur code and Supplementary Material are available at\nhttps://github.com/Lqm26/RMFNet.\n","authors":["Quanmin Liang","Zhilin Huang","Xiawu Zheng","Feidiao Yang","Jun Peng","Kai Huang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2406.19640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19638v1","updated":"2024-06-28T03:58:02Z","published":"2024-06-28T03:58:02Z","title":"Precision matters: Precision-aware ensemble for weakly supervised\n  semantic segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such\nas image-level labels, to train the segmentation model. Despite the impressive\nachievement in recent WSSS methods, we identify that introducing weak labels\nwith high mean Intersection of Union (mIoU) does not guarantee high\nsegmentation performance. Existing studies have emphasized the importance of\nprioritizing precision and reducing noise to improve overall performance. In\nthe same vein, we propose ORANDNet, an advanced ensemble approach tailored for\nWSSS. ORANDNet combines Class Activation Maps (CAMs) from two different\nclassifiers to increase the precision of pseudo-masks (PMs). To further\nmitigate small noise in the PMs, we incorporate curriculum learning. This\ninvolves training the segmentation model initially with pairs of smaller-sized\nimages and corresponding PMs, gradually transitioning to the original-sized\npairs. By combining the original CAMs of ResNet-50 and ViT, we significantly\nimprove the segmentation performance over the single-best model and the naive\nensemble model, respectively. We further extend our ensemble method to CAMs\nfrom AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance\nbenefits in advanced WSSS models. It highlights the potential of our ORANDNet\nas a final add-on module for WSSS models.\n","authors":["Junsung Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2406.19638v1.pdf","comment":"5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop"},{"id":"http://arxiv.org/abs/2310.01712v2","updated":"2024-06-28T03:53:56Z","published":"2023-10-03T00:54:13Z","title":"Generative Autoencoding of Dropout Patterns","summary":"  We propose a generative model termed Deciphering Autoencoders. In this model,\nwe assign a unique random dropout pattern to each data point in the training\ndataset and then train an autoencoder to reconstruct the corresponding data\npoint using this pattern as information to be encoded. Even if a completely\nrandom dropout pattern is assigned to each data point regardless of their\nsimilarities, a sufficiently large encoder can smoothly map them to a\nlow-dimensional latent space to reconstruct individual training data points.\nDuring inference, using a dropout pattern different from those used during\ntraining allows the model to function as a generator. Since the training of\nDeciphering Autoencoders relies solely on reconstruction error, it offers more\nstable training compared to other generative models. Despite their simplicity,\nDeciphering Autoencoders show sampling quality comparable to DCGAN on the\nCIFAR-10 dataset.\n","authors":["Shunta Maeda"],"pdf_url":"https://arxiv.org/pdf/2310.01712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18070v3","updated":"2024-06-28T03:50:19Z","published":"2024-06-26T05:01:37Z","title":"EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation","summary":"  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n","authors":["Baoqi Pei","Guo Chen","Jilan Xu","Yuping He","Yicheng Liu","Kanghua Pan","Yifei Huang","Yali Wang","Tong Lu","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2406.18070v3.pdf","comment":"Champion solutions in the EgoVis CVPR 2024 workshop"},{"id":"http://arxiv.org/abs/2406.19635v1","updated":"2024-06-28T03:46:53Z","published":"2024-06-28T03:46:53Z","title":"Model Predictive Simulation Using Structured Graphical Models and\n  Transformers","summary":"  We propose an approach to simulating trajectories of multiple interacting\nagents (road users) based on transformers and probabilistic graphical models\n(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline\nis based on the MTR model, which predicts multiple future trajectories\nconditioned on the past trajectories and static road layout features. We then\nimprove upon these generated trajectories using a PGM, which contains factors\nwhich encode prior knowledge, such as a preference for smooth trajectories, and\navoidance of collisions with static obstacles and other moving agents. We\nperform (approximate) MAP inference in this PGM using the Gauss-Newton method.\nFinally we sample $K=32$ trajectories for each of the $N \\sim 100$ agents for\nthe next $T=8 \\Delta$ time steps, where $\\Delta=10$ is the sampling rate per\nsecond. Following the Model Predictive Control (MPC) paradigm, we only return\nthe first element of our forecasted trajectories at each step, and then we\nreplan, so that the simulation can constantly adapt to its changing\nenvironment. We therefore call our approach \"Model Predictive Simulation\" or\nMPS. We show that MPS improves upon the MTR baseline, especially in safety\ncritical metrics such as collision rate. Furthermore, our approach is\ncompatible with any underlying forecasting model, and does not require extra\ntraining, so we believe it is a valuable contribution to the community.\n","authors":["Xinghua Lou","Meet Dave","Shrinu Kushagra","Miguel Lazaro-Gredilla","Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2406.19635v1.pdf","comment":"Special Mention at the Waymo Sim Agents Challenge 2024"},{"id":"http://arxiv.org/abs/2406.19632v1","updated":"2024-06-28T03:43:49Z","published":"2024-06-28T03:43:49Z","title":"PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation","summary":"  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields\nnecessitates effective UAV image segmentation, which faces challenges due to\nthe dynamic perspectives of UAV-captured images. Traditional segmentation\nalgorithms falter as they cannot accurately mimic the complexity of UAV\nperspectives, and the cost of obtaining multi-perspective labeled datasets is\nprohibitive. To address these issues, we introduce the PPTFormer, a novel\n\\textbf{P}seudo Multi-\\textbf{P}erspective \\textbf{T}rans\\textbf{former}\nnetwork that revolutionizes UAV image segmentation. Our approach circumvents\nthe need for actual multi-perspective data by creating pseudo perspectives for\nenhanced multi-perspective learning. The PPTFormer network boasts Perspective\nDecomposition, novel Perspective Prototypes, and a specialized encoder and\ndecoder that together achieve superior segmentation results through Pseudo\nMulti-Perspective Attention (PMP Attention) and fusion. Our experiments\ndemonstrate that PPTFormer achieves state-of-the-art performance across five\nUAV segmentation datasets, confirming its capability to effectively simulate\nUAV flight perspectives and significantly advance segmentation precision. This\nwork presents a pioneering leap in UAV scene understanding and sets a new\nbenchmark for future developments in semantic segmentation.\n","authors":["Deyi Ji","Wenwei Jin","Hongtao Lu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.19632v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.19630v1","updated":"2024-06-28T03:36:38Z","published":"2024-06-28T03:36:38Z","title":"Optimal Video Compression using Pixel Shift Tracking","summary":"  The Video comprises approximately ~85\\% of all internet traffic, but video\nencoding/compression is being historically done with hard coded rules, which\nhas worked well but only to a certain limit. We have seen a surge in video\ncompression algorithms using ML-based models in the last few years and many of\nthem have outperformed several legacy codecs. The models range from encoding\nvideo end to end using an ML approach or replacing some intermediate steps in\nlegacy codecs using ML models to increase the efficiency of those steps.\n  Optimizing video storage is an essential aspect of video processing, so we\nare proposing one of the possible approaches to achieve it is by avoiding\nredundant data at each frame. In this paper, we want to introduce the approach\nof redundancies removal in subsequent frames for a given video as a main\napproach for video compression. We call this method Redundancy Removal using\nShift (R\\textsuperscript2S). This method can be utilized across various Machine\nLearning model algorithms, and make the compression more accessible and\nadaptable. In this study, we have utilized a computer vision-based pixel point\ntracking method to identify redundant pixels to encode video for optimal\nstorage.\n","authors":["Hitesh Saai Mananchery Panneerselvam","Smit Anand"],"pdf_url":"https://arxiv.org/pdf/2406.19630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18893v2","updated":"2024-06-28T03:22:33Z","published":"2024-06-27T05:08:46Z","title":"AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image\n  Models","summary":"  We consider the problem of customizing text-to-image diffusion models with\nuser-supplied reference images. Given new prompts, the existing methods can\ncapture the key concept from the reference images but fail to align the\ngenerated image with the prompt. In this work, we seek to address this key\nissue by proposing new methods that can easily be used in conjunction with\nexisting customization methods that optimize the embeddings/weights at various\nintermediate stages of the text encoding process.\n  The first contribution of this paper is a dissection of the various stages of\nthe text encoding process leading up to the conditioning vector for\ntext-to-image models. We take a holistic view of existing customization methods\nand notice that key and value outputs from this process differs substantially\nfrom their corresponding baseline (non-customized) models (e.g., baseline\nstable diffusion). While this difference does not impact the concept being\ncustomized, it leads to other parts of the generated image not being aligned\nwith the prompt. Further, we also observe that these keys and values allow\nindependent control various aspects of the final generation, enabling semantic\nmanipulation of the output. Taken together, the features spanning these keys\nand values, serve as the basis for our next contribution where we fix the\naforementioned issues with existing methods. We propose a new post-processing\nalgorithm, AlignIT, that infuses the keys and values for the concept of\ninterest while ensuring the keys and values for all other tokens in the input\nprompt are unchanged.\n  Our proposed method can be plugged in directly to existing customization\nmethods, leading to a substantial performance improvement in the alignment of\nthe final result with the input prompt while retaining the customization\nquality.\n","authors":["Aishwarya Agarwal","Srikrishna Karanam","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.18893v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16537v2","updated":"2024-06-28T03:21:15Z","published":"2024-06-24T11:16:37Z","title":"Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization","summary":"  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods. Our code will be\nreleased at https://github.com/Character-Adapter/Character-Adapte\n","authors":["Yuhang Ma","Wenting Xu","Jiji Tang","Qinfeng Jin","Rongsheng Zhang","Zeng Zhao","Changjie Fan","Zhipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06777v3","updated":"2024-06-28T03:07:29Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension","summary":"  Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18958v2","updated":"2024-06-28T02:47:07Z","published":"2024-06-27T07:40:59Z","title":"AnyControl: Create Your Artwork with Versatile Control on Text-to-Image\n  Generation","summary":"  The field of text-to-image (T2I) generation has made significant progress in\nrecent years, largely driven by advancements in diffusion models. Linguistic\ncontrol enables effective content creation, but struggles with fine-grained\ncontrol over image generation. This challenge has been explored, to a great\nextent, by incorporating additional user-supplied spatial conditions, such as\ndepth maps and edge maps, into pre-trained T2I models through extra encoding.\nHowever, multi-control image synthesis still faces several challenges.\nSpecifically, current approaches are limited in handling free combinations of\ndiverse input control signals, overlook the complex relationships among\nmultiple spatial conditions, and often fail to maintain semantic alignment with\nprovided textual prompts. This can lead to suboptimal user experiences. To\naddress these challenges, we propose AnyControl, a multi-control image\nsynthesis framework that supports arbitrary combinations of diverse control\nsignals. AnyControl develops a novel Multi-Control Encoder that extracts a\nunified multi-modal embedding to guide the generation process. This approach\nenables a holistic understanding of user inputs, and produces high-quality,\nfaithful results under versatile control signals, as demonstrated by extensive\nquantitative and qualitative evaluations. Our project page is available in\nhttps://any-control.github.io.\n","authors":["Yanan Sun","Yanchen Liu","Yinhao Tang","Wenjie Pei","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19602v1","updated":"2024-06-28T02:18:16Z","published":"2024-06-28T02:18:16Z","title":"A Survey on Deep Clustering: From the Prior Perspective","summary":"  Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.\n","authors":["Yiding Lu","Haobin Li","Yunfan Li","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2406.19602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18915v2","updated":"2024-06-28T02:13:22Z","published":"2024-06-27T06:12:01Z","title":"Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models","summary":"  Large-scale endeavors like RT-1 and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 5 real-world and 12 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser and\nCode-As-Policies. We believe Manipulate-Anything can be the scalable method for\nboth generating data for robotics and solving novel tasks in a zero-shot\nsetting.\n","authors":["Jiafei Duan","Wentao Yuan","Wilbert Pumacay","Yi Ru Wang","Kiana Ehsani","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.18915v2.pdf","comment":"Project page: https://robot-ma.github.io/"},{"id":"http://arxiv.org/abs/2406.14534v2","updated":"2024-06-28T02:12:20Z","published":"2024-06-20T17:47:30Z","title":"Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration","summary":"  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n","authors":["Long Lei","Jun Zhou","Jialun Pei","Baoliang Zhao","Yueming Jin","Yuen-Chun Jeremy Teoh","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.14534v2.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.11445v2","updated":"2024-06-28T01:24:45Z","published":"2024-06-17T11:57:14Z","title":"Solving the Inverse Problem of Electrocardiography for Cardiac Digital\n  Twins: A Survey","summary":"  Cardiac digital twins are personalized virtual representations used to\nunderstand complex heart mechanisms. Solving the ECG inverse problem is crucial\nfor accurate virtual heart modelling, enabling the derivation of internal\nelectrical activity information from recorded surface potentials. Despite\nchallenges from cardiac complexity, noisy ECG data, and computational\nefficiency, recent advancements hold significant promise for enhancing virtual\nheart modelling, ultimately advancing precision medicine in cardiology. This\npaper aims to provide a comprehensive review of the methods of solving ECG\ninverse problem, the validation strategies, the clinical applications, and\nfuture perspectives. For the computing methodologies, we broadly classify\nstate-of-the-art approaches into two categories: deterministic and\nprobabilistic methods, including conventional and deep learning-based\ntechniques. Integrating physics laws with deep learning models holds promise,\nbut challenges such as capturing dynamic electrophysiology accurately,\naccessing accurate domain knowledge, and quantifying prediction uncertainty\npersist. Integrating models into clinical workflows while ensuring\ninterpretability and usability for healthcare professionals is essential.\nOvercoming these challenges will drive further research in cardiac digital\ntwins.\n","authors":["Lei Li","Julia Camps","Blanca Rodriguez","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2406.11445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04940v2","updated":"2024-06-28T01:23:10Z","published":"2024-05-08T10:15:04Z","title":"Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID","summary":"  Text-to-image person re-identification (ReID) retrieves pedestrian images\naccording to textual descriptions. Manually annotating textual descriptions is\ntime-consuming, restricting the scale of existing datasets and therefore the\ngeneralization ability of ReID models. As a result, we study the transferable\ntext-to-image ReID problem, where we train a model on our proposed large-scale\ndatabase and directly deploy it to various datasets for evaluation. We obtain\nsubstantial training data via Multi-modal Large Language Models (MLLMs).\nMoreover, we identify and address two key challenges in utilizing the obtained\ntextual descriptions. First, an MLLM tends to generate descriptions with\nsimilar structures, causing the model to overfit specific sentence patterns.\nThus, we propose a novel method that uses MLLMs to caption images according to\nvarious templates. These templates are obtained using a multi-turn dialogue\nwith a Large Language Model (LLM). Therefore, we can build a large-scale\ndataset with diverse textual descriptions. Second, an MLLM may produce\nincorrect descriptions. Hence, we introduce a novel method that automatically\nidentifies words in a description that do not correspond with the image. This\nmethod is based on the similarity between one text and all patch token\nembeddings in the image. Then, we mask these words with a larger probability in\nthe subsequent training epoch, alleviating the impact of noisy textual\ndescriptions. The experimental results demonstrate that our methods\nsignificantly boost the direct transfer text-to-image ReID performance.\nBenefiting from the pre-trained model weights, we also achieve state-of-the-art\nperformance in the traditional evaluation settings.\n","authors":["Wentao Tan"],"pdf_url":"https://arxiv.org/pdf/2405.04940v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19593v1","updated":"2024-06-28T01:14:43Z","published":"2024-06-28T01:14:43Z","title":"SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs","summary":"  Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.\n","authors":["Xin Su","Man Luo","Kris W Pan","Tien Pei Chou","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2406.19593v1.pdf","comment":null}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2406.20087v1","updated":"2024-06-28T17:55:24Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19987v1","updated":"2024-06-28T15:18:40Z","published":"2024-06-28T15:18:40Z","title":"Concept Lens: Visually Analyzing the Consistency of Semantic\n  Manipulation in GANs","summary":"  As applications of generative AI become mainstream, it is important to\nunderstand what generative models are capable of producing, and the extent to\nwhich one can predictably control their outputs. In this paper, we propose a\nvisualization design, named Concept Lens, for jointly navigating the data\ndistribution of a generative model, and concept manipulations supported by the\nmodel. Our work is focused on modern vision-based generative adversarial\nnetworks (GAN), and their learned latent spaces, wherein concept discovery has\ngained significant interest as a means of image manipulation. Concept Lens is\ndesigned to support users in understanding the diversity of a provided set of\nconcepts, the relationship between concepts, and the suitability of concepts to\ngive semantic controls for image generation. Key to our approach is the\nhierarchical grouping of concepts, generated images, and the associated joint\nexploration. We show how Concept Lens can reveal consistent semantic\nmanipulations for editing images, while also serving as a diagnostic tool for\nstudying the limitations and trade-offs of concept discovery methods.\n","authors":["Sangwon Jeong","Mingwei Li","Matthew Berger","Shusen Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19954v1","updated":"2024-06-28T14:40:03Z","published":"2024-06-28T14:40:03Z","title":"BESTOW: Efficient and Streamable Speech Language Model with the Best of\n  Two Worlds in GPT and T5","summary":"  Incorporating speech understanding capabilities into pretrained\nlarge-language models has become a vital research direction (SpeechLLM). The\nprevious architectures can be categorized as: i) GPT-style, prepend speech\nprompts to the text prompts as a sequence of LLM inputs like a decoder-only\nmodel; ii) T5-style, introduce speech cross-attention to each layer of the\npretrained LLMs. We propose BESTOW architecture to bring the BESt features from\nTwO Worlds into a single model that is highly efficient and has strong\nmultitask capabilities. Moreover, there is no clear streaming solution for\neither style, especially considering the solution should generalize to speech\nmultitask. We reformulate streamable SpeechLLM as a read-write policy problem\nand unifies the offline and streaming research with BESTOW architecture. Hence\nwe demonstrate the first open-source SpeechLLM solution that enables Streaming\nand Multitask at scale (beyond ASR) at the same time. This streamable solution\nachieves very strong performance on a wide range of speech tasks (ASR, AST,\nSQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower\ntraining/inference cost, and demonstrates LLM knowledge transferability to\nspeech.\n","authors":["Zhehuai Chen","He Huang","Oleksii Hrinchuk","Krishna C. Puvvada","Nithin Rao Koluguri","Piotr Żelasko","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19928v1","updated":"2024-06-28T13:57:27Z","published":"2024-06-28T13:57:27Z","title":"Interactive Topic Models with Optimal Transport","summary":"  Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.\n","authors":["Garima Dhanania","Sheshera Mysore","Chau Minh Pham","Mohit Iyyer","Hamed Zamani","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2406.19928v1.pdf","comment":"Pre-print; Work in progress"},{"id":"http://arxiv.org/abs/2406.19895v1","updated":"2024-06-28T13:03:55Z","published":"2024-06-28T13:03:55Z","title":"The Relationship Between Time and Distance Perception in Egocentric and\n  Discrete Virtual Locomotion (Teleportation)","summary":"  Traveling distances in the real world inherently involves time, as moving to\na desired location is a continuous process. This temporal component plays a\nrole when estimating the distance covered. However, in virtual environments,\nthis relationship is often changed or absent. Common teleportation techniques\nenable instantaneous transitions, lacking any temporal element that might aid\nin distance perception. Since distances are found to be commonly underestimated\nin virtual environments, we investigate the influence of time on this\nmisperception, specifically in target-selection-based teleportation interfaces.\nOur first experiment explores how introducing a delay proportional to the\ndistance covered by teleportation affects participants' perception of\ndistances, focusing on underestimation, accuracy, and precision. Participants\nare required to teleport along a predefined path with varying delays. A second\nexperiment is designed to determine whether this effect manifests in a more\napplication-specific scenario. The results indicate a significant reduction in\ndistance underestimation, improving from 27% to 16.8% with a delayed\nteleportation method. Other sub-scales of distance estimation hardly differ.\nDespite targeted adaptations of previous study designs, participants have again\nfound strategies supporting them in estimating distances. We conclude that time\nis a factor affecting distance perception and should be considered alongside\nother factors identified in the literature.\n","authors":["Matthias Wölwer","Daniel Zielasko"],"pdf_url":"https://arxiv.org/pdf/2406.19895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19859v1","updated":"2024-06-28T11:58:26Z","published":"2024-06-28T11:58:26Z","title":"MetaDesigner: Advancing Artistic Typography through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","summary":"  MetaDesigner revolutionizes artistic typography synthesis by leveraging the\nstrengths of Large Language Models (LLMs) to drive a design paradigm centered\naround user engagement. At the core of this framework lies a multi-agent system\ncomprising the Pipeline, Glyph, and Texture agents, which collectively enable\nthe creation of customized WordArt, ranging from semantic enhancements to the\nimposition of complex textures. MetaDesigner incorporates a comprehensive\nfeedback mechanism that harnesses insights from multimodal models and user\nevaluations to refine and enhance the design process iteratively. Through this\nfeedback loop, the system adeptly tunes hyperparameters to align with\nuser-defined stylistic and thematic preferences, generating WordArt that not\nonly meets but exceeds user expectations of visual appeal and contextual\nrelevance. Empirical validations highlight MetaDesigner's capability to\neffectively serve diverse WordArt applications, consistently producing\naesthetically appealing and context-sensitive results.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Qi He","Wangmeng Xiang","Hanyuan Chen","Jin-Peng Lan","Xianhui Lin","Kang Zhu","Bin Luo","Yifeng Geng","Xuansong Xie","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19859v1.pdf","comment":"18 pages, 16 figures, Project:\n  https://modelscope.cn/studios/WordArt/WordArt"},{"id":"http://arxiv.org/abs/2401.13699v2","updated":"2024-06-28T11:49:52Z","published":"2024-01-22T03:17:41Z","title":"Generative AI-Driven Human Digital Twin in IoT-Healthcare: A\n  Comprehensive Survey","summary":"  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n","authors":["Jiayuan Chen","You Shi","Changyan Yi","Hongyang Du","Jiawen Kang","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2401.13699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10170v4","updated":"2024-06-28T11:25:41Z","published":"2023-12-15T19:37:39Z","title":"UINav: A Practical Approach to Train On-Device Automation Agents","summary":"  Automation systems that can autonomously drive application user interfaces to\ncomplete user tasks are of great benefit, especially when users are\nsituationally or permanently impaired. Prior automation systems do not produce\ngeneralizable models while AI-based automation agents work reliably only in\nsimple, hand-crafted applications or incur high computation costs. We propose\nUINav, a demonstration-based approach to train automation agents that fit\nmobile devices, yet achieving high success rates with modest numbers of\ndemonstrations. To reduce the demonstration overhead, UINav uses a referee\nmodel that provides users with immediate feedback on tasks where the agent\nfails, and automatically augments human demonstrations to increase diversity in\ntraining data. Our evaluation shows that with only 10 demonstrations UINav can\nachieve 70% accuracy, and that with enough demonstrations it can surpass 90%\naccuracy.\n","authors":["Wei Li","Fu-Lin Hsu","Will Bishop","Folawiyo Campbell-Ajala","Max Lin","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2312.10170v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19746v1","updated":"2024-06-28T08:35:15Z","published":"2024-06-28T08:35:15Z","title":"Voluminous Fur Stroking Experience through Interactive Visuo-Haptic\n  Model in Virtual Reality","summary":"  The tactile sensation of stroking soft fur, known for its comfort and\nemotional benefits, has numerous applications in virtual reality,\nanimal-assisted therapy, and household products. Previous studies have\nprimarily utilized actual fur to present a voluminous fur experience that poses\nchallenges concerning versatility and flexibility. In this study, we develop a\nsystem that integrates a head-mounted display with an ultrasound haptic display\nto provide visual and haptic feedback. Measurements taken using an artificial\nskin sheet reveal directional differences in tactile and visual responses to\nvoluminous fur. Based on observations and measurements, we propose interactive\nmodels that dynamically adjust to hand movements, simulating fur-stroking\nsensations. Our experiments demonstrate that the proposed model using visual\nand haptic modalities significantly enhances the realism of a fur-stroking\nexperience. Our findings suggest that the interactive visuo-haptic model offers\na promising fur-stroking experience in virtual reality, potentially enhancing\nthe user experience in therapeutic, entertainment, and retail applications.\n","authors":["Juro Hosoi","Du Jin","Yuki Ban","Shin'ichi Warisawa"],"pdf_url":"https://arxiv.org/pdf/2406.19746v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.19720v1","updated":"2024-06-28T08:09:55Z","published":"2024-06-28T08:09:55Z","title":"CUPID: Improving Battle Fairness and Position Satisfaction in Online\n  MOBA Games with a Re-matchmaking System","summary":"  The multiplayer online battle arena (MOBA) genre has gained significant\npopularity and economic success, attracting considerable research interest\nwithin the Human-Computer Interaction community. Enhancing the gaming\nexperience requires a deep understanding of player behavior, and a crucial\naspect of MOBA games is matchmaking, which aims to assemble teams of comparable\nskill levels. However, existing matchmaking systems often neglect important\nfactors such as players' position preferences and team assignment, resulting in\nimbalanced matches and reduced player satisfaction. To address these\nlimitations, this paper proposes a novel framework called CUPID, which\nintroduces a novel process called ``re-matchmaking'' to optimize team and\nposition assignments to improve both fairness and player satisfaction. CUPID\nincorporates a pre-filtering step to ensure a minimum level of matchmaking\nquality, followed by a pre-match win-rate prediction model that evaluates the\nfairness of potential assignments. By simultaneously considering players'\nposition satisfaction and game fairness, CUPID aims to provide an enhanced\nmatchmaking experience. Extensive experiments were conducted on two\nlarge-scale, real-world MOBA datasets to validate the effectiveness of CUPID.\nThe results surpass all existing state-of-the-art baselines, with an average\nrelative improvement of 7.18% in terms of win prediction accuracy. Furthermore,\nCUPID has been successfully deployed in a popular online mobile MOBA game. The\ndeployment resulted in significant improvements in match fairness and player\nsatisfaction, as evidenced by critical Human-Computer Interaction (HCI) metrics\ncovering usability, accessibility, and engagement, observed through A/B\ntesting. To the best of our knowledge, CUPID is the first re-matchmaking system\ndesigned specifically for large-scale MOBA games.\n","authors":["Ge Fan","Chaoyun Zhang","Kai Wang","Yingjie Li","Junyang Chen","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2406.19720v1.pdf","comment":"38 pages, accepted by CSCW 24"},{"id":"http://arxiv.org/abs/2406.01284v2","updated":"2024-06-28T08:05:56Z","published":"2024-06-03T12:53:29Z","title":"Extraction of Weak Surface Diaphragmatic Electromyogram Using Modified\n  Progressive FastICA Peel-Off","summary":"  Diaphragmatic electromyogram (EMGdi) contains crucial information about human\nrespiration therefore can be used to monitor respiratory condition. Although it\nis practical to record EMGdi noninvasively and conveniently by placing surface\nelectrodes over chest skin, extraction of such weak surface EMGdi (sEMGdi) from\ngreat noisy environment is a challenging task, limiting its clinical use\ncompared with esophageal EMGdi. In this paper, a novel method is presented for\nextracting weak sEMGdi signal from high-noise environment based on fast\nindependent component analysis (FastICA), constrained FastICA and a peel-off\nstrategy. It is truly a modified version of of progressive FastICA peel-off\n(PFP) framework, where the constrained FastICA helps to extract and refine\nrespiration-related sEMGdi signals, while the peel-off strategy ensures the\ncomplete extraction of weaker sEMGdi components. The method was validated using\nboth synthetic and clinical signals. It was demonstrated that our method was\nable to extract clean sEMGdi signals efficiently with little distortion. It\noutperformed state-of-the-art comparison methods in terms of sufficiently high\nSIR and CORR at all noise levels when tested on synthetic data, while also\nachieved an accuracy of 95.06% and a F2-score of 96.73% for breath\nidentification on clinical data. The study presents a valuable solution for\nnoninvasive extraction of sEMGdi signals, providing a convenient and valuable\nway of ventilator synchrony with a significant potential in aiding respiratory\nrehabilitation and health.\n","authors":["Yao Li","Dongsheng Zhao","Haowen Zhao","Xu Zhang","Min Shao"],"pdf_url":"https://arxiv.org/pdf/2406.01284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.17963v2","updated":"2024-06-28T06:44:45Z","published":"2024-06-25T22:44:53Z","title":"Empowering Interdisciplinary Insights with Dynamic Graph Embedding\n  Trajectories","summary":"  We developed DyGETViz, a novel framework for effectively visualizing dynamic\ngraphs (DGs) that are ubiquitous across diverse real-world systems. This\nframework leverages recent advancements in discrete-time dynamic graph (DTDG)\nmodels to adeptly handle the temporal dynamics inherent in dynamic graphs.\nDyGETViz effectively captures both micro- and macro-level structural shifts\nwithin these graphs, offering a robust method for representing complex and\nmassive dynamic graphs. The application of DyGETViz extends to a diverse array\nof domains, including ethology, epidemiology, finance, genetics, linguistics,\ncommunication studies, social studies, and international relations. Through its\nimplementation, DyGETViz has revealed or confirmed various critical insights.\nThese include the diversity of content sharing patterns and the degree of\nspecialization within online communities, the chronological evolution of\nlexicons across decades, and the distinct trajectories exhibited by\naging-related and non-related genes. Importantly, DyGETViz enhances the\naccessibility of scientific findings to non-domain experts by simplifying the\ncomplexities of dynamic graphs. Our framework is released as an open-source\nPython package for use across diverse disciplines. Our work not only addresses\nthe ongoing challenges in visualizing and analyzing DTDG models but also\nestablishes a foundational framework for future investigations into dynamic\ngraph representation and analysis across various disciplines.\n","authors":["Yiqiao Jin","Andrew Zhao","Yeon-Chang Lee","Meng Ye","Ajay Divakaran","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17963v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.19663v1","updated":"2024-06-28T05:14:42Z","published":"2024-06-28T05:14:42Z","title":"Aerial Push-Button with Two-Stage Tactile Feedback using Reflected\n  Airborne Ultrasound Focus","summary":"  We developed a new aerial push-button with tactile feedback using focused\nairborne ultrasound. This study has two significant novelties compared to past\nrelated studies: 1) ultrasound emitters are equipped behind the user's finger\nand reflected ultrasound emission that is focused just above the solid plane\nplaced under the finger presents tactile feedback to a finger pad, and 2)\ntactile feedback is presented at two stages during pressing motion; at the time\nof pushing the button and withdrawing the finger from it. The former has a\nsignificant advantage in apparatus implementation in that the input surface of\nthe device can be composed of a generic thin plane including touch panels,\npotentially capable of presenting input touch feedback only when the user\ntouches objects on the screen. We experimentally found that the two-stage\ntactile presentation is much more effective in strengthening perceived tactile\nstimulation and feeling of input completion when compared with a conventional\nsingle-stage method. This study proposes a composition of an aerial push-button\nin much more practical use than ever. The proposed system composition is\nexpected to be one of the simplest frameworks in the airborne ultrasound\ntactile interface.\n","authors":["Hiroya Sugawara","Masaya Takasaki","Keisuke Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2406.19663v1.pdf","comment":"9 pages, 15 figures, original manuscript edited by Microsoft Word"},{"id":"http://arxiv.org/abs/2406.19648v1","updated":"2024-06-28T04:33:41Z","published":"2024-06-28T04:33:41Z","title":"Designing and Evaluating Multi-Chatbot Interface for Human-AI\n  Communication: Preliminary Findings from a Persuasion Task","summary":"  The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.\n","authors":["Sion Yoon","Tae Eun Kim","Yoo Jung Oh"],"pdf_url":"https://arxiv.org/pdf/2406.19648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18887v2","updated":"2024-06-28T02:35:51Z","published":"2024-05-29T08:42:49Z","title":"4Doodle: Two-handed Gestures for Immersive Sketching of Architectural\n  Models","summary":"  Three-dimensional immersive sketching for content creation and modeling has\nbeen studied for some time. However, research in this domain mainly focused on\nCAVE-like scenarios. These setups can be expensive and offer a narrow\ninteraction space. Building more affordable setups using head-mounted displays\nis possible, allowing greater immersion and a larger space for user physical\nmovements. This paper presents a fully immersive environment using bi-manual\ngestures to sketch and create content freely in the virtual world. This\napproach can be applied to many scenarios, allowing people to express their\nideas or review existing designs. To cope with known motor difficulties and\ninaccuracy of freehand 3D sketching, we explore proxy geometry and a laser-like\nmetaphor to draw content directly from models and create content surfaces. Our\ncurrent prototype offers 24 cubic meters for movement, limited by the room\nsize. It features infinite virtual drawing space through pan and scale\ntechniques and is larger than the typical 6-sided cave at a fraction of the\ncost. In a preliminary study conducted with architects and engineers, our\nsystem showed a clear promise as a tool for sketching and 3D content creation\nin virtual reality with a great emphasis on bi-manual gestures.\n","authors":["Fernando Fonseca","Maurício Sousa","Daniel Mendes","Alfredo Ferreira","Joaquim Jorge"],"pdf_url":"https://arxiv.org/pdf/2405.18887v2.pdf","comment":"9 pages; 15 Figures"},{"id":"http://arxiv.org/abs/2402.00808v2","updated":"2024-06-28T02:16:13Z","published":"2024-02-01T17:44:46Z","title":"Exploring the Dynamics between Cobot's Production Rhythm, Locus of\n  Control and Emotional State in a Collaborative Assembly Scenario","summary":"  In industrial scenarios, there is widespread use of collaborative robots\n(cobots), and growing interest is directed at evaluating and measuring the\nimpact of some characteristics of the cobot on the human factor. In the present\npilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -\nAdapted to the participant's pace) of a cobot has on the Experiential Locus of\nControl (ELoC) and the emotional state of 31 participants has been examined.\nThe operators' performance, the degree of basic internal Locus of Control, and\nthe attitude towards the robots were also considered. No difference was found\nregarding the emotional state and the ELoC in the three conditions, but\nconsidering the other psychological variables, a more complex situation\nemerges. Overall, results seem to indicate a need to consider the person's\npsychological characteristics to offer a differentiated and optimal interaction\nexperience.\n","authors":["Marta Mondellini","Matteo Lavit Nicora","Pooja Prajod","Elisabeth André","Rocco Vertechy","Alessandro Antonietti","Matteo Malosio"],"pdf_url":"https://arxiv.org/pdf/2402.00808v2.pdf","comment":"Accepted to 4th IEEE International Conference on Human-Machine\n  Systems"},{"id":"http://arxiv.org/abs/2406.19601v1","updated":"2024-06-28T02:10:25Z","published":"2024-06-28T02:10:25Z","title":"Designing multi-model conversational AI financial systems: understanding\n  sensitive values of women entrepreneurs in Brazil","summary":"  Small business owners (SBOs), specially women, face several challenges in\neveryday life, especially when asking for microcredit loans from financial\ninstitutions. Usual difficulties include low credit scores, unbaked situations,\noutstanding debts, informal employment situations, inability to showcase their\npayable capacity, and lack of financial guarantor. Moreover, SBOs often need\nhelp applying for microcredit loans due to the lack of information on how to\nproceed. The task of asking for a loan is a complex practice, and asymmetric\npower relationships might emerge, but that benefits micro-entrepreneurs only\nsometimes. In this paper, we interviewed 20 women entrepreneurs living in a\nlow-income community in Brazil. We wanted to unveil value tensions derived from\nthis practice that might influence the design of AI technologies for the\npublic. In doing so, we used a conversational system as a probe to understand\nthe opportunities for empowering their practices with the support of AI\nmultimedia conversational systems. We derived seven recommendations for\ndesigning AI systems for evaluating micro-business health in low-income\ncommunities.\n","authors":["Heloisa Candello","Gabriel Meneguelli Soella","Leandro de Carvalho Nascimento"],"pdf_url":"https://arxiv.org/pdf/2406.19601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19600v1","updated":"2024-06-28T02:03:30Z","published":"2024-06-28T02:03:30Z","title":"Virtual Urban Field Studies: Evaluating Urban Interaction Design Using\n  Context-Based Interface Prototypes","summary":"  In this study, we propose the use of virtual urban field studies (VUFS)\nthrough context-based interface prototypes for evaluating the interaction\ndesign of auditory interfaces. Virtual field tests use mixed-reality\ntechnologies to combine the fidelity of real-world testing with the\naffordability and speed of testing in the lab. In this paper, we apply this\nconcept to rapidly test sound designs for autonomous vehicle (AV)--pedestrian\ninteraction with a high degree of realism and fidelity. We also propose the use\nof psychometrically validated measures of presence in validating the\nverisimilitude of VUFS. Using mixed qualitative and quantitative methods, we\nanalysed users' perceptions of presence in our VUFS prototype and the\nrelationship to our prototype's effectiveness. We also examined the use of\nhigher-order ambisonic spatialised audio and its impact on presence. Our\nresults provide insights into how VUFS can be designed to facilitate presence\nas well as design guidelines for how this can be leveraged.\n","authors":["Robert Dongas","Kazjon Grace","Samuel Gillespie","Marius Hoggenmueller","Martin Tomitsch","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2406.19600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19581v1","updated":"2024-06-28T00:08:13Z","published":"2024-06-28T00:08:13Z","title":"HarmonICA: Neural non-stationarity correction and source separation for\n  motor neuron interfaces","summary":"  A major outstanding problem when interfacing with spinal motor neurons is how\nto accurately compensate for non-stationary effects in the signal during source\nseparation routines, particularly when they cannot be estimated in advance.\nThis forces current systems to instead use undifferentiated bulk signal, which\nlimits the potential degrees of freedom for control. In this study we propose a\npotential solution, using an unsupervised learning algorithm to blindly correct\nfor the effects of latent processes which drive the signal non-stationarities.\nWe implement this methodology within the theoretical framework of a quasilinear\nversion of independent component analysis (ICA). The proposed design,\nHarmonICA, sidesteps the identifiability problems of nonlinear ICA, allowing\nfor equivalent predictability to linear ICA whilst retaining the ability to\nlearn complex nonlinear relationships between non-stationary latents and their\neffects on the signal. We test HarmonICA on both invasive and non-invasive\nrecordings both simulated and real, demonstrating an ability to blindly\ncompensate for the non-stationary effects specific to each, and thus to\nsignificantly enhance the quality of a source separation routine.\n","authors":["Alexander Kenneth Clarke","Agnese Grison","Irene Mendez Guerra","Pranav Mamidanna","Shihan Ma","Silvia Muceli","Dario Farina"],"pdf_url":"https://arxiv.org/pdf/2406.19581v1.pdf","comment":null}]}}